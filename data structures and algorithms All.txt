Page 127
Chapter
3 Arrays, Linked Lists, and Recursion
Contents
3.1 Using Arrays . . . . . . . . . . . . . . . . . . . . . . . 104
3.1.1 Storing Game Entries in an Array . . . . . . . . . . . 104
3.1.2 Sorting an Array . . . . . . . . . . . . . . . . . . . . 109
3.1.3 Two-Dimensional Arrays and Positional Games . . . 111
3.2 Singly Linked Lists . . . . . . . . . . . . . . . . . . . . 117
3.2.1 Implementing a Singly Linked List . . . . . . . . . . 117
3.2.2 Insertion to the Front of a Singly Linked List . . . . 119
3.2.3 Removal from the Front of a Singly Linked List . . . 119
3.2.4 Implementing a Generic Singly Linked List . . . . . . 121
3.3 Doubly Linked Lists . . . . . . . . . . . . . . . . . . . 123
3.3.1 Insertion into a Doubly Linked List . . . . . . . . . . 123
3.3.2 Removal from a Doubly Linked List . . . . . . . . . 124
3.3.3 A C++ Implementation . . . . . . . . . . . . . . . . 125
3.4 Circularly Linked Lists and List Reversal . . . . . . . . 129
3.4.1 Circularly Linked Lists . . . . . . . . . . . . . . . . . 129
3.4.2 Reversing a Linked List . . . . . . . . . . . . . . . . 133
3.5 Recursion . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.5.1 Linear Recursion . . . . . . . . . . . . . . . . . . . . 140
3.5.2 Binary Recursion . . . . . . . . . . . . . . . . . . . 144
3.5.3 Multiple Recursion . . . . . . . . . . . . . . . . . . 147
3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 149
Page 128
104Chapter 3. Arrays, Linked Lists, and Recursion
3.1 Using Arrays
In this section, we explore a few applications of arrays—the concrete data structures
introduced in Section 1.1.3 that access their entries using integer indices.
3.1.1 Storing Game Entries in an Array
The first application we study is for storing entries in an array; in particular, high
score entries for a video game. Storing objects in arrays is a common use for arrays,
and we could just as easily have chosen to store records for patients in a hospital or
the names of players on a football team. Nevertheless, let us focus on storing high
score entries, which is a simple application that is already rich enough to present
some important data structuring concepts.
Let us begin by thinking about what we want to include in an object represent
ing a high score entry. Obviously, one component to include is an integer repre
senting the score itself, which we call score. Another useful thing to include is the
name of the person earning this score, which we simply call name. We could go on
from here, adding fields representing the date the score was earned or game statis
tics that led to that score. Let us keep our example simple, however, and just have
two fields, score and name. The class structure is shown in Code Fragment 3.1.
class GameEntry {// a game score entry
public:
GameEntry(const string& n="", int s=0); // constructor
string getName() const;// get player name
int getScore() const;// get score
private:
string name;// player’s name
int score;// player’s score
};
Code Fragment 3.1: A C++ class representing a game entry.
In Code Fragment 3.2, we provide the definitions of the class constructor and
two accessor member functions.
GameEntry::GameEntry(const string& n, int s) // constructor
: name(n), score(s) { }
// accessors
string GameEntry::getName() const { return name; }
int GameEntry::getScore() const { return score; }
Code Fragment 3.2: GameEntry constructor and accessors.
Page 129
3.1. Using Arrays105
A Class for High Scores
Let’s now design a class, called Scores, to store our game-score information. We
store the highest scores in an array entries. The maximum number of scores may
vary from instance to instance, so we create a member variable, maxEntries, stor
ing the desired maximum. Its value is specified when a Scores object is first con
structed. In order to keep track of the actual number of entries, we define a member
variable numEntries. It is initialized to zero, and it is updated as entries are added
or removed. We provide a constructor, a destructor, a member function for adding
a new score, and one for removing a score at a given index. The definition is given
in Code Fragment 3.3.
class Scores {// stores game high scores
public:
Scores(int maxEnt = 10);// constructor
˜Scores();// destructor
void add(const GameEntry& e);// add a game entry
GameEntry remove(int i)// remove the ith entry
throw(IndexOutOfBounds);
private:
int maxEntries;// maximum number of entries
int numEntries;// actual number of entries
GameEntry* entries;// array of game entries
};
Code Fragment 3.3: A C++ class for storing high game scores.
In Code Fragment 3.4, we present the class constructor, which allocates the
desired amount of storage for the array using the “new” operator. Recall from
Section 1.1.3 that C++ represents a dynamic array as a pointer to its first element,
and this command returns such a pointer. The class destructor, ~Scores, deletes this
array.
Scores::Scores(int maxEnt) {// constructor
maxEntries = maxEnt;// save the max size
entries = new GameEntry[maxEntries];// allocate array storage
numEntries = 0;// initially no elements
}
Scores::˜Scores() {// destructor
delete[ ] entries;
}
Code Fragment 3.4: A C++ class GameEntry representing a game entry.
The entries that have been added to the array are stored in indices 0 through
numEntries −1. As more users play our video game, additional GameEntry objects
Page 130
106Chapter 3. Arrays, Linked Lists, and Recursion
are copied into the array. This is done using the class’s add member function, which
we describe below. Only the highest maxEntries scores are retained. We also
provide a member function, remove(i), which removes the entry at index i from
the array. We assume that 0 ≤ i ≤ numEntries − 1. If not, the remove function,
throws an IndexOutOfBounds exception. We do not define this exception here, but
it is derived from the class RuntimeException from Section 2.4.
In our design, we have chosen to order the GameEntry objects by their score
values, from highest to lowest. (In Exercise C-3.2, we explore an alternative design
in which entries are not ordered.) We illustrate an example of the data structure in
Figure 3.1.
Mike
1105
0 1Rob
750
2Paul
720
3Anna
660
4Rose
590
5Jack
510
6 7 8 9
Figure 3.1: The entries array of length eight storing six GameEntry objects in the
cells from index 0 to 5. Here maxEntries is 10 and numEntries is 6.
Insertion
Next, let us consider how to add a new GameEntry e to the array of high scores. In
particular, let us consider how we might perform the following update operation on
an instance of the Scores class.
add(e): Insert game entry e into the collection of high scores. If
this causes the number of entries to exceed maxEntries,
the smallest is removed.
The approach is to shift all the entries of the array whose scores are smaller than
e’s score to the right, in order to make space for the new entry. (See Figure 3.2.)
Mike
1105
0 1Rob
750
2Paul
720
3Anna
660
4Rose
590
5Jack
510
6 7 8 9Jill
740
Figure 3.2: Preparing to add a new GameEntry object (“Jill”,740) to the entries
array. In order to make room for the new entry, we shift all the entries with smaller
scores to the right by one position.
Page 131
3.1. Using Arrays107
Once we have identified the position in the entries array where the new game
entry, e, belongs, we copy e into this position. (See Figure 3.3.)
Mike
1105
0 1Rob
750Jill
740
2Paul
720
3Anna
660
4Rose
590
5Jack
510
6 7 8 9
Figure 3.3: After adding the new entry at index 2.
The details of our algorithm for adding the new game entry e to the entries array
are similar to this informal description and are given in Code Fragment 3.5. First,
we consider whether the array is already full. If so, we check whether the score of
the last entry in the array (which is at entries[maxEntries − 1]) is at least as large
as e’s score. If so, we can return immediately since e is not high enough to replace
any of the existing highest scores. If the array is not yet full, we know that one new
entry will be added, so we increment the value of numEntries. Next, we identify all
the entries whose scores are smaller than e’s and shift them one entry to the right.
To avoid overwriting existing array entries, we start from the right end of the array
and work to the left. The loop continues until we encounter an entry whose score
is not smaller than e’s, or we fall off the front end of the array. In either case, the
new entry is added at index i + 1.
void Scores::add(const GameEntry& e) { // add a game entry
int newScore = e.getScore();// score to add
if (numEntries == maxEntries) {// the array is full
if (newScore <= entries[maxEntries−1].getScore())
return;// not high enough - ignore
}
else numEntries++;// if not full, one more entry
int i = numEntries−2;// start with the next to last
while ( i >= 0 && newScore > entries[i].getScore() ) {
entries[i+1] = entries[i];// shift right if smaller
i−−;
} entries[i+1] = e;// put e in the empty spot
}
Code Fragment 3.5: C++ code for inserting a GameEntry object.
Check the code carefully to see that all the limiting cases have been handled
correctly by the add function (for example, largest score, smallest score, empty
array, full array). The number of times we perform the loop in this function depends
on the number of entries that we need to shift. This is pretty fast if the number of
entries is small. But if there are a lot to move, then this method could be fairly slow.
Page 132
108Chapter 3. Arrays, Linked Lists, and Recursion
Object Removal
Suppose some hot shot plays our video game and gets his or her name on our high
score list. In this case, we might want to have a function that lets us remove a game
entry from the list of high scores. Therefore, let us consider how we might remove
a GameEntry object from the entries array. That is, let us consider how we might
implement the following operation:
remove(i): Remove and return the game entry e at index i in the
entries array. If index i is outside the bounds of the
entries array, then this function throws an exception; oth
erwise, the entries array is updated to remove the ob
ject at index i and all objects previously stored at indices
higher than i are “shifted left” to fill in for the removed
object.
Our implementation of remove is similar to that of add, but in reverse. To
remove the entry at index i, we start at index i and move all the entries at indices
higher than i one position to the left. (See Figure 3.4.)
Mike
1105
0 1Rob
750Jill
740
2Paul
720
3Anna
660
4Rose
590
5Jack
510
6 7 8 9Return:
Figure 3.4: Removal of the entry (“Paul”,720) at index 3.
The code for performing the removal is presented in Code Fragment 3.6.
GameEntry Scores::remove(int i) throw(IndexOutOfBounds) {
if ((i < 0) | | (i >= numEntries))// invalid index
throw IndexOutOfBounds("Invalid index");
GameEntry e = entries[i];// save the removed object
for (int j = i+1; j < numEntries; j++)
entries[j−1] = entries[j];// shift entries left
numEntries−−;// one fewer entry
return e;// return the removed object
}
Code Fragment 3.6: C++ code for performing the remove operation.
Page 133
3.1. Using Arrays109
The removal operation involves a few subtle points. In order to return the value
of the removed game entry (let’s call it e), we must first save e in a temporary vari
able. When we are done, the function will return this value. The shifting process
starts at the position just following the removal, j = i+1. We repeatedly copy the
entry at index j to index j − 1, and then increment j, until coming to the last ele
ment of the set. Similar to the case of insertion, this left-to-right order is essential
to avoid overwriting existing entries. To complete the function, we return a copy of
the removed entry that was saved in e.
These functions for adding and removing objects in an array of high scores are
simple. Nevertheless, they form the basis of techniques that are used repeatedly
to build more sophisticated data structures. These other structures may be more
general than our simple array-based solution, and they may support many more
operations. But studying the concrete array data structure, as we are doing now, is
a great starting point for understanding these more sophisticated structures, since
every data structure has to be implemented using concrete means.
3.1.2 Sorting an Array
In the previous subsection, we worked hard to show how we can add or remove
objects at a certain index i in an array while keeping the previous order of the
objects intact. In this section, we consider how to rearrange objects of an array that
are ordered arbitrarily in ascending order. This is known as sorting.
We study several sorting algorithms in this book, most of which appear in Chap
ter 11. As a warmup, we describe a simple sorting algorithm called insertion-sort.
In this case, we describe a specific version of the algorithm where the input is an ar
ray of comparable elements. We consider more general kinds of sorting algorithms
later in this book.
We begin with a high-level outline of the insertion-sort algorithm. We start
with the first element in the array. One element by itself is already sorted. Then we
consider the next element in the array. If it is smaller than the first, we swap them.
Next we consider the third element in the array. We swap it leftward until it is in
its proper order with the first two elements. We continue in this manner with each
element of the array, swapping it leftward until it is in its proper position.
It is easy to see why this algorithm is called “insertion-sort”—each iteration
of the algorithm inserts the next element into the current sorted part of the array,
which was previously the subarray in front of that element. We may implement
the above outline using two nested loops. The outer loop considers each element
in the array in turn, and the inner loop moves that element to its proper location
with the (sorted) subarray of elements that are to its left. We illustrate the resulting
algorithm in Code Fragment 3.7.
This description is already quite close to actual C++ code. It indicates which
Page 134
110Chapter 3. Arrays, Linked Lists, and Recursion
Algorithm InsertionSort(A):
Input: An array A of n comparable elements
Output: The array A with elements rearranged in nondecreasing order
for i ← 1 to n−1 do
{Insert A[i] at its proper location in A[0],A[1],...,A[i−1]}
cur ← A[i]
j ← i−1
while j ≥ 0 and A[ j] > cur do
A[ j + 1] ← A[ j]
j ← j−1
A[ j + 1] ← cur {cur is now in the right place}
Code Fragment 3.7: Algorithmic description of the insertion-sort algorithm.
temporary variables are needed, how the loops are structured, and what decisions
need to be made. We illustrate an example run in Figure 3.5.
We present C++ code for our insertion-sort algorithm in Code Fragment 3.8.
We assume that the array to be sorted consists of elements of type char, but it is easy
to generalize this to other data types. The array A in the algorithm is implemented
as a char array. Recall that each array in C++ is represented as a pointer to its first
element, so the parameter A is declared to be of type char*. We also pass the size
of the array in an integer parameter n. The rest is a straightforward translation of
the description given in Code Fragment 3.7 into C++ syntax.
void insertionSort(char* A, int n) {// sort an array of n characters
for (int i = 1; i < n; i++) {// insertion loop
char cur = A[i];// current character to insert
int j = i − 1;// start at previous character
while ((j >= 0) && (A[j] > cur)) { // while A[j] is out of order
A[j + 1] = A[j];// move A[j] right
j−−;// decrement j
}
A[j + 1] = cur;// this is the proper place for cur
}
}
Code Fragment 3.8: C++ code implementing the insertion-sort algorithm.
An interesting thing happens in the insertion-sort algorithm if the array is al
ready sorted. In this case, the inner loop does only one comparison, determines that
there is no swap needed, and returns back to the outer loop. Of course, we might
have to do a lot more work than this if the input array is extremely out of order.
Indeed, the worst case arises if the initial array is given in descending order.
Page 135
3.1. Using Arrays111
Figure 3.5: Execution of the insertion-sort algorithm on an array of eight characters.
We show the completed (sorted) part of the array in white, and we color the next
element that is being inserted into the sorted part of the array with light blue. We
also highlight the character on the left, since it is stored in the cur variable. Each
row corresponds to an iteration of the outer loop, and each copy of the array in a
row corresponds to an iteration of the inner loop. Each comparison is shown with
an arc. In addition, we indicate whether that comparison resulted in a move or not.
3.1.3 Two-Dimensional Arrays and Positional Games
Many computer games, be they strategy games, simulation games, or first-person
conflict games, use a two-dimensional “board.” Programs that deal with such po
sitional games need a way of representing objects in a two-dimensional space. A
natural way to do this is with a two-dimensional array, where we use two indices,
say i and j, to refer to the cells in the array. The first index usually refers to a row
number and the second to a column number. Given such an array we can then main
tain two-dimensional game boards, as well as perform other kinds of computations
Page 136
112Chapter 3. Arrays, Linked Lists, and Recursion
involving data that is stored in rows and columns.
Arrays in C++ are one-dimensional; we use a single index to access each cell
of an array. Nevertheless, there is a way we can define two-dimensional arrays in
C++—we can create a two-dimensional array as an array of arrays. That is, we can
define a two-dimensional array to be an array with each of its cells being another
array. Such a two-dimensional array is sometimes also called a matrix. In C++, we
declare a two-dimensional array as follows:
int M[8][10];// matrix with 8 rows and 10 columns
This statement creates a two-dimensional “array of arrays,” M, which is 8 × 10,
having 8 rows and 10 columns. That is, M is an array of length 8 such that each
element of M is an array of length 10 of integers. (See Figure 3.6.)
Figure 3.6: A two-dimensional integer array that has 8 rows and 10 columns. The
value of M[3][5] is 100 and the value of M[6][2] is 632.
Given integer variables i and j, we could output the element of row i and col
umn j (or equivalently, the jth element of the ith array) as follows:
cout << M[i][j];// output element in row i column j
It is often a good idea to use symbolic constants to define the dimensions in
order to make your intentions clearer to someone reading your program.
const int N DAYS = 7;
const int N HOURS = 24;
int schedule[N DAYS][N HOURS];
Dynamic Allocation of Matrices
If the dimensions of a two-dimensional array are not known in advance, it is nec
essary to allocate the array dynamically. This can be done by applying the method
that we discussed earlier for allocating arrays in Section 1.1.3, but instead, we need
to apply it to each individual row of the matrix.
Page 137
3.1. Using Arrays113
For example, suppose that we wish to allocate an integer matrix with n rows and
m columns. Each row of the matrix is an array of integers of length m. Recall that
a dynamic array is represented as a pointer to its first element, so each row would
be declared to be of type int*. How do we group the individual rows together to
form the matrix? The matrix is an array of row pointers. Since each row pointer is
of type int*, the matrix is of type int**, that is, a pointer to a pointer of integers.
To generate our matrix, we first declare M to be of this type and allocate the
n row pointers with the command “M = new int*[n].” The ith row of the matrix
is allocated with the statement “M[i] = new int[m].” In Code Fragment 3.9, we
show how to do this given two integer variables n and m.
int** M = new int*[n];// allocate an array of row pointers
for (int i = 0; i < n; i++)
M[i] = new int[m];// allocate the i-th row
Code Fragment 3.9: Allocating storage for a matrix as an array of arrays.
Once allocated, we can access its elements just as before, for example, as
“M[i][ j].” As shown in Code Fragment 3.10, deallocating the matrix involves re
versing these steps. First, we deallocate each of the rows, one by one. We then
deallocate the array of row pointers. Since we are deleting an array, we use the
command “delete[].”
for (int i = 0; i < n; i++)
delete[ ] M[i];// delete the i-th row
delete[ ] M;// delete the array of row pointers
Code Fragment 3.10: Deallocating storage for a matrix as an array of arrays.
Using STL Vectors to Implement Matrices
As we can see from the previous section, dynamic allocation of matrices is rather
cumbersome. The STL vector class (recall Section 1.5.5) provides a much more
elegant way to process matrices. We adapt the same approach as above by imple
menting a matrix as a vector of vectors. Each row of our matrix is declared as
“vector<int>.” Thus, the entire matrix is declared to be a vector of rows, that is,
“vector<vector<int>>.” Let us declare M to be of this type.
Letting n denote the desired number of rows in the matrix, the constructor call
M(n) allocates storage for the rows. However, this does not allocate the desired
number of columns. The reason is that the default constructor is called for each
row, and the default is to construct an empty array.
To fix this, we make use of a nice feature of the vector class constructor. There
is an optional second argument, which indicates the value to use when initializing
Page 138
114Chapter 3. Arrays, Linked Lists, and Recursion
each element of the vector. In our case, each element of M is a vector of m integers,
that is, “vector<int>(m).” Thus, given integer variables n and m, the following code
fragment generates an n×m matrix as a vector of vectors.
vector< vector<int> > M(n, vector<int>(m));
cout << M[i][j] << endl;
The space between vector<int> and the following “>” has been added to prevent
ambiguity with the C++ input operator “>>.” Because the STL vector class au
tomatically takes care of deleting its members, we do not need to write a loop to
explicitly delete the rows, as we needed with dynamic arrays.
Two-dimensional arrays have many applications. Next, we explore a simple
application of two-dimensional arrays for implementing a positional game.
Tic-Tac-Toe
As most school children know, Tic-Tac-Toe is a game played on a three-by-three
board. Two players, X and O, alternate in placing their respective marks in the cells
of this board, starting with player X. If either player succeeds in getting three of his
or her marks in a row, column, or diagonal, then that player wins.
This is admittedly not a sophisticated positional game, and it’s not even that
much fun to play, since a good player O can always force a tie. Tic-Tac-Toe’s saving
grace is that it is a nice, simple example showing how two-dimensional arrays can
be used for positional games. Software for more sophisticated positional games,
such as checkers, chess, or the popular simulation games, are all based on the same
approach we illustrate here for using a two-dimensional array for Tic-Tac-Toe. (See
Exercise P-7.11.)
The basic idea is to use a two-dimensional array, board, to maintain the game
board. Cells in this array store values that indicate if that cell is empty or stores an X
or O. That is, board is a three-by-three matrix. For example, its middle row consists
of the cells board[1][0], board[1][1], and board[1][2]. In our case, we choose to
make the cells in the board array be integers, with a 0 indicating an empty cell, a 1
indicating an X, and a −1 indicating O. This encoding allows us to have a simple
way of testing whether a given board configuration is a win for X or O, namely, if
the values of a row, column, or diagonal add up to −3 or 3, respectively.
We give a complete C++ program for maintaining a Tic-Tac-Toe board for two
players in Code Fragments 3.11 and 3.12. We show the resulting output in Fig
ure 3.8. Note that this code is just for maintaining the Tic-Tac-Toe board and regis
tering moves; it doesn’t perform any strategy or allow someone to play Tic-Tac-Toe
against the computer. The details of such a program are beyond the scope of this
chapter, but it might nonetheless make a good project (see Exercise P-7.11).
Page 139
3.1. Using Arrays115
Figure 3.7: A Tic-Tac-Toe board and the array representing it.
#include <cstdlib>// system definitions
#include <iostream>// I/O definitions
using namespace std;// make std:: accessible
const int X = 1, O = −1, EMPTY = 0; // possible marks
int board[3][3];// playing board
int currentPlayer;// current player (X or O)
void clearBoard() {// clear the board
for (int i = 0; i < 3; i++)
for (int j = 0; j < 3; j++)
board[i][j] = EMPTY;// every cell is empty
currentPlayer = X;// player X starts
} void putMark(int i, int j) {// mark row i column j
board[i][j] = currentPlayer;// mark with current player
currentPlayer = −currentPlayer;// switch players
}
bool isWin(int mark) {// is mark the winner?
int win = 3*mark;// +3 for X and -3 for O
return ((board[0][0] + board[0][1] + board[0][2] == win)// row 0
| | (board[1][0] + board[1][1] + board[1][2] == win)// row 1
| | (board[2][0] + board[2][1] + board[2][2] == win)// row 2
| | (board[0][0] + board[1][0] + board[2][0] == win)// column 0
| | (board[0][1] + board[1][1] + board[2][1] == win)// column 1
| | (board[0][2] + board[1][2] + board[2][2] == win)// column 2
| | (board[0][0] + board[1][1] + board[2][2] == win)// diagonal
| | (board[2][0] + board[1][1] + board[0][2] == win));// diagonal
}
Code Fragment 3.11: A C++ program for playing Tic-Tac-Toe between two players.
(Continues in Code Fragment 3.12.)
Page 140
116Chapter 3. Arrays, Linked Lists, and Recursion
int getWinner() {// who wins? (EMPTY means tie)
if (isWin(X)) return X;
else if (isWin(O)) return O;
else return EMPTY;
}
void printBoard() {// print the board
for (int i = 0; i < 3; i++) {
for (int j = 0; j < 3; j++) {
switch (board[i][j]) {
case X:cout << "X"; break;
case O:cout << "O"; break;
case EMPTY: cout << " "; break;
}
if (j < 2) cout << "|";// column boundary
}
if (i < 2) cout << "\n-+-+-\n";// row boundary
}
}
int main() {// main program
clearBoard();// clear the board
putMark(0,0);putMark(1,1);// add the marks
putMark(0,1);putMark(0,2);
putMark(2,0);putMark(1,2);
putMark(2,2);putMark(2,1);
putMark(1,0);
printBoard();// print the final board
int winner = getWinner();
if (winner != EMPTY)// print the winner
cout << " " << (winner == X ? ’X’ : ’0’) << " wins" << endl;
else
cout << " Tie" << endl;
return EXIT SUCCESS;
}
Code Fragment 3.12: A C++ program for playing Tic-Tac-Toe between two players.
(Continued from Code Fragment 3.11.)
X|X|O
-+-+-
X|O|O
-+-+-
X|O|X X wins
Figure 3.8: Output of the Tic-Tac-Toe program.
Page 141
3.2. Singly Linked Lists117
3.2 Singly Linked Lists
In the previous section, we presented the array data structure and discussed some
of its applications. Arrays are nice and simple for storing things in a certain order,
but they have drawbacks. They are not very adaptable. For instance, we have
to fix the size n of an array in advance, which makes resizing an array difficult.
(This drawback is remedied in STL vectors.) Insertions and deletions are difficult
because elements need to be shifted around to make space for insertion or to fill
empty positions after deletion. In this section, we explore an important alternate
implementation of sequence, known as the singly linked list.
A linked list, in its simplest form, is a collection of nodes that together form a
linear ordering. As in the children’s game “Follow the Leader,” each node stores
a pointer, called next, to the next node of the list. In addition, each node stores its
associated element. (See Figure 3.9.)
Figure 3.9: Example of a singly linked list of airport codes. The next pointers are
shown as arrows. The null pointer is denoted by ∅.
The next pointer inside a node is a link or pointer to the next node of the list.
Moving from one node to another by following a next reference is known as link
hopping or pointer hopping. The first and last nodes of a linked list are called
the head and tail of the list, respectively. Thus, we can link-hop through the list,
starting at the head and ending at the tail. We can identify the tail as the node having
a null next reference. The structure is called a singly linked list because each node
stores a single link.
Like an array, a singly linked list maintains its elements in a certain order, as
determined by the chain of next links. Unlike an array, a singly linked list does not
have a predetermined fixed size. It can be resized by adding or removing nodes.
3.2.1 Implementing a Singly Linked List
Let us implement a singly linked list of strings. We first define a class StringNode
shown in Code Fragment 3.13. The node stores two values, the member elem stores
the element stored in this node, which in this case is a character string. (Later, in
Section 3.2.4, we describe how to define nodes that can store arbitrary types of
elements.) The member next stores a pointer to the next node of the list. We make
the linked list class a friend, so that it can access the node’s private members.
Page 142
118Chapter 3. Arrays, Linked Lists, and Recursion
class StringNode {// a node in a list of strings
private:
string elem;// element value
StringNode* next;// next item in the list
friend class StringLinkedList;// provide StringLinkedList access
};
Code Fragment 3.13: A node in a singly linked list of strings.
In Code Fragment 3.14, we define a class StringLinkedList for the actual linked
list. It supports a number of member functions, including a constructor and destruc
tor and functions for insertion and deletion. Their implementations are presented
later. Its private data consists of a pointer to the head node of the list.
class StringLinkedList {// a linked list of strings
public:
StringLinkedList();// empty list constructor
˜StringLinkedList();// destructor
bool empty() const;// is list empty?
const string& front() const;// get front element
void addFront(const string& e);// add to front of list
void removeFront();// remove front item list
private:
StringNode* head;// pointer to the head of list
};
Code Fragment 3.14: A class definition for a singly linked list of strings.
A number of simple member functions are shown in Code Fragment 3.15. The
list constructor creates an empty list by setting the head pointer to NULL. The de
structor repeatedly removes elements from the list. It exploits the fact that the func
tion remove (presented below) destroys the node that it removes. To test whether
the list is empty, we simply test whether the head pointer is NULL.
StringLinkedList::StringLinkedList()// constructor
: head(NULL) { }
StringLinkedList::˜StringLinkedList()// destructor
{ while (!empty()) removeFront(); }
bool StringLinkedList::empty() const// is list empty?
{ return head == NULL; }
const string& StringLinkedList::front() const// get front element
{ return head−>elem; }
Code Fragment 3.15: Some simple member functions of class StringLinkedList.
Page 143
3.2. Singly Linked Lists119
3.2.2 Insertion to the Front of a Singly Linked List
We can easily insert an element at the head of a singly linked list. We first create a
new node, and set its elem value to the desired string and set its next link to point to
the current head of the list. We then set head to point to the new node. The process
is illustrated in Figure 3.10.
(a)
(b)
(c)
Figure 3.10: Insertion of an element at the head of a singly linked list: (a) before
the insertion; (b) creation of a new node; (c) after the insertion.
An implementation is shown in Code Fragment 3.16. Note that access to the
private members elem and next of the StringNode class would normally be prohib
ited, but it is allowed here because StringLinkedList was declared to be a friend of
StringNode.
void StringLinkedList::addFront(const string& e) { // add to front of list
StringNode* v = new StringNode;// create new node
v−>elem = e;// store data
v−>next = head;// head now follows v
head = v;// v is now the head
}
Code Fragment 3.16: Insertion to the front of a singly linked list.
3.2.3 Removal from the Front of a Singly Linked List
Next, we consider how to remove an element from the front of a singly linked list.
We essentially undo the operations performed for insertion. We first save a pointer
Page 144
120Chapter 3. Arrays, Linked Lists, and Recursion
to the old head node and advance the head pointer to the next node in the list. We
then delete the old head node. This operation is illustrated in Figure 3.11.
(a)
(b)
(c)
Figure 3.11: Removal of an element at the head of a singly linked list: (a) before
the removal; (b) “linking out” the old new node; (c) after the removal.
An implementation of this operation is provided in Code Fragment 3.17. We
assume that the user has checked that the list is nonempty before applying this
operation. (A more careful implementation would throw an exception if the list
were empty.) The function deletes the node in order to avoid any memory leaks.
We do not return the value of the deleted node. If its value is desired, we can call
the front function prior to the removal.
void StringLinkedList::removeFront() {// remove front item
StringNode* old = head;// save current head
head = old−>next;// skip over old head
delete old;// delete the old head
}
Code Fragment 3.17: Removal from the front of a singly linked list.
It is noteworthy that we cannot as easily delete the last node of a singly linked
list, even if we had a pointer to it. In order to delete a node, we need to update the
next link of the node immediately preceding the deleted node. Locating this node
involves traversing the entire list and could take a long time. (We remedy this in
Section 3.3 when we discuss doubly linked lists.)
Page 145
3.2. Singly Linked Lists121
3.2.4 Implementing a Generic Singly Linked List
The implementation of the singly linked list given in Section 3.2.1 assumes that the
element type is a character string. It is easy to convert the implementation so that it
works for an arbitrary element type through the use of C++’s template mechanism.
The resulting generic singly linked list class is called SLinkedList.
We begin by presenting the node class, called SNode, in Code Fragment 3.18.
The element type associated with each node is parameterized by the type vari
able E. In contrast to our earlier version in Code Fragment 3.13, references to the
data type “string” have been replaced by “E.” When referring to our templated
node and list class, we need to include the suffix “<E>.” For example, the class is
SLinkedList<E> and the associated node is SNode<E>.
template <typename E>
class SNode {// singly linked list node
private:
E elem;// linked list element value
SNode<E>* next;// next item in the list
friend class SLinkedList<E>;// provide SLinkedList access
};
Code Fragment 3.18: A node in a generic singly linked list.
The generic list class is presented in Code Fragment 3.19. As above, refer
ences to the specific element type “string” have been replaced by references to the
generic type parameter “E.” To keep things simple, we have omitted housekeeping
functions such as a copy constructor.
template <typename E>
class SLinkedList {// a singly linked list
public:
SLinkedList();// empty list constructor
˜SLinkedList();// destructor
bool empty() const;// is list empty?
const E& front() const;// return front element
void addFront(const E& e);// add to front of list
void removeFront();// remove front item list
private:
SNode<E>* head;// head of the list
};
Code Fragment 3.19: A class definition for a generic singly linked list.
In Code Fragment 3.20, we present the class member functions. Note the sim
ilarity with Code Fragments 3.15 through 3.17. Observe that each definition is
prefaced by the template specifier template <typename E>.
Page 146
122Chapter 3. Arrays, Linked Lists, and Recursion
template <typename E>
SLinkedList<E>::SLinkedList()// constructor
: head(NULL) { }
template <typename E>
bool SLinkedList<E>::empty() const// is list empty?
{ return head == NULL; }
template <typename E>
const E& SLinkedList<E>::front() const// return front element
{ return head−>elem; }
template <typename E>
SLinkedList<E>::˜SLinkedList()// destructor
{ while (!empty()) removeFront(); }
template <typename E>
void SLinkedList<E>::addFront(const E& e) {// add to front of list
SNode<E>* v = new SNode<E>;// create new node
v−>elem = e;// store data
v−>next = head;// head now follows v
head = v;// v is now the head
}
template <typename E>
void SLinkedList<E>::removeFront() {// remove front item
SNode<E>* old = head;// save current head
head = old−>next;// skip over old head
delete old;// delete the old head
}
Code Fragment 3.20: Other member functions for a generic singly linked list.
We can generate singly linked lists of various types by simply setting the tem
plate parameter as desired as shown in the following code fragment.
SLinkedList<string> a;// list of strings
a.addFront("MSP");
// . . .
SLinkedList<int> b;// list of integers
b.addFront(13);
Code Fragment 3.21: Examples using the generic singly linked list class.
Because templated classes carry a relatively high notational burden, we often
sacrifice generality for simplicity, and avoid the use of templated classes in some
of our examples.
Page 147
3.3. Doubly Linked Lists123
3.3 Doubly Linked Lists
As we saw in the previous section, removing an element at the tail of a singly
linked list is not easy. Indeed, it is time consuming to remove any node other than
the head in a singly linked list, since we do not have a quick way of accessing the
node immediately preceding the one we want to remove. There are many appli
cations where we do not have quick access to such a predecessor node. For such
applications, it would be nice to have a way of going both directions in a linked list.
There is a type of linked list that allows us to go in both directions—forward
and reverse—in a linked list. It is the doubly linked list. In addition to its element
member, a node in a doubly linked list stores two pointers, a next link and a prev
link, which point to the next node in the list and the previous node in the list, re
spectively. Such lists allow for a great variety of quick update operations, including
efficient insertion and removal at any given position.
Header and Trailer Sentinels
To simplify programming, it is convenient to add special nodes at both ends of
a doubly linked list: a header node just before the head of the list, and a trailer
node just after the tail of the list. These “dummy” or sentinel nodes do not store
any elements. They provide quick access to the first and last nodes of the list. In
particular, the header’s next pointer points to the first node of the list, and the prev
pointer of the trailer node points to the last node of the list. An example is shown
in Figure 3.12.
Figure 3.12: A doubly linked list with sentinels, header and trailer, marking the
ends of the list. An empty list would have these sentinels pointing to each other.
We do not show the null prev pointer for the header nor do we show the null next
pointer for the trailer.
3.3.1 Insertion into a Doubly Linked List
Because of its double link structure, it is possible to insert a node at any position
within a doubly linked list. Given a node v of a doubly linked list (which could
possibly be the header, but not the trailer), let z be a new node that we wish to insert
Page 148
124Chapter 3. Arrays, Linked Lists, and Recursion
immediately after v. Let w the be node following v, that is, w is the node pointed to
by v’s next link. (This node exists, since we have sentinels.) To insert z after v, we
link it into the current list, by performing the following operations:
• Make z’s prev link point to v
• Make z’s next link point to w
• Make w’s prev link point to z
• Make v’s next link point to z
This process is illustrated in Figure 3.13, where v points to the node JFK, w points
to PVD, and z points to the new node BWI. Observe that this process works if v is
any node ranging from the header to the node just prior to the trailer.
(a)
(b)
Figure 3.13: Adding a new node after the node storing JFK: (a) creating a new node
with element BWI and linking it in; (b) after the insertion.
3.3.2 Removal from a Doubly Linked List
Likewise, it is easy to remove a node v from a doubly linked list. Let u be the node
just prior to v, and w be the node just following v. (These nodes exist, since we
have sentinels.) To remove node v, we simply have u and w point to each other
instead of to v. We refer to this operation as the linking out of v. We perform the
following operations.
• Make w’s prev link point to u
• Make u’s next link point to w
• Delete node v
This process is illustrated in Figure 3.14, where v is the node PVD, u is the node
JFK, and w is the node SFO. Observe that this process works if v is any node from
the header to the tail node (the node just prior to the trailer).
Page 149
3.3. Doubly Linked Lists125
(a)
(b)
(c)
Figure 3.14: Removing the node storing PVD: (a) before the removal; (b) linking
out the old node; (c) after node deletion.
3.3.3 A C++ Implementation
Let us consider how to implement a doubly linked list in C++. First, we present a
C++ class for a node of the list in Code Fragment 3.22. To keep the code simple,
we have chosen not to derive a templated class as we did in Section 3.2.1 for singly
linked lists. Instead, we provide a typedef statement that defines the element type,
called Elem. We define it to be a string, but any other type could be used instead.
Each node stores an element. It also contains pointers to both the previous and next
nodes of the list. We declare DLinkedList to be a friend, so it can access the node’s
private members.
typedef string Elem;// list element type
class DNode {// doubly linked list node
private:
Elem elem;// node element value
DNode* prev;// previous node in list
DNode* next;// next node in list
friend class DLinkedList;// allow DLinkedList access
};
Code Fragment 3.22: C++ implementation of a doubly linked list node.
Next, we present the definition of the doubly linked list class, DLinkedList,
in Code Fragment 3.23. In addition to a constructor and destructor, the public
members consist of a function that indicates whether the list is currently empty
Page 150
126Chapter 3. Arrays, Linked Lists, and Recursion
(meaning that it has no nodes other than the sentinels) and accessors to retrieve
the front and back elements. We also provide methods for inserting and removing
elements from the front and back of the list. There are two private data members,
header and trailer, which point to the sentinels. Finally, we provide two protected
utility member functions, add and remove. They are used internally by the class
and by its subclasses, but they cannot be invoked from outside the class.
class DLinkedList {// doubly linked list
public:
DLinkedList();// constructor
˜DLinkedList();// destructor
bool empty() const;// is list empty?
const Elem& front() const;// get front element
const Elem& back() const;// get back element
void addFront(const Elem& e);// add to front of list
void addBack(const Elem& e);// add to back of list
void removeFront();// remove from front
void removeBack();// remove from back
private:// local type definitions
DNode* header;// list sentinels
DNode* trailer;
protected:// local utilities
void add(DNode* v, const Elem& e);// insert new node before v
void remove(DNode* v);// remove node v
};
Code Fragment 3.23: Implementation of a doubly linked list class.
Let us begin by presenting the class constructor and destructor as shown in
Code Fragment 3.24. The constructor creates the sentinel nodes and sets each to
point to the other, and the destructor removes all but the sentinel nodes.
DLinkedList::DLinkedList() {// constructor
header = new DNode;// create sentinels
trailer = new DNode;
header−>next = trailer;// have them point to each other
trailer−>prev = header;
}
DLinkedList::˜DLinkedList() {// destructor
while (!empty()) removeFront();// remove all but sentinels
delete header;// remove the sentinels
delete trailer;
}
Code Fragment 3.24: Class constructor and destructor.
Page 151
3.3. Doubly Linked Lists127
Next, in Code Fragment 3.25 we show the basic class accessors. To determine
whether the list is empty, we check that there is no node between the two sentinels.
We do this by testing whether the trailer follows immediately after the header. To
access the front element of the list, we return the element associated with the node
that follows the list header. To access the back element, we return the element
associated with node that precedes the trailer. Both operations assume that the list
is nonempty. We could have enhanced these functions by throwing an exception if
an attempt is made to access the front or back of an empty list, just as we did in
Code Fragment 3.6.
bool DLinkedList::empty() const// is list empty?
{ return (header−>next == trailer); }
const Elem& DLinkedList::front() const// get front element
{ return header−>next−>elem; }
const Elem& DLinkedList::back() const// get back element
{ return trailer−>prev−>elem; }
Code Fragment 3.25: Accessor functions for the doubly linked list class.
In Section 3.3.1, we discussed how to insert a node into a doubly linked list.
The local utility function add, which is shown in Code Fragment 3.26, implements
this operation. In order to add a node to the front of the list, we create a new node,
and insert it immediately after the header, or equivalently, immediately before the
node that follows the header. In order to add a new node to the back of the list, we
create a new node, and insert it immediately before the trailer.
// insert new node before v
void DLinkedList::add(DNode* v, const Elem& e) {
DNode* u = new DNode; u−>elem = e; // create a new node for e
u−>next = v;// link u in between v
u−>prev = v−>prev;// . . .and v->prev
v−>prev−>next = v−>prev = u;
}
void DLinkedList::addFront(const Elem& e) // add to front of list
{ add(header−>next, e); }
void DLinkedList::addBack(const Elem& e) // add to back of list
{ add(trailer, e); }
Code Fragment 3.26: Inserting a new node into a doubly linked list. The protected
utility function add inserts a node z before an arbitrary node v. The public member
functions addFront and addBack both invoke this utility function.
Page 152
128Chapter 3. Arrays, Linked Lists, and Recursion
Observe that the above code works even if the list is empty (meaning that the
only nodes are the header and trailer). For example, if addBack is invoked on an
empty list, then the value of trailer->prev is a pointer to the list header. Thus,
the node is added between the header and trailer as desired. One of the major
advantages of providing sentinel nodes is to avoid handling of special cases, which
would otherwise be needed.
Finally, let us discuss deletion. In Section 3.3.2, we showed how to remove
an arbitrary node from a doubly linked list. In Code Fragment 3.27, we present
local utility function remove, which performs the operation. In addition to linking
out the node, it also deletes the node. The public member functions removeFront
and removeBack are implemented by deleting the nodes immediately following the
header and immediately preceding the trailer, respectively.
void DLinkedList::remove(DNode* v) {// remove node v
DNode* u = v−>prev;// predecessor
DNode* w = v−>next;// successor
u−>next = w;// unlink v from list
w−>prev = u;
delete v;
}
void DLinkedList::removeFront()// remove from font
{ remove(header−>next); }
void DLinkedList::removeBack()// remove from back
{ remove(trailer−>prev); }
Code Fragment 3.27: Removing a node from a doubly linked list. The local utility
function remove removes the node v. The public member functions removeFront
and removeBack invoke this utility function.
There are many more features that we could have added to our simple imple
mentation of a doubly linked list. Although we have provided access to the ends of
the list, we have not provided any mechanism for accessing or modifying elements
in the middle of the list. Later, in Chapter 6, we discuss the concept of iterators,
which provides a mechanism for accessing arbitrary elements of a list.
We have also performed no error checking in our implementation. It is the
user’s responsibility not to attempt to access or remove elements from an empty
list. In a more robust implementation of a doubly linked list, we would design the
member functions front, back, removeFront, and removeBack to throw an excep
tion when an attempt is made to perform one of these functions on an empty list.
Nonetheless, this simple implementation illustrates how easy it is to manipulate
this useful data structure.
Page 153
3.4. Circularly Linked Lists and List Reversal129
3.4 Circularly Linked Lists and List Reversal
In this section, we study some applications and extensions of linked lists.
3.4.1 Circularly Linked Lists
A circularly linked list has the same kind of nodes as a singly linked list. That is,
each node in a circularly linked list has a next pointer and an element value. But,
rather than having a head or tail, the nodes of a circularly linked list are linked
into a cycle. If we traverse the nodes of a circularly linked list from any node by
following next pointers, we eventually visit all the nodes and cycle back to the
node from which we started.
Even though a circularly linked list has no beginning or end, we nevertheless
need some node to be marked as a special node, which we call the cursor. The
cursor node allows us to have a place to start from if we ever need to traverse a
circularly linked list.
There are two positions of particular interest in a circular list. The first is the
element that is referenced by the cursor, which is called the back, and the element
immediately following this in the circular order, which is called the front. Although
it may seem odd to think of a circular list as having a front and a back, observe that,
if we were to cut the link between the node referenced by the cursor and this node’s
immediate successor, the result would be a singly linked list from the front node to
the back node.
LAXMSPATLBOS(front)(back)cursor
Figure 3.15: A circularly linked list. The node referenced by the cursor is called the
back, and the node immediately following is called the front.
We define the following functions for a circularly linked list:
front(): Return the element referenced by the cursor; an error re
sults if the list is empty.
back(): Return the element immediately after the cursor; an error
results if the list is empty.
advance(): Advance the cursor to the next node in the list.
Page 154
130Chapter 3. Arrays, Linked Lists, and Recursion
add(e): Insert a new node with element e immediately after the
cursor; if the list is empty, then this node becomes the
cursor and its next pointer points to itself.
remove(): Remove the node immediately after the cursor (not the
cursor itself, unless it is the only node); if the list be
comes empty, the cursor is set to null.
In Code Fragment 3.28, we show a C++ implementation of a node of a cir
cularly linked list, assuming that each node contains a single string. The node
structure is essentially identical to that of a singly linked list (recall Code Frag
ment 3.13). To keep the code simple, we have not implemented a templated class.
Instead, we provide a typedef statement that defines the element type Elem to be
the base type of the list, which in this case is a string.
typedef string Elem;// element type
class CNode {// circularly linked list node
private:
Elem elem;// linked list element value
CNode* next;// next item in the list
friend class CircleList;// provide CircleList access
};
Code Fragment 3.28: A node of a circularly linked list.
Next, in Code Fragment 3.29, we present the class definition for a circularly
linked list called CircleList. In addition to the above functions, the class provides
a constructor, a destructor, and a function to detect whether the list is empty. The
private member consists of the cursor, which points to some node of the list.
class CircleList {// a circularly linked list
public:
CircleList();// constructor
˜CircleList();// destructor
bool empty() const;// is list empty?
const Elem& front() const;// element at cursor
const Elem& back() const;// element following cursor
void advance();// advance cursor
void add(const Elem& e);// add after cursor
void remove();// remove node after cursor
private:
CNode* cursor;// the cursor
};
Code Fragment 3.29: Implementation of a circularly linked list class.
Page 155
3.4. Circularly Linked Lists and List Reversal131
Code Fragment 3.30 presents the class’s constructor and destructor. The con
structor generates an empty list by setting the cursor to NULL. The destructor iter
atively removes nodes until the list is empty. We exploit the fact that the member
function remove (given below) deletes the node that it removes.
CircleList::CircleList()// constructor
: cursor(NULL) { }
CircleList::˜CircleList()// destructor
{ while (!empty()) remove(); }
Code Fragment 3.30: The constructor and destructor.
We present a number of simple member functions in Code Fragment 3.31. To
determine whether the list is empty, we test whether the cursor is NULL. The ad
vance function advances the cursor to the next element.
bool CircleList::empty() const// is list empty?
{ return cursor == NULL; }
const Elem& CircleList::back() const// element at cursor
{ return cursor−>elem; }
const Elem& CircleList::front() const// element following cursor
{ return cursor−>next−>elem; }
void CircleList::advance()// advance cursor
{ cursor = cursor−>next; }
Code Fragment 3.31: Simple member functions.
Next, let us consider insertion. Recall that insertions to the circularly linked list
occur after the cursor. We begin by creating a new node and initializing its data
member. If the list is empty, we create a new node that points to itself. We then
direct the cursor to point to this element. Otherwise, we link the new node just after
the cursor. The code is presented in Code Fragment 3.32.
void CircleList::add(const Elem& e) {// add after cursor
CNode* v = new CNode;// create a new node
v−>elem = e;
if (cursor == NULL) {// list is empty?
v−>next = v;// v points to itself
cursor = v;// cursor points to v
}
else {// list is nonempty?
v−>next = cursor−>next;// link in v after cursor
cursor−>next = v;
}
}
Code Fragment 3.32: Inserting a node just after the cursor of a circularly linked list.
Page 156
132Chapter 3. Arrays, Linked Lists, and Recursion
Finally, we consider removal. We assume that the user has checked that the list
is nonempty before invoking this function. (A more careful implementation would
throw an exception if the list is empty.) There are two cases. If this is the last node
of the list (which can be tested by checking that the node to be removed points to
itself) we set the cursor to NULL. Otherwise, we link the cursor’s next pointer to
skip over the removed node. We then delete the node. The code is presented in
Code Fragment 3.33.
void CircleList::remove() {// remove node after cursor
CNode* old = cursor−>next;// the node being removed
if (old == cursor)// removing the only node?
cursor = NULL;// list is now empty
else
cursor−>next = old−>next;// link out the old node
delete old;// delete the old node
}
Code Fragment 3.33: Removing the node following the cursor.
To keep the code simple, we have omitted error checking. In front, back, and
advance, we should first test whether the list is empty, since otherwise the cursor
pointer will be NULL. In the first two cases, we should throw some sort of excep
tion. In the case of advance, if the list is empty, we can simply return.
Maintaining a Playlist for a Digital Audio Player
To help illustrate the use of our CircleList implementation of the circularly linked
list, let us consider how to build a simple interface for maintaining a playlist for
a digital audio player, also known as an MP3 player. The songs of the player are
stored in a circular list. The cursor points to the current song. By advancing the
cursor, we can move from one song to the next. We can also add new songs and
remove songs by invoking the member functions insert and remove, respectively.
Of course, a complete implementation would need to provide a method for playing
the current song, but our purpose is to illustrate how the circularly linked list can
be applied to this task.
To make this more concrete, suppose that you have a friend who loves retro
music, and you want to create a playlist of songs from the bygone Disco Era. The
main program is presented Code Fragment 3.34. We declare an object playList
to be a CircleList. The constructor creates an empty playlist. We proceed to add
three songs, “Stayin Alive,” “Le Freak,” and “Jive Talkin.” The comments on the
right show the current contents of the list in square brackets. The first entry of the
list is the element immediately following the cursor (which is where insertion and
removal occur), and the last entry in the list is cursor (which is indicated with an
asterisk).
Page 157
3.4. Circularly Linked Lists and List Reversal133
Suppose that we decide to replace “Stayin Alive” with “Disco Inferno.” We
advance the cursor twice so that “Stayin Alive” comes immediately after the cursor.
We then remove this entry and insert its replacement.
int main() {
CircleList playList;// [ ]
playList.add("Stayin Alive");// [Stayin Alive*]
playList.add("Le Freak");// [Le Freak, Stayin Alive*]
playList.add("Jive Talkin");// [Jive Talkin, Le Freak, Stayin Alive*]
playList.advance();// [Le Freak, Stayin Alive, Jive Talkin*]
playList.advance();// [Stayin Alive, Jive Talkin, Le Freak*]
playList.remove();// [Jive Talkin, Le Freak*]
playList.add("Disco Inferno"); // [Disco Inferno, Jive Talkin, Le Freak*]
return EXIT SUCCESS;
}
Code Fragment 3.34: Using the CircleList class to implement a playlist for a digital
audio player.
3.4.2 Reversing a Linked List
As another example of the manipulation of linked lists, we present a simple function
for reversing the elements of a doubly linked list. Given a list L, our approach
involves first copying the contents of L in reverse order into a temporary list T, and
then copying the contents of T back into L (but without reversing).
To achieve the initial reversed copy, we repeatedly extract the first element of
L and copy it to the front of T. (To see why this works, observe that the later an
element appears in L, the earlier it will appear in T.) To copy the contents of T
back to L, we repeatedly extract elements from the front of T, but this time we
copy each one to the back of list L. Our C++ implementation is presented in Code
Fragment 3.35.
void listReverse(DLinkedList& L) {// reverse a list
DLinkedList T;// temporary list
while (!L.empty()) {// reverse L into T
string s = L.front(); L.removeFront();
T.addFront(s);
}
while (!T.empty()) {// copy T back to L
string s = T.front(); T.removeFront();
L.addBack(s);
}
}
Code Fragment 3.35: A function that reverses the contents of a doubly linked list L.
Page 158
134Chapter 3. Arrays, Linked Lists, and Recursion
3.5 Recursion
We have seen that repetition can be achieved by writing loops, such as for loops
and while loops. Another way to achieve repetition is through recursion, which
occurs when a function refers to itself in its own definition. We have seen examples
of functions calling other functions, so it should come as no surprise that most
modern programming languages, including C++, allow a function to call itself. In
this section, we see why this capability provides an elegant and powerful alternative
for performing repetitive tasks.
The Factorial Function
To illustrate recursion, let us begin with a simple example of computing the value of
the factorial function. The factorial of a positive integer n, denoted n!, is defined
as the product of the integers from 1 to n. If n = 0, then n! is defined as 1 by
convention. More formally, for any integer n ≥ 0,
n! =  1 n·(n−1)·(n−2)···3·2·1if if n n = ≥ 0 1.
For example, 5! = 5 · 4 · 3 · 2 · 1 = 120. To make the connection with functions
clearer, we use the notation factorial(n) to denote n!.
The factorial function can be defined in a manner that suggests a recursive
formulation. To see this, observe that
factorial(5) = 5·(4·3·2·1) = 5·factorial(4).
Thus, we can define factorial(5) in terms of factorial(4). In general, for a positive
integer n, we can define factorial(n) to be n · factorial(n − 1). This leads to the
following recursive definition
factorial(n) =  1 n·factorial(n−1)if if n n = ≥ 0 1.
This definition is typical of many recursive definitions. First, it contains one
or more base cases, which are defined nonrecursively in terms of fixed quantities.
In this case, n = 0 is the base case. It also contains one or more recursive cases,
which are defined by appealing to the definition of the function being defined. Ob
serve that there is no circularity in this definition because each time the function is
invoked, its argument is smaller by one.
Page 159
3.5. Recursion135
A Recursive Implementation of the Factorial Function
Let us consider a C++ implementation of the factorial function shown in Code Frag
ment 3.36 under the name recursiveFactorial. Notice that no looping was needed
here. The repeated recursive invocations of the function take the place of looping.
int recursiveFactorial(int n) {// recursive factorial function
if (n == 0) return 1;// basis case
else return n * recursiveFactorial(n−1); // recursive case
}
Code Fragment 3.36: A recursive implementation of the factorial function.
We can illustrate the execution of a recursive function definition by means of a
recursion trace. Each entry of the trace corresponds to a recursive call. Each new
recursive function call is indicated by an arrow to the newly called function. When
the function returns, an arrow showing this return is drawn, and the return value
may be indicated with this arrow. An example of a trace is shown in Figure 3.16.
What is the advantage of using recursion? Although the recursive implementa
tion of the factorial function is somewhat simpler than the iterative version, in this
case there is no compelling reason for preferring recursion over iteration. For some
problems, however, a recursive implementation can be significantly simpler and
easier to understand than an iterative implementation. Such an example follows.
Figure 3.16: A recursion trace for the call recursiveFactorial(4).
Page 160
136Chapter 3. Arrays, Linked Lists, and Recursion
Drawing an English Ruler
As a more complex example of the use of recursion, consider how to draw the
markings of a typical English ruler. Such a ruler is broken into intervals, and each
interval consists of a set of ticks, placed at intervals of 1/2 inch, 1/4 inch, and so
on. As the size of the interval decreases by half, the tick length decreases by one.
(See Figure 3.17.)
---- 0----- 0--- 0
---
------
---
--------- 1
---
------
---
---- 1------- 2
---
------
---
--------- 3
--
----
--
---- 2----- 1
(a)(b)(c)
Figure 3.17: Three sample outputs of an English ruler drawing: (a) a 2-inch ruler
with major tick length 4; (b) a 1-inch ruler with major tick length 5; (c) a 3-inch
ruler with major tick length 3.
Each fraction of an inch also has a numeric label. The longest tick length is
called the major tick length. We won’t worry about actual distances, however, and
just print one tick per line.
A Recursive Approach to Ruler Drawing
Our approach to drawing such a ruler consists of three functions. The main function
drawRuler draws the entire ruler. Its arguments are the total number of inches in
the ruler, nInches, and the major tick length, majorLength. The utility function dra
wOneTick draws a single tick of the given length. It can also be given an optional
integer label, which is printed if it is nonnegative.
Page 161
3.5. Recursion137
The interesting work is done by the recursive function drawTicks, which draws
the sequence of ticks within some interval. Its only argument is the tick length
associated with the interval’s central tick. Consider the English ruler with major
tick length 5 shown in Figure 3.17(b). Ignoring the lines containing 0 and 1, let us
consider how to draw the sequence of ticks lying between these lines. The central
tick (at 1/2 inch) has length 4. Observe that the two patterns of ticks above and
below this central tick are identical, and each has a central tick of length 3. In
general, an interval with a central tick length L ≥ 1 is composed of the following:
• An interval with a central tick length L−1
• A single tick of length L
• An interval with a central tick length L−1
With each recursive call, the length decreases by one. When the length drops to
zero, we simply return. As a result, this recursive process always terminates. This
suggests a recursive process in which the first and last steps are performed by call
ing the drawTicks(L − 1) recursively. The middle step is performed by calling
the function drawOneTick(L). This recursive formulation is shown in Code Frag
ment 3.37. As in the factorial example, the code has a base case (when L = 0). In
this instance we make two recursive calls to the function.
// one tick with optional label
void drawOneTick(int tickLength, int tickLabel = −1) {
for (int i = 0; i < tickLength; i++)
cout << "-";
if (tickLabel >= 0) cout << " " << tickLabel;
cout << "\n";
}
void drawTicks(int tickLength) {// draw ticks of given length
if (tickLength > 0) {// stop when length drops to 0
drawTicks(tickLength−1);// recursively draw left ticks
drawOneTick(tickLength);// draw center tick
drawTicks(tickLength−1);// recursively draw right ticks
}
}
void drawRuler(int nInches, int majorLength) {// draw the entire ruler
drawOneTick(majorLength, 0);// draw tick 0 and its label
for (int i = 1; i <= nInches; i++) {
drawTicks(majorLength−1);// draw ticks for this inch
drawOneTick(majorLength, i);// draw tick i and its label
}
}
Code Fragment 3.37: A recursive implementation of a function that draws a ruler.
Page 162
138Chapter 3. Arrays, Linked Lists, and Recursion
Illustrating Ruler Drawing using a Recursion Trace
The recursive execution of the recursive drawTicks function, defined above, can be
visualized using a recursion trace.
The trace for drawTicks is more complicated than in the factorial example,
however, because each instance makes two recursive calls. To illustrate this, we
show the recursion trace in a form that is reminiscent of an outline for a document.
See Figure 3.18.
Figure 3.18: A partial recursion trace for the call drawTicks(3). The second pattern
of calls for drawTicks(2) is not shown, but it is identical to the first.
Throughout this book, we see many other examples of how recursion can be
used in the design of data structures and algorithms.
Page 163
3.5. Recursion139
Further Illustrations of Recursion
As we discussed above, recursion is the concept of defining a function that makes
a call to itself. When a function calls itself, we refer to this as a recursive call. We
also consider a function M to be recursive if it calls another function that ultimately
leads to a call back to M.
The main benefit of a recursive approach to algorithm design is that it allows us
to take advantage of the repetitive structure present in many problems. By making
our algorithm description exploit this repetitive structure in a recursive way, we can
often avoid complex case analyses and nested loops. This approach can lead to
more readable algorithm descriptions, while still being quite efficient.
In addition, recursion is a useful way for defining objects that have a repeated
similar structural form, such as in the following examples.
Example 3.1: Modern operating systems define file-system directories (which are
also sometimes called “folders”) in a recursive way. Namely, a file system consists
of a top-level directory, and the contents of this directory consists of files and other
directories, which in turn can contain files and other directories, and so on. The
base directories in the file system contain only files, but by using this recursive
definition, the operating system allows for directories to be nested arbitrarily deep
(as long as there is enough space in memory).
Example 3.2: Much of the syntax in modern programming languages is defined
in a recursive way. For example, we can define an argument list in C++ using the
following notation:
argument-list:
argument
argument-list , argument
In other words, an argument list consists of either (i) an argument or (ii) an argu
ment list followed by a comma and an argument. That is, an argument list consists
of a comma-separated list of arguments. Similarly, arithmetic expressions can be
defined recursively in terms of primitives (like variables and constants) and arith
metic expressions.
Example 3.3: There are many examples of recursion in art and nature. One of the
most classic examples of recursion used in art is in the Russian Matryoshka dolls.
Each doll is made of solid wood or is hollow and contains another Matryoshka doll
inside it.
Page 164
140Chapter 3. Arrays, Linked Lists, and Recursion
3.5.1 Linear Recursion
The simplest form of recursion is linear recursion, where a function is defined
so that it makes at most one recursive call each time it is invoked. This type of
recursion is useful when we view an algorithmic problem in terms of a first or last
element plus a remaining set that has the same structure as the original set.
Summing the Elements of an Array Recursively
Suppose, for example, we are given an array, A, of n integers that we want to sum
together. We can solve this summation problem using linear recursion by observing
that the sum of all n integers in A is equal to A[0], if n = 1, or the sum of the first n−
1 integers in A plus the last element in A. In particular, we can solve this summation
problem using the recursive algorithm described in Code Fragment 3.38.
Algorithm LinearSum(A,n):
Input: A integer array A and an integer n ≥ 1, such that A has at least n elements
Output: The sum of the first n integers in A
if n = 1 then
return A[0]
else
return LinearSum(A,n−1)+ A[n−1]
Code Fragment 3.38: Summing the elements in an array using linear recursion.
This example also illustrates an important property that a recursive function
should always possess—the function terminates. We ensure this by writing a non
recursive statement for the case n = 1. In addition, we always perform the recursive
call on a smaller value of the parameter (n−1) than that which we are given (n), so
that, at some point (at the “bottom” of the recursion), we will perform the nonre
cursive part of the computation (returning A[0]). In general, an algorithm that uses
linear recursion typically has the following form:
• Test for base cases. We begin by testing for a set of base cases (there should
be at least one). These base cases should be defined so that every possible
chain of recursive calls eventually reaches a base case, and the handling of
each base case should not use recursion.
• Recur. After testing for base cases, we then perform a single recursive call.
This recursive step may involve a test that decides which of several possible
recursive calls to make, but it should ultimately choose to make just one of
these calls each time we perform this step. Moreover, we should define each
possible recursive call so that it makes progress towards a base case.
Page 165
3.5. Recursion141
Analyzing Recursive Algorithms using Recursion Traces
We can analyze a recursive algorithm by using a visual tool known as a recursion
trace. We used recursion traces, for example, to analyze and visualize the recur
sive factorial function of Section 3.5, and we similarly use recursion traces for the
recursive sorting algorithms of Sections 11.1 and 11.2.
To draw a recursion trace, we create a box for each instance of the function
and label it with the parameters of the function. Also, we visualize a recursive call
by drawing an arrow from the box of the calling function to the box of the called
function. For example, we illustrate the recursion trace of the LinearSum algorithm
of Code Fragment 3.38 in Figure 3.19. We label each box in this trace with the
parameters used to make this call. Each time we make a recursive call, we draw
a line to the box representing the recursive call. We can also use this diagram to
visualize stepping through the algorithm, since it proceeds by going from the call
for n to the call for n−1, to the call for n−2, and so on, all the way down to the call
for 1. When the final call finishes, it returns its value back to the call for 2, which
adds in its value, and returns this partial sum to the call for 3, and so on, until the
call for n−1 returns its partial sum to the call for n.
Figure 3.19: Recursion trace for an execution of LinearSum(A,n) with input param
eters A = {4,3,6,2,5} and n = 5.
From Figure 3.19, it should be clear that for an input array of size n, Algorithm
LinearSum makes n calls. Hence, it takes an amount of time that is roughly propor
tional to n, since it spends a constant amount of time performing the nonrecursive
part of each call. Moreover, we can also see that the memory space used by the
algorithm (in addition to the array A) is also roughly proportional to n, since we
need a constant amount of memory space for each of the n boxes in the trace at the
Page 166
142Chapter 3. Arrays, Linked Lists, and Recursion
time we make the final recursive call (for n = 1).
Reversing an Array by Recursion
Next, let us consider the problem of reversing the n elements of an array, A, so that
the first element becomes the last, the second element becomes second to the last,
and so on. We can solve this problem using linear recursion, by observing that the
reversal of an array can be achieved by swapping the first and last elements and
then recursively reversing the remaining elements in the array. We describe the
details of this algorithm in Code Fragment 3.39, using the convention that the first
time we call this algorithm we do so as ReverseArray(A,0,n−1).
Algorithm ReverseArray(A,i, j):
Input: An array A and nonnegative integer indices i and j
Output: The reversal of the elements in A starting at index i and ending at j
if i < j then
Swap A[i] and A[j]
ReverseArray(A,i+1, j −1)
return
Code Fragment 3.39: Reversing the elements of an array using linear recursion.
Note that, in this algorithm, we actually have two base cases, namely, when
i = j and when i > j. Moreover, in either case, we simply terminate the algorithm,
since a sequence with zero elements or one element is trivially equal to its reversal.
Furthermore, note that in the recursive step we are guaranteed to make progress
towards one of these two base cases. If n is odd, we eventually reach the i = j case,
and if n is even, we eventually reach the i > j case. The above argument immedi
ately implies that the recursive algorithm of Code Fragment 3.39 is guaranteed to
terminate.
Defining Problems in Ways that Facilitate Recursion
To design a recursive algorithm for a given problem, it is useful to think of the dif
ferent ways we can subdivide this problem to define problems that have the same
general structure as the original problem. This process sometimes means we need
to redefine the original problem to facilitate similar-looking subproblems. For ex
ample, with the ReverseArray algorithm, we added the parameters i and j so that a
recursive call to reverse the inner part of the array A would have the same structure
(and same syntax) as the call to reverse all of A. Then, rather than initially calling
the algorithm as ReverseArray(A), we call it initially as ReverseArray(A,0,n−1).
In general, if one has difficulty finding the repetitive structure needed to design a re
cursive algorithm, it is sometimes useful to work out the problem on a few concrete
examples to see how the subproblems should be defined.
Page 167
3.5. Recursion143
Tail Recursion
Using recursion can often be a useful tool for designing algorithms that have ele
gant, short definitions. But this usefulness does come at a modest cost. When we
use a recursive algorithm to solve a problem, we have to use some of the memory
locations in our computer to keep track of the state of each active recursive call.
When computer memory is at a premium, then it is useful in some cases to be able
to derive nonrecursive algorithms from recursive ones.
We can use the stack data structure, discussed in Section 5.1, to convert a recur
sive algorithm into a nonrecursive algorithm, but there are some instances when we
can do this conversion more easily and efficiently. Specifically, we can easily con
vert algorithms that use tail recursion. An algorithm uses tail recursion if it uses
linear recursion and the algorithm makes a recursive call as its very last operation.
For example, the algorithm of Code Fragment 3.39 uses tail recursion to reverse
the elements of an array.
It is not enough that the last statement in the function definition includes a
recursive call, however. In order for a function to use tail recursion, the recursive
call must be absolutely the last thing the function does (unless we are in a base case,
of course). For example, the algorithm of Code Fragment 3.38 does not use tail
recursion, even though its last statement includes a recursive call. This recursive
call is not actually the last thing the function does. After it receives the value
returned from the recursive call, it adds this value to A[n−1] and returns this sum.
That is, the last thing this algorithm does is an add, not a recursive call.
When an algorithm uses tail recursion, we can convert the recursive algorithm
into a nonrecursive one, by iterating through the recursive calls rather than call
ing them explicitly. We illustrate this type of conversion by revisiting the prob
lem of reversing the elements of an array. In Code Fragment 3.40, we give a
nonrecursive algorithm that performs this task by iterating through the recursive
calls of the algorithm of Code Fragment 3.39. We initially call this algorithm as
IterativeReverseArray(A,0,n−1).
Algorithm IterativeReverseArray(A,i, j):
Input: An array A and nonnegative integer indices i and j
Output: The reversal of the elements in A starting at index i and ending at j
while i < j do
Swap A[i] and A[ j]
i ← i+ 1
j ← j −1
return
Code Fragment 3.40: Reversing the elements of an array using iteration.
Page 168
144Chapter 3. Arrays, Linked Lists, and Recursion
3.5.2 Binary Recursion
When an algorithm makes two recursive calls, we say that it uses binary recursion.
These calls can, for example, be used to solve two similar halves of some problem,
as we did in Section 3.5 for drawing an English ruler. As another application of
binary recursion, let us revisit the problem of summing the n elements of an integer
array A. In this case, we can sum the elements in A by: (i) recursively summing the
elements in the first half of A; (ii) recursively summing the elements in the second
half of A; and (iii) adding these two values together. We give the details in the
algorithm of Code Fragment 3.41, which we initially call as BinarySum(A,0,n).
Algorithm BinarySum(A,i,n):
Input: An array A and integers i and n
Output: The sum of the n integers in A starting at index i
if n = 1 then
return A[i]
return BinarySum(A,i,⌈n/2⌉) + BinarySum(A,i + ⌈n/2⌉,⌊n/2⌋)
Code Fragment 3.41: Summing the elements in an array using binary recursion.
To analyze Algorithm BinarySum, we consider, for simplicity, the case where
n is a power of two. The general case of arbitrary n is considered in Exercise R-4.5.
Figure 3.20 shows the recursion trace of an execution of function BinarySum(0,8).
We label each box with the values of parameters i and n, which represent the start
ing index and length of the sequence of elements to be summed, respectively. No
tice that the arrows in the trace go from a box labeled (i,n) to another box labeled
(i,n/2) or (i + n/2,n/2). That is, the value of parameter n is halved at each recur
sive call. Thus, the depth of the recursion, that is, the maximum number of function
instances that are active at the same time, is 1 + log2 n. Thus, Algorithm Binary
Sum uses an amount of additional space roughly proportional to this value. This is
a big improvement over the space needed by the LinearSum function of Code Frag
ment 3.38. The running time of Algorithm BinarySum is still roughly proportional
to n, however, since each box is visited in constant time when stepping through our
algorithm and there are 2n − 1 boxes.
Figure 3.20: Recursion trace for the execution of BinarySum(0,8).
Page 169
3.5. Recursion145
Computing Fibonacci Numbers via Binary Recursion
Let us consider the problem of computing the kth Fibonacci number. Recall from
Section 2.2.3, that the Fibonacci numbers are recursively defined as follows:
F0 = 0
F1 = 1
Fi = Fi−1 +Fi−2 for i > 1
By directly applying this definition, Algorithm BinaryFib, shown in Code Frag
ment 3.42, computes the sequence of Fibonacci numbers using binary recursion.
Algorithm BinaryFib(k):
Input: Nonnegative integer k
Output: The kth Fibonacci number Fk
if k ≤ 1 then
return k
else
return BinaryFib(k−1) + BinaryFib(k−2)
Code Fragment 3.42: Computing the kth Fibonacci number using binary recursion.
Unfortunately, in spite of the Fibonacci definition looking like a binary recur
sion, using this technique is inefficient in this case. In fact, it takes an exponential
number of calls to compute the kth Fibonacci number in this way. Specifically, let
nk denote the number of calls performed in the execution of BinaryFib(k). Then,
we have the following values for the nk’s:
n0 = 1
n1 = 1
n2 = n1 +n0 +1 = 1+1+1 = 3
n3 = n2 +n1 +1 = 3+1+1 = 5
n4 = n3 +n2 +1 = 5+3+1 = 9
n5 = n4 +n3 +1 = 9+5+1 = 15
n6 = n5 +n4 +1 = 15+9+1 = 25
n7 = n6 +n5 +1 = 25+15+1 = 41
n8 = n7 +n6 +1 = 41+25+1 = 67
If we follow the pattern forward, we see that the number of calls more than doubles
for each two consecutive indices. That is, n4 is more than twice n2, n5 is more than
twice n3, n6 is more than twice n4, and so on. Thus, nk > 2k/2, which means that
BinaryFib(k) makes a number of calls that are exponential in k. In other words,
using binary recursion to compute Fibonacci numbers is very inefficient.
Page 170
146Chapter 3. Arrays, Linked Lists, and Recursion
Computing Fibonacci Numbers via Linear Recursion
The main problem with the approach above, based on binary recursion, is that the
computation of Fibonacci numbers is really a linearly recursive problem. It is not
a good candidate for using binary recursion. We simply got tempted into using
binary recursion because of the way the kth Fibonacci number, Fk, depends on the
two previous values, Fk−1 and Fk−2. But we can compute Fk much more efficiently
using linear recursion.
In order to use linear recursion, however, we need to slightly redefine the prob
lem. One way to accomplish this conversion is to define a recursive function that
computes a pair of consecutive Fibonacci numbers (Fk,Fk−1) using the convention
F−1 = 0. Then we can use the linearly recursive algorithm shown in Code Frag
ment 3.43.
Algorithm LinearFibonacci(k):
Input: A nonnegative integer k
Output: Pair of Fibonacci numbers (Fk,Fk−1)
if k ≤ 1 then
return (k,0)
else
(i, j) ← LinearFibonacci(k−1)
return (i + j,i)
Code Fragment 3.43: Computing the kth Fibonacci number using linear recursion.
The algorithm given in Code Fragment 3.43 shows that using linear recursion
to compute Fibonacci numbers is much more efficient than using binary recursion.
Since each recursive call to LinearFibonacci decreases the argument k by 1, the
original call LinearFibonacci(k) results in a series of k − 1 additional calls. That
is, computing the kth Fibonacci number via linear recursion requires k function
calls. This performance is significantly faster than the exponential time needed by
the algorithm based on binary recursion, which was given in Code Fragment 3.42.
Therefore, when using binary recursion, we should first try to fully partition the
problem in two (as we did for summing the elements of an array) or we should be
sure that overlapping recursive calls are really necessary.
Usually, we can eliminate overlapping recursive calls by using more memory to
keep track of previous values. In fact, this approach is a central part of a technique
called dynamic programming, which is related to recursion and is discussed in
Section 12.2.
Page 171
3.5. Recursion147
3.5.3 Multiple Recursion
Generalizing from binary recursion, we use multiple recursion when a function
may make multiple recursive calls, with that number potentially being more than
two. One of the most common applications of this type of recursion is used when
we want to enumerate various configurations in order to solve a combinatorial puz
zle. For example, the following are all instances of summation puzzles.
pot + pan = bib
dog + cat = pig
boy+ girl = baby
To solve such a puzzle, we need to assign a unique digit (that is, 0,1,...,9) to each
letter in the equation, in order to make the equation true. Typically, we solve such
a puzzle by using our human observations of the particular puzzle we are trying
to solve to eliminate configurations (that is, possible partial assignments of digits
to letters) until we can work though the feasible configurations left, testing for the
correctness of each one.
If the number of possible configurations is not too large, however, we can use
a computer to simply enumerate all the possibilities and test each one, without
employing any human observations. In addition, such an algorithm can use multiple
recursion to work through the configurations in a systematic way. We show pseudo
code for such an algorithm in Code Fragment 3.44. To keep the description general
enough to be used with other puzzles, the algorithm enumerates and tests all k
length sequences without repetitions of the elements of a given set U. We build the
sequences of k elements by the following steps:
1. Recursively generating the sequences of k−1 elements
2. Appending to each such sequence an element not already contained in it.
Throughout the execution of the algorithm, we use the set U to keep track of the
elements not contained in the current sequence, so that an element e has not been
used yet if and only if e is in U.
Another way to look at the algorithm of Code Fragment 3.44 is that it enumer
ates every possible size-k ordered subset of U, and tests each subset for being a
possible solution to our puzzle.
For summation puzzles, U = {0,1,2,3,4,5,6,7,8,9} and each position in the
sequence corresponds to a given letter. For example, the first position could stand
for b, the second for o, the third for y, and so on.
Page 172
148Chapter 3. Arrays, Linked Lists, and Recursion
Algorithm PuzzleSolve(k,S,U):
Input: An integer k, sequence S, and set U
Output: An enumeration of all k-length extensions to S using elements in U
without repetitions
for each e in U do
Remove e from U {e is now being used}
Add e to the end of S
if k = 1 then
Test whether S is a configuration that solves the puzzle
if S solves the puzzle then
return “Solution found: ” S
else
PuzzleSolve(k − 1,S,U)
Add e back to U {e is now unused}
Remove e from the end of S
Code Fragment 3.44: Solving a combinatorial puzzle by enumerating and testing
all possible configurations.
In Figure 3.21, we show a recursion trace of a call to PuzzleSolve(3,S,U),
where S is empty and U = {a,b,c}. During the execution, all the permutations
of the three characters are generated and tested. Note that the initial call makes
three recursive calls, each of which in turn makes two more. If we had executed
PuzzleSolve(3,S,U) on a set U consisting of four elements, the initial call would
have made four recursive calls, each of which would have a trace looking like the
one in Figure 3.21.
Figure 3.21: Recursion trace for an execution of PuzzleSolve(3,S,U), where S is
empty and U = {a,b,c}. This execution generates and tests all permutations of a, b,
and c. We show the permutations generated directly below their respective boxes
Page 217
Chapter
5 Stacks, Queues, and Deques
Contents
5.1 Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
5.1.1 The Stack Abstract Data Type . . . . . . . . . . . . 195
5.1.2 The STL Stack . . . . . . . . . . . . . . . . . . . . 196
5.1.3 A C++ Stack Interface . . . . . . . . . . . . . . . . 196
5.1.4 A Simple Array-Based Stack Implementation . . . . 198
5.1.5 Implementing a Stack with a Generic Linked List . . 202
5.1.6 Reversing a Vector Using a Stack . . . . . . . . . . . 203
5.1.7 Matching Parentheses and HTML Tags . . . . . . . 204
5.2 Queues . . . . . . . . . . . . . . . . . . . . . . . . . . 208
5.2.1 The Queue Abstract Data Type . . . . . . . . . . . 208
5.2.2 The STL Queue . . . . . . . . . . . . . . . . . . . . 209
5.2.3 A C++ Queue Interface . . . . . . . . . . . . . . . . 210
5.2.4 A Simple Array-Based Implementation . . . . . . . . 211
5.2.5 Implementing a Queue with a Circularly Linked List . 213
5.3 Double-Ended Queues . . . . . . . . . . . . . . . . . . 217
5.3.1 The Deque Abstract Data Type . . . . . . . . . . . 217
5.3.2 The STL Deque . . . . . . . . . . . . . . . . . . . . 218
5.3.3 Implementing a Deque with a Doubly Linked List . . 218
5.3.4 Adapters and the Adapter Design Pattern . . . . . . 220
5.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 223
Page 218
194Chapter 5. Stacks, Queues, and Deques
5.1 Stacks
A stack is a container of objects that are inserted and removed according to the last
in first-out (LIFO) principle. Objects can be inserted into a stack at any time, but
only the most recently inserted (that is, “last”) object can be removed at any time.
The name “stack” is derived from the metaphor of a stack of plates in a spring
loaded, cafeteria plate dispenser. In this case, the fundamental operations involve
the “pushing” and “popping” of plates on the stack. When we need a new plate
from the dispenser, we “pop” the top plate off the stack, and when we add a plate,
we “push” it down on the stack to become the new top plate. Perhaps an even more
amusing metaphor would be a PEZ©R candy dispenser, which stores mint candies
in a spring-loaded container that “pops” out the top-most candy in the stack when
the top of the dispenser is lifted. (See Figure 5.1.) Stacks are a fundamental data
structure. They are used in many applications, including the following.
Example 5.1: Internet Web browsers store the addresses of recently visited sites
on a stack. Each time a user visits a new site, that site’s address is “pushed” onto the
stack of addresses. The browser then allows the user to “pop” back to previously
visited sites using the “back” button.
Example 5.2: Text editors usually provide an “undo” mechanism that cancels re
cent editing operations and reverts to former states of a document. This undo oper
ation can be accomplished by keeping text changes in a stack.
Figure 5.1: A schematic drawing of a PEZ©R dispenser; a physical implementation
of the stack ADT. (PEZ©R is a registered trademark of PEZ Candy, Inc.)
Page 219
5.1. Stacks195
5.1.1 The Stack Abstract Data Type
Stacks are the simplest of all data structures, yet they are also among the most
important, since they are used in a host of different applications that include many
more sophisticated data structures. Formally, a stack is an abstract data type (ADT)
that supports the following operations:
push(e): Insert element e at the top of the stack.
pop(): Remove the top element from the stack; an error occurs
if the stack is empty.
top(): Return a reference to the top element on the stack, with
out removing it; an error occurs if the stack is empty.
Additionally, let us also define the following supporting functions:
size(): Return the number of elements in the stack.
empty(): Return true if the stack is empty and false otherwise.
Example 5.3: The following table shows a series of stack operations and their
effects on an initially empty stack of integers.
Operation Output Stack Contents
push(5)–(5)
push(3)–(5,3)
pop()–(5)
push(7)–(5,7)
pop()–(5)
top()5(5)
pop()–()
pop()“error” ()
top()“error” ()
empty()true ()
push(9)–(9)
push(7)–(9,7)
push(3)–(9,7,3)
push(5)–(9,7,3,5)
size()4(9,7,3,5)
pop()–(9,7,3)
push(8)–(9,7,3,8)
pop()–(9,7,3)
top()3(9,7,3)
Page 220
196Chapter 5. Stacks, Queues, and Deques
5.1.2 The STL Stack
The Standard Template Library provides an implementation of a stack. The un
derlying implementation is based on the STL vector class, which is presented in
Sections 1.5.5 and 6.1.4. In order to declare an object of type stack, it is neces
sary to first include the definition file, which is called “stack.” As with the STL
vector, the class stack is part of the std namespace, so it is necessary either to use
“std::stack” or to provide a “using” statement. The stack class is templated with
the class of the individual elements. For example, the code fragment below declares
a stack of integers.
#include <stack>
using std::stack;// make stack accessible
stack<int> myStack;// a stack of integers
We refer to the type of individual elements as the stack’s base type. As with STL
vectors, an STL stack dynamically resizes itself as new elements are pushed on.
The STL stack class supports the same operators as our interface. Below, we
list the principal member functions. Let s be declared to be an STL vector, and let
e denote a single object whose type is the same as the base type of the stack. (For
example, s is a vector of integers, and e is an integer.)
size(): Return the number of elements in the stack.
empty(): Return true if the stack is empty and false otherwise.
push(e): Push e onto the top of the stack.
pop(): Pop the element at the top of the stack.
top(): Return a reference to the element at the top of the stack.
There is one significant difference between the STL implementation and our
own definitions of the stack operations. In the STL implementation, the result of
applying either of the operations top or pop to an empty stack is undefined. In
particular, no exception is thrown. Even though no exception is thrown, it may
very likely result in your program aborting. Thus, it is up to the programmer to be
sure that no such illegal accesses are attempted.
5.1.3 A C++ Stack Interface
Before discussing specific implementations of the stack, let us first consider how
to define an abstract data type for a stack. When defining an abstract data type,
our principal concern is specifying the Application Programming Interface (API),
or simply interface, which describes the names of the public members that the
ADT must support and how they are to be declared and used. An interface is not
a complete description of all the public members. For example, it does not include
Page 221
5.1. Stacks197
the private data members. Rather, it is a list of members that any implementation
must provide. The C++ programming language does not provide a simple method
for defining interfaces, and therefore, the interface defined here is not an official
C++ class. It is offered principally for the purpose of illustration.
The informal interface for the stack ADT is given in Code Fragment 5.1. This
interface defines a class template. Recall from Section 2.3 that such a definition
implies that the base type of element being stored in the stack will be provided by
the user. In Code Fragment 5.1, this element type is indicated by E. For example,
E may be any fundamental type (such as int, char, bool, and double), any built-in
or user-defined class (such as string), or a pointer to any of these.
template <typename E>
class Stack {// an interface for a stack
public:
int size() const;// number of items in stack
bool empty() const;// is the stack empty?
const E& top() const throw(StackEmpty);// the top element
void push(const E& e);// push x onto the stack
void pop() throw(StackEmpty);// remove the top element
};
Code Fragment 5.1: An informal Stack interface (not a complete C++ class).
Observe that the member functions size, empty, and top are all declared to be
const, which informs the compiler that they do not alter the contents of the stack.
The member function top returns a constant reference to the top of the stack, which
means that its value may be read but not written.
Note that pop does not return the element that was popped. If the user wants to
know this value, it is necessary to perform a top operation first, and save the value.
The member function push takes a constant reference to an object of type E as its
argument. Recall from Section 1.4 that this is the most efficient way of passing
objects to a function.
An error condition occurs when calling either of the functions pop or top on an
empty stack. This is signaled by throwing an exception of type StackEmpty, which
is defined in Code Fragment 5.2.
// Exception thrown on performing top or pop of an empty stack.
class StackEmpty : public RuntimeException {
public:
StackEmpty(const string& err) : RuntimeException(err) {}
};
Code Fragment 5.2: Exception thrown by functions pop and top when called on an
empty stack. This class is derived from RuntimeException from Section 2.4.
Page 222
198Chapter 5. Stacks, Queues, and Deques
5.1.4 A Simple Array-Based Stack Implementation
We can implement a stack by storing its elements in an array. Specifically, the stack
in this implementation consists of an N-element array S plus an integer variable t
that gives the index of the top element in array S. (See Figure 5.2.)
0 1 2t N−1S
Figure 5.2: Realization of a stack by means of an array S. The top element in the
stack is stored in the cell S[t].
Recalling that arrays in C++ start at index 0, we initialize t to −1, and use this
value for t to identify when the stack is empty. Likewise, we can use this variable
to determine the number of elements in a stack (t + 1). We also introduce a new
type of exception, called StackFull, to signal the error condition that arises if we
try to insert a new element and the array S is full. Exception StackFull is specific to
our implementation of a stack and is not defined in the stack ADT. Given this new
exception, we can then implement the stack ADT functions as described in Code
Fragment 5.3.
Algorithm size():
return t + 1
Algorithm empty():
return (t < 0)
Algorithm top():
if empty() then
throw StackEmpty exception
return S[t]
Algorithm push(e):
if size() = N then
throw StackFull exception
t ← t + 1
S[t] ← e
Algorithm pop():
if empty() then
throw StackEmpty exception
t ← t −1
Code Fragment 5.3: Implementation of a stack by means of an array.
The correctness of the functions in the array-based implementation follows im
mediately from the definition of the functions themselves. Table 5.1 shows the
Page 223
5.1. Stacks199
running times for member functions in a realization of a stack by an array. Each
of the stack functions in the array realization executes a constant number of state
ments involving arithmetic operations, comparisons, and assignments. Thus, in this
implementation of the Stack ADT, each function runs in constant time, that is, they
each run in O(1) time.
Operation Time
size O(1)
empty O(1)
top O(1)
push O(1)
pop O(1)
Table 5.1: Performance of an array-based stack. The space usage is O(N), where N
is the array’s size. Note that the space usage is independent from the number n ≤ N
of elements that are actually in the stack.
A C++ Implementation of a Stack
In this section, we present a concrete C++ implementation of the above pseudo
code specification by means of a class, called ArrayStack. Our approach is to store
the elements of a stack in an array. To keep the code simple, we have omitted the
standard housekeeping utilities, such as a destructor, an assignment operator, and a
copy constructor. We leave their implementations as an exercise.
We begin by providing the ArrayStack class definition in Code Fragment 5.4.
template <typename E>
class ArrayStack {
enum { DEF CAPACITY = 100 };// default stack capacity
public:
ArrayStack(int cap = DEF CAPACITY); // constructor from capacity
int size() const;// number of items in the stack
bool empty() const;// is the stack empty?
const E& top() const throw(StackEmpty); // get the top element
void push(const E& e) throw(StackFull); // push element onto stack
void pop() throw(StackEmpty);// pop the stack
// . . .housekeeping functions omitted
private:// member data
E* S;// array of stack elements
int capacity;// stack capacity
int t;// index of the top of the stack
};
Code Fragment 5.4: The class ArrayStack, which implements the Stack interface.
Page 224
200Chapter 5. Stacks, Queues, and Deques
In addition to the member functions required by the interface, we also provide
a constructor, that is given the desired capacity of the stack as its only argument. If
no argument is given, the default value given by DEF CAPACITY is used. This is
an example of using default arguments in function calls. We use an enumeration
to define this default capacity value. This is the simplest way of defining symbolic
integer constants within a C++ class. Our class is templated with the element type,
denoted by E. The stack’s storage, denoted S, is a dynamically allocated array of
type E, that is, a pointer to E.
Next, we present the implementations of the ArrayStack member functions in
Code Fragment 5.5. The constructor allocates the array storage, whose size is set to
the default capacity. The members capacity and t are also set to their initial values.
In spite of the syntactical complexities of defining templated member functions in
C++, the remaining member functions are straightforward implementations of their
definitions in Code 5.3. Observe that functions top and pop first check that the
stack is not empty, and otherwise, they throw an exception. Similarly, push first
checks that the stack is not full, and otherwise, it throws an exception.
template <typename E> ArrayStack<E>::ArrayStack(int cap)
: S(new E[cap]), capacity(cap), t(−1) { } // constructor from capacity
template <typename E> int ArrayStack<E>::size() const
{ return (t + 1); }// number of items in the stack
template <typename E> bool ArrayStack<E>::empty() const
{ return (t < 0); }// is the stack empty?
template <typename E>// return top of stack
const E& ArrayStack<E>::top() const throw(StackEmpty) {
if (empty()) throw StackEmpty("Top of empty stack");
return S[t];
}
template <typename E>// push element onto the stack
void ArrayStack<E>::push(const E& e) throw(StackFull) {
if (size() == capacity) throw StackFull("Push to full stack");
S[++t] = e;
}
template <typename E>// pop the stack
void ArrayStack<E>::pop() throw(StackEmpty) {
if (empty()) throw StackEmpty("Pop from empty stack");
−−t;
}
Code Fragment 5.5: Implementations of the member functions of class ArrayStack
(excluding housekeeping functions).
Page 225
5.1. Stacks201
Example Output
In Code Fragment 5.6 below, we present an example of the use of our ArrayStack
class. To demonstrate the flexibility of our implementation, we show two stacks of
different base types. The instance A is a stack of integers of the default capacity
(100). The instance B is a stack of character strings of capacity 10.
ArrayStack<int> A;// A = [ ], size = 0
A.push(7);// A = [7*], size = 1
A.push(13);// A = [7, 13*], size = 2
cout << A.top() << endl; A.pop();// A = [7*], outputs: 13
A.push(9);// A = [7, 9*], size = 2
cout << A.top() << endl;// A = [7, 9*], outputs: 9
cout << A.top() << endl; A.pop();// A = [7*], outputs: 9
ArrayStack<string> B(10);// B = [ ], size = 0
B.push("Bob");// B = [Bob*], size = 1
B.push("Alice");// B = [Bob, Alice*], size = 2
cout << B.top() << endl; B.pop();// B = [Bob*], outputs: Alice
B.push("Eve");// B = [Bob, Eve*], size = 2
Code Fragment 5.6: An example of the use of the ArrayStack class. The contents
of the stack are shown in the comment following the operation. The top of the stack
is indicated by an asterisk (“*”).
Note that our implementation, while simple and efficient, could be enhanced
in a number of ways. For example, it assumes a fixed upper bound N on the ulti
mate size of the stack. In Code Fragment 5.4, we chose the default capacity value
N = 100 more or less arbitrarily (although the user can set the capacity in the con
structor). An application may actually need much less space than the given initial
size, and this would be wasteful of memory. Alternatively, an application may need
more space than this, in which case our stack implementation might “crash” if too
many elements are pushed onto the stack.
Fortunately, there are other implementations that do not impose an arbitrary size
limitation. One such method is to use the STL stack class, which was introduced
earlier in this chapter. The STL stack is also based on the STL vector class, and
it offers the advantage that it is automatically expanded when the stack overflows
its current storage limits. In practice, the STL stack would be the easiest and most
practical way to implement an array-based stack. Later in this chapter, we see other
methods that use space proportional to the actual size of the stack.
In instances where we have a good estimate on the number of items needing to
go in the stack, the array-based implementation is hard to beat from the perspective
of speed and simplicity. Stacks serve a vital role in a number of computing applica
tions, so it is helpful to have a fast stack ADT implementation, such as the simple
array-based implementation.
Page 226
202Chapter 5. Stacks, Queues, and Deques
5.1.5 Implementing a Stack with a Generic Linked List
In this section, we show how to implement the stack ADT using a singly linked list.
Our approach is to use the generic singly linked list, called SLinkedList, which was
presented earlier in Section 3.2.4. The definition of our stack, called LinkedStack,
is presented in Code Fragment 5.7.
To avoid the syntactic messiness inherent in C++ templated classes, we have
chosen not to implement a fully generic templated class. Instead, we have opted to
define a type for the stack’s elements, called Elem. In this example, we define Elem
to be of type string. We leave the task of producing a truly generic implementation
as an exercise. (See Exercise R-5.7.)
typedef string Elem;// stack element type
class LinkedStack {// stack as a linked list
public:
LinkedStack();// constructor
int size() const;// number of items in the stack
bool empty() const;// is the stack empty?
const Elem& top() const throw(StackEmpty); // the top element
void push(const Elem& e);// push element onto stack
void pop() throw(StackEmpty);// pop the stack
private:// member data
SLinkedList<Elem> S;// linked list of elements
int n;// number of elements
};
Code Fragment 5.7: The class LinkedStack, a linked list implementation of a stack.
The principal data member of the class is the generic linked list of type Elem,
called S. Since the SLinkedList class does not provide a member function size, we
store the current size in a member variable, n.
In Code Fragment 5.8, we present the implementations of the constructor and
the size and empty functions. Our constructor creates the initial stack and initial
izes n to zero. We do not provide an explicit destructor, relying instead on the
SLinkedList destructor to deallocate the linked list S.
LinkedStack::LinkedStack()
: S(), n(0) { }// constructor
int LinkedStack::size() const
{ return n; }// number of items in the stack
bool LinkedStack::empty() const
{ return n == 0; }// is the stack empty?
Code Fragment 5.8: Constructor and size functions for the LinkedStack class.
Page 227
5.1. Stacks203
The definitions of the stack operations, top, push, and pop, are presented in
Code Fragment 5.9. Which side of the list, head or tail, should we chose for the top
of the stack? Since SLinkedList can insert and delete elements in constant time only
at the head, the head is clearly the better choice. Therefore, the member function
top returns S.front(). The functions push and pop invoke the functions addFront
and removeFront, respectively, and update the number of elements.
// get the top element
const Elem& LinkedStack::top() const throw(StackEmpty) {
if (empty()) throw StackEmpty("Top of empty stack");
return S.front();
}
void LinkedStack::push(const Elem& e) { // push element onto stack
++n;
S.addFront(e);
}
// pop the stack
void LinkedStack::pop() throw(StackEmpty) {
if (empty()) throw StackEmpty("Pop from empty stack");
−−n;
S.removeFront();
}
Code Fragment 5.9: Principal operations for the LinkedStack class.
5.1.6 Reversing a Vector Using a Stack
We can use a stack to reverse the elements in a vector, thereby producing a nonre
cursive algorithm for the array-reversal problem introduced in Section 3.5.1. The
basic idea is to push all the elements of the vector in order into a stack and then
fill the vector back up again by popping the elements off of the stack. In Code
Fragment 5.10, we give a C++ implementation of this algorithm.
template <typename E>
void reverse(vector<E>& V) {// reverse a vector
ArrayStack<E> S(V.size());
for (int i = 0; i < V.size(); i++)// push elements onto stack
S.push(V[i]);
for (int i = 0; i < V.size(); i++) {// pop them in reverse order
V[i] = S.top(); S.pop();
}
}
Code Fragment 5.10: A generic function that uses a stack to reverse a vector.
For example, if the input vector to function reverse contained the five strings
[Jack, Kate, Hurley, Jin, Michael], then on returning from the function, the vector
would contain [Michael, Jin, Hurley, Kate, Jack].
Page 228
204Chapter 5. Stacks, Queues, and Deques
5.1.7 Matching Parentheses and HTML Tags
In this section, we explore two related applications of stacks. The first is matching
parentheses and grouping symbols in arithmetic expressions. Arithmetic expres
sions can contain various pairs of grouping symbols, such as
• Parentheses: “(” and “)”
• Braces: “{” and “}”
• Brackets: “[” and “]”
• Floor function symbols: “⌊” and “⌋”
• Ceiling function symbols: “⌈” and “⌉,”
and each opening symbol must match with its corresponding closing symbol. For
example, a left bracket symbol (“[”) must match with a corresponding right bracket
(“]”) as in the following expression:
• Correct: ()(()){([()])}
• Correct: ((()(()){([()])}))
• Incorrect: )(()){([()])}
• Incorrect: ({[])}
• Incorrect: (
We leave the precise definition of matching of grouping symbols to Exercise R-5.8.
An Algorithm for Parentheses Matching
An important problem in processing arithmetic expressions is to make sure their
grouping symbols match up correctly. We can use a stack S to perform the matching
of grouping symbols in an arithmetic expression with a single left-to-right scan.
The algorithm tests that left and right symbols match up and also that the left and
right symbols are both of the same type.
Suppose we are given a sequence X = x0x1x2...xn−1, where each xi is a token
that can be a grouping symbol, a variable name, an arithmetic operator, or a number.
The basic idea behind checking that the grouping symbols in S match correctly, is
to process the tokens in X in order. Each time we encounter an opening symbol, we
push that symbol onto S, and each time we encounter a closing symbol, we pop the
top symbol from the stack S (assuming S is not empty) and we check that these two
symbols are of corresponding types. (For example, if the symbol “(” was pushed,
the symbol “)” should be its match.) If the stack is empty after we have processed
the whole sequence, then the symbols in X match.
Assuming that the push and pop operations are implemented to run in constant
time, this algorithm runs in O(n) total time. We give a pseudo-code description of
this algorithm in Code Fragment 5.11.
Page 229
5.1. Stacks205
Algorithm ParenMatch(X,n):
Input: An array X of n tokens, each of which is either a grouping symbol, a
variable, an arithmetic operator, or a number
Output: true if and only if all the grouping symbols in X match
Let S be an empty stack
for i ← 0 to n−1 do
if X[i] is an opening grouping symbol then
S.push(X[i])
else if X[i] is a closing grouping symbol then
if S.empty() then
return false{nothing to match with}
if S.top() does not match the type of X[i] then
return false{wrong type}
S.pop()
if S.empty() then
return true{every symbol matched}
else
return false{some symbols were never matched}
Code Fragment 5.11: Algorithm for matching grouping symbols in an arithmetic
expression.
Matching Tags in an HTML Document
Another application in which matching is important is in the validation of HTML
documents. HTML is the standard format for hyperlinked documents on the In
ternet. In an HTML document, portions of text are delimited by HTML tags. A
simple opening HTML tag has the form “<name>” and the corresponding closing
tag has the form “</name>.” Commonly used HTML tags include:
• body: document body
• h1: section header
• center: center justify
• p: paragraph
• ol: numbered (ordered) list
• li: list item
We show a sample HTML document and a possible rendering in Figure 5.3. Our
goal is to write a program to check that the tags properly match.
A very similar approach to that given in Code Fragment 5.11 can be used to
match the tags in an HTML document. We push each opening tag on a stack, and
when we encounter a closing tag, we pop the stack and verify that the two tags
match.
Page 230
206Chapter 5. Stacks, Queues, and Deques
<body>
<center>
<h1> The Little Boat </h1>
</center>
<p> The storm tossed the little
boat like a cheap sneaker in an
old washing machine. The three
drunken fishermen were used to
such treatment, of course, but
not the tree salesman, who even
as a stowaway now felt that he
had overpaid for the voyage. </p>
<ol>
<li> Will the salesman die? </li>
<li> What color is the boat? </li>
<li> And what about Naomi? </li>
</ol>
</body>The Little Boat
The storm tossed the little boat
like a cheap sneaker in an old
washing machine. The three
drunken fishermen were used to
such treatment, of course, but not
the tree salesman, who even as
a stowaway now felt that he had
overpaid for the voyage.
1. Will the salesman die?
2. What color is the boat?
3. And what about Naomi?
(a)(b)
Figure 5.3: HTML tags: (a) an HTML document; (b) its rendering.
In Code Fragments 5.12 through 5.14, we present a C++ program for matching
tags in an HTML document read from the standard input stream. For simplicity,
we assume that all tags are syntactically well formed.
First, the procedure getHtmlTags reads the input line by line, extracts all the
tags as strings, and stores them in a vector, which it returns.
vector<string> getHtmlTags() {// store tags in a vector
vector<string> tags;// vector of html tags
while (cin) {// read until end of file
string line;
getline(cin, line);// input a full line of text
int pos = 0;// current scan position
int ts = line.find("<", pos);// possible tag start
while (ts != string::npos) {// repeat until end of string
int te = line.find(">", ts+1);// scan for tag end
tags.push back(line.substr(ts, te−ts+1)); // append tag to the vector
pos = te + 1;// advance our position
ts = line.find("<", pos);
}
}
return tags;// return vector of tags
}
Code Fragment 5.12: Get a vector of HTML tags from the input, and store them in
a vector of strings.
Page 231
5.1. Stacks207
Given the example shown in Figure 5.3(a), this procedure would return the
following vector:
<body>, <center>, <h1>, </h1>, </center>, . . . , </body>
In Code Fragment 5.12, we employ a variable pos, which maintains the current
position in the input line. We use the built-in string member function find to locate
the first occurrence of “<” that follows the current position. (Recall the discussion
of string operations from Section 1.5.5.) This tag start position is stored in the
variable ts. We then find the next occurrence of “>,” and store this tag end position
in te. The tag itself consists of the substring of length te−ts+1 starting at position
ts. This is pushed onto the vector tags. We then update the current position to be
te+1 and repeat until we find no further occurrences of “<.” This occurs when the
find function returns the special value string::npos.
Next, the procedure isHtmlMatched, shown in Code Fragments 5.12, imple
ments the process of matching the tags.
// check for matching tags
bool isHtmlMatched(const vector<string>& tags) {
LinkedStack S;// stack for opening tags
typedef vector<string>::const iterator Iter;// iterator type
// iterate through vector
for (Iter p = tags.begin(); p != tags.end(); ++p) {
if (p−>at(1) != ’/’)// opening tag?
S.push(*p);// push it on the stack
else {// else must be closing tag
if (S.empty()) return false;// nothing to match - failure
string open = S.top().substr(1);// opening tag excluding ’<’
string close = p−>substr(2);// closing tag excluding ’</’
if (open.compare(close) != 0) return false; // fail to match
else S.pop();// pop matched element
}
}
if (S.empty()) return true;// everything matched - good
else return false;// some unmatched - bad
}
Code Fragment 5.13: Check whether HTML tags stored in the vector tags are
matched.
We create a stack, called S, in which we store the opening tags. We then iterate
through the vector of tags. If the second character tag string is not “/,” then this is
an opening tag, and it is pushed onto the stack. Otherwise, it is a closing tag, and
we check that it matches the tag on top of the stack. To compare the opening and
closing tags, we use the string substr member function to strip the first character off
the opening tag (thus removing the “<”) and the first two characters off the closing
tag (thus removing the “</”). We check that these two substrings are equal, using
Page 232
208Chapter 5. Stacks, Queues, and Deques
the built-in string function compare. When the loop terminates, we know that every
closing tag matches its corresponding opening tag. To finish the job, we need to
check that there were no unmatched opening tags. We test this by checking that the
stack is now empty.
Finally, the main program is presented in Code Fragment 5.14. It invokes the
function getHtmlTags to read the tags, and then it passes these to isHtmlMatched
to test them.
int main() {// main HTML tester
if (isHtmlMatched(getHtmlTags()))// get tags and test them
cout << "The input file is a matched HTML document." << endl;
else
cout << "The input file is not a matched HTML document." << endl;
}
Code Fragment 5.14: The main program to test whether the input file consists of
matching HTML tags.
5.2 Queues
Another fundamental data structure is the queue, which is a close relative of the
stack. A queue is a container of elements that are inserted and removed according
to the first-in first-out (FIFO) principle. Elements can be inserted in a queue at any
time, but only the element that has been in the queue the longest can be removed at
any time. We usually say that elements enter the queue at the rear and are removed
from the front. The metaphor for this terminology is a line of people waiting to get
on an amusement park ride. People enter at the rear of the line and get on the ride
from the front of the line.
5.2.1 The Queue Abstract Data Type
Formally, the queue abstract data type defines a container that keeps elements in
a sequence, where element access and deletion are restricted to the first element
in the sequence, which is called the front of the queue, and element insertion is
restricted to the end of the sequence, which is called the rear of the queue. This
restriction enforces the rule that items are inserted and deleted in a queue according
to the first-in first-out (FIFO) principle.
The queue abstract data type (ADT) supports the following operations:
enqueue(e): Insert element e at the rear of the queue.
dequeue(): Remove element at the front of the queue; an error occurs
if the queue is empty.
Page 233
5.2. Queues209
front(): Return, but do not remove, a reference to the front ele
ment in the queue; an error occurs if the queue is empty.
The queue ADT also includes the following supporting member functions:
size(): Return the number of elements in the queue.
empty(): Return true if the queue is empty and false otherwise.
We illustrate the operations in the queue ADT in the following example.
Example 5.4: The following table shows a series of queue operations and their
effects on an initially empty queue, Q, of integers.
Operation Output front ← Q ← rear
enqueue(5)–(5)
enqueue(3)–(5,3)
front()5(5,3)
size()2(5,3)
dequeue()–(3)
enqueue(7)–(3,7)
dequeue()–(7)
front()7(7)
dequeue()–()
dequeue() “error” ()
empty()true ()
5.2.2 The STL Queue
The Standard Template Library provides an implementation of a queue. As with
the STL stack, the underlying implementation is based on the STL vector class
(Sections 1.5.5 and 6.1.4). In order to declare an object of type queue, it is neces
sary to first include the definition file, which is called “queue.” As with the STL
vector, the class queue is part of the std namespace, so it is necessary either to use
“std::queue” or to provide an appropriate “using” statement. The queue class is
templated with the base type of the individual elements. For example, the code
fragment below declares a queue of floats.
#include <queue>
using std::queue;// make queue accessible
queue<float> myQueue;// a queue of floats
As with instances of STL vectors and stacks, an STL queue dynamically resizes
itself as new elements are added.
The STL queue supports roughly the same operators as our interface, but the
syntax and semantics are slightly different. Below, we list the principal member
Page 234
210Chapter 5. Stacks, Queues, and Deques
functions. Let q be declared to be an STL queue, and let e denote a single object
whose type is the same as the base type of the queue. (For example, q is a queue of
floats, and e is a float.)
size(): Return the number of elements in the queue.
empty(): Return true if the queue is empty and false otherwise.
push(e): Enqueue e at the rear of the queue.
pop(): Dequeue the element at the front of the queue.
front(): Return a reference to the element at the queue’s front.
back(): Return a reference to the element at the queue’s rear.
Unlike our queue interface, the STL queue provides access to both the front
and back of the queue. Similar to the STL stack, the result of applying any of
the operations front, back, or pop to an empty STL queue is undefined. Unlike
our interface, no exception is thrown, but it may very likely result in the program
aborting. It is up to the programmer to be sure that no such illegal accesses are
attempted.
5.2.3 A C++ Queue Interface
Our interface for the queue ADT is given in Code Fragment 5.15. As with the stack
ADT, the class is templated. The queue’s base element type E is provided by the
user.
template <typename E>
class Queue {// an interface for a queue
public:
int size() const;// number of items in queue
bool empty() const;// is the queue empty?
const E& front() const throw(QueueEmpty); // the front element
void enqueue (const E& e);// enqueue element at rear
void dequeue() throw(QueueEmpty);// dequeue element at front
};
Code Fragment 5.15: An informal Queue interface (not a complete C++ class).
Note that the size and empty functions have the same meaning as their coun
terparts in the Stack ADT. These two member functions and front are known as
accessor functions, for they return a value and do not change the contents of the
data structure. Also note the use of the exception QueueEmpty to indicate the error
state of an empty queue.
The member functions size, empty, and front are all declared to be const,
which informs the compiler that they do not alter the contents of the queue. Note
Page 235
5.2. Queues211
that the member function front returns a constant reference to the top of the queue.
An error condition occurs when calling either of the functions front or dequeue
on an empty queue. This is signaled by throwing an exception QueueEmpty, which
is defined in Code Fragment 5.16.
class QueueEmpty : public RuntimeException {
public:
QueueEmpty(const string& err) : RuntimeException(err) { }
};
Code Fragment 5.16: Exception thrown by functions front or dequeue when called
on an empty queue. This class is derived from RuntimeException from Section 2.4.
5.2.4 A Simple Array-Based Implementation
We present a simple realization of a queue by means of an array, Q, with capacity
N, for storing its elements. The main issue with this implementation is deciding
how to keep track of the front and rear of the queue.
One possibility is to adapt the approach we used for the stack implementation.
In particular, let Q[0] be the front of the queue and have the queue grow from
there. This is not an efficient solution, however, for it requires that we move all
the elements forward one array cell each time we perform a dequeue operation.
Such an implementation would therefore require Θ(n) time to perform the dequeue
function, where n is the current number of elements in the queue. If we want to
achieve constant time for each queue function, we need a different approach.
Using an Array in a Circular Way
To avoid moving objects once they are placed in Q, we define three variables, f, r,
n, which have the following meanings:
• f is the index of the cell of Q storing the front of the queue. If the queue is
nonempty, this is the index of the element to be removed by dequeue.
• r is an index of the cell of Q following the rear of the queue. If the queue is
not full, this is the index where the element is inserted by enqueue.
• n is the current number of elements in the queue.
Initially, we set n = 0 and f = r = 0, indicating an empty queue. When we
dequeue an element from the front of the queue, we decrement n and increment f
to the next cell in Q. Likewise, when we enqueue an element, we increment r and
increment n. This allows us to implement the enqueue and dequeue functions in
constant time
Page 236
212Chapter 5. Stacks, Queues, and Deques
Nonetheless, there is still a problem with this approach. Consider, for example,
what happens if we repeatedly enqueue and dequeue a single element N different
times. We would have f = r = N. If we were then to try to insert the element
just one more time, we would get an array-out-of-bounds error, even though there
is plenty of room in the queue in this case. To avoid this problem and be able to
utilize all of the array Q, we let the f and r indices “wrap around” the end of Q.
That is, we now view Q as a “circular array” that goes from Q[0] to Q[N − 1] and
then immediately back to Q[0] again. (See Figure 5.4.)
0 1 2 fr N−1 0 1 2 rf N−1
(a)(b)
Figure 5.4: Using array Q in a circular fashion: (a) the “normal” configuration with
f ≤ r; (b) the “wrapped around” configuration with r < f . The cells storing queue
elements are shaded.
Using the Modulo Operator to Implement a Circular Array
Implementing this circular view of Q is actually pretty easy. Each time we in
crement f or r, we simply need to compute this increment as “(f + 1) mod N”
or “(r + 1) mod N,” respectively, where the operator “mod ” is the modulo oper
ator. This operator is computed for a positive number by taking the remainder
after an integral division. For example, 48 divided by 5 is 9 with remainder 3,
so 48 mod 5 = 3. Specifically, given integers x and y, such that x ≥ 0 and y > 0,
x mod y is the unique integer 0 ≤ r < y such that x = qy + r, for some integer q.
Recall that C++ uses “%” to denote the modulo operator.
We present our implementation in Code Fragment 5.17. Note that we have
introduced a new exception, called QueueFull, to signal that no more elements can
be inserted in the queue. Our implementation of a queue by means of an array is
similar to that of a stack, and is left as an exercise.
The array-based queue implementation is quite efficient. All of the operations
of the queue ADT are performed in O(1) time. The space usage is O(N), where N
is the size of the array, determined at the time the queue is created. Note that the
space usage is independent from the number n < N of elements that are actually in
the queue.
As with the array-based stack implementation, the only real disadvantage of
the array-based queue implementation is that we artificially set the capacity of the
queue to be some number N. In a real application, we may actually need more
or less queue capacity than this, but if we have a good estimate of the number of
Page 237
5.2. Queues213
Algorithm size():
return n
Algorithm empty():
return (n = 0)
Algorithm front():
if empty() then
throw QueueEmpty exception
return Q[ f ]
Algorithm dequeue():
if empty() then
throw QueueEmpty exception
f ← ( f + 1) mod N
n = n−1
Algorithm enqueue(e):
if size() = N then
throw QueueFull exception
Q[r] ← e
r ← (r + 1) mod N
n = n + 1
Code Fragment 5.17: Implementation of a queue using a circular array.
elements that will be in the queue at the same time, then the array-based imple
mentation is quite efficient. One such possible application of a queue is dynamic
memory allocation in C++, which is discussed in Chapter 14.
5.2.5 Implementing a Queue with a Circularly Linked List
In this section, we present a C++ implementation of the queue ADT using a linked
representation. Recall that we delete from the head of the queue and insert at the
rear. Thus, unlike our linked stack of Code Fragment 5.7, we cannot use our singly
linked list class, since it provides efficient access only to one side of the list. In
stead, our approach is to use the circularly linked list, called CircleList, which was
introduced earlier in Section 3.4.1.
Recall that CircleList maintains a pointer, called the cursor, which points to one
node of the list. Also recall that CircleList provides two member functions, back
and front. The function back returns a reference to the element to which the cursor
points, and the function front returns a reference to the element that immediately
follows it in the circular list. In order to implement a queue, the element referenced
by back will be the rear of the queue and the element referenced by front will be the
front. (Why would it not work to reverse matters using the back of the circular list
Page 238
214Chapter 5. Stacks, Queues, and Deques
as the front of the queue and the front of the circular list as the rear of the queue?)
Also recall that CircleList supports the following modifier functions. The func
tion add inserts a new node just after the cursor, the function remove removes the
node immediately following the cursor, and the function advance moves the cursor
forward to the next node of the circular list.
In order to implement the queue operation enqueue, we first invoke the function
add, which inserts a new element just after the cursor, that is, just after the rear
of the queue. We then invoke advance, which advances the cursor to this new
element, thus making the new node the rear of the queue. The process is illustrated
in Figure 5.5.
LAXMSPATL(front)(rear)cursor
(a)
LAXMSPATLBOS(front)(rear)cursor
(b)
LAXMSPATLBOS(front)(rear)cursor
(c)
Figure 5.5: Enqueueing “BOS” into a queue represented as a circularly linked list:
(a) before the operation; (b) after adding the new node; (c) after advancing the
cursor.
In order to implement the queue operation dequeue, we invoke the function
remove, thus removing the node just after the cursor, that is, the front of the queue.
The process is illustrated in Figure 5.6.
The class structure for the resulting class, called LinkedQueue, is shown in
Code Fragment 5.18. To avoid the syntactic messiness inherent in C++ templated
classes, we have chosen not to implement a fully generic templated class. Instead,
we have opted to define a type for the queue’s elements, called Elem. In this ex
ample, we define Elem to be of type string. The queue is stored in the circular list
Page 239
5.2. Queues215
LAXMSPATLBOS(front)(rear)cursor
(a)
MSPATLBOS(front)(rear)cursor
LAX
(b)
Figure 5.6: Dequeueing an element (in this case “LAX”) from the front queue rep
resented as a circularly linked list: (a) before the operation; (b) after removing the
node immediately following the cursor.
data structure C. In order to support the size function (which CircleList does not
provide), we also maintain the queue size in the member n.
typedef string Elem;// queue element type
class LinkedQueue {// queue as doubly linked list
public:
LinkedQueue();// constructor
int size() const;// number of items in the queue
bool empty() const;// is the queue empty?
const Elem& front() const throw(QueueEmpty); // the front element
void enqueue(const Elem& e);// enqueue element at rear
void dequeue() throw(QueueEmpty);// dequeue element at front
private:// member data
CircleList C;// circular list of elements
int n;// number of elements
};
Code Fragment 5.18: The class LinkedQueue, an implementation of a queue based
on a circularly linked list.
In Code Fragment 5.19, we present the implementations of the constructor and
the basic accessor functions, size, empty, and front. Our constructor creates the
initial queue and initializes n to zero. We do not provide an explicit destructor,
relying instead on the destructor provided by CircleList. Observe that the func
tion front throws an exception if an attempt is made to access the first element of
an empty queue. Otherwise, it returns the element referenced by the front of the
circular list, which, by our convention, is also the front element of the queue.
Page 240
216Chapter 5. Stacks, Queues, and Deques
LinkedQueue::LinkedQueue()// constructor
: C(), n(0) { }
int LinkedQueue::size() const// number of items in the queue
{ return n; }
bool LinkedQueue::empty() const// is the queue empty?
{ return n == 0; }
// get the front element
const Elem& LinkedQueue::front() const throw(QueueEmpty) {
if (empty())
throw QueueEmpty("front of empty queue");
return C.front();// list front is queue front
}
Code Fragment 5.19: Constructor and accessor functions for the LinkedQueue class.
The definition of the queue operations, enqueue and dequeue are presented in
Code Fragment 5.20. Recall that enqueuing involves invoking the add function to
insert the new item immediately following the cursor and then advancing the cursor.
Before dequeuing, we check whether the queue is empty, and, if so, we throw an
exception. Otherwise, dequeuing involves removing the element that immediately
follows the cursor. In either case, we update the number of elements in the queue.
// enqueue element at rear
void LinkedQueue::enqueue(const Elem& e) {
C.add(e);// insert after cursor
C.advance();// . . .and advance
n++;
}
// dequeue element at front
void LinkedQueue::dequeue() throw(QueueEmpty) {
if (empty())
throw QueueEmpty("dequeue of empty queue");
C.remove();// remove from list front
n−−;
}
Code Fragment 5.20: The enqueue and dequeue functions for LinkedQueue.
Observe that, all the operations of the queue ADT are implemented in O(1)
time. Therefore this implementation is quite efficient. Unlike the array-based im
plementation, by expanding and contracting dynamically, this implementation uses
space proportional to the number of elements that are present in the queue at any
time.
Page 241
5.3. Double-Ended Queues217
5.3 Double-Ended Queues
Consider now a queue-like data structure that supports insertion and deletion at both
the front and the rear of the queue. Such an extension of a queue is called a double
ended queue, or deque, which is usually pronounced “deck” to avoid confusion
with the dequeue function of the regular queue ADT, which is pronounced like
the abbreviation “D.Q.” An easy way to remember the “deck” pronunciation is to
observe that a deque is like a deck of cards in the hands of a crooked card dealer—it
is possible to deal off both the top and the bottom.
5.3.1 The Deque Abstract Data Type
The functions of the deque ADT are as follows, where D denotes the deque:
insertFront(e): Insert a new element e at the beginning of the deque.
insertBack(e): Insert a new element e at the end of the deque.
eraseFront(): Remove the first element of the deque; an error occurs if
the deque is empty.
eraseBack(): Remove the last element of the deque; an error occurs if
the deque is empty.
Additionally, the deque includes the following support functions:
front(): Return the first element of the deque; an error occurs if
the deque is empty.
back(): Return the last element of the deque; an error occurs if
the deque is empty.
size(): Return the number of elements of the deque.
empty(): Return true if the deque is empty and false otherwise.
Example 5.5: The following example shows a series of operations and their ef
fects on an initially empty deque, D, of integers.
Operation Output D
insertFront(3)–(3)
insertFront(5)–(5,3)
front()5(5,3)
eraseFront()–(3)
insertBack(7)–(3,7)
back()7(3,7)
eraseFront()–(7)
eraseBack()–()
Page 242
218Chapter 5. Stacks, Queues, and Deques
5.3.2 The STL Deque
As with the stack and queue, the Standard Template Library provides an implemen
tation of a deque. The underlying implementation is based on the STL vector class
(Sections 1.5.5 and 6.1.4). The pattern of usage is similar to that of the STL stack
and STL queue. First, we need to include the definition file “deque.” Since it is a
member of the std namespace, we need to either preface each usage “std::deque”
or provide an appropriate “using” statement. The deque class is templated with
the base type of the individual elements. For example, the code fragment below
declares a deque of strings.
#include <deque>
using std::deque;// make deque accessible
deque<string> myDeque;// a deque of strings
As with STL stacks and queues, an STL deque dynamically resizes itself as new
elements are added.
With minor differences, the STL deque class supports the same operators as
our interface. Here is a list of the principal operations.
size(): Return the number of elements in the deque.
empty(): Return true if the deque is empty and false otherwise.
push front(e): Insert e at the beginning the deque.
push back(e): Insert e at the end of the deque.
pop front(): Remove the first element of the deque.
pop back(): Remove the last element of the deque.
front(): Return a reference to the deque’s first element.
back(): Return a reference to the deque’s last element.
Similar to STL stacks and queues, the result of applying any of the operations
front, back, push front, or push back to an empty STL queue is undefined. Thus,
no exception is thrown, but the program may abort.
5.3.3 Implementing a Deque with a Doubly Linked List
In this section, we show how to implement the deque ADT using a linked repre
sentation. As with the queue, a deque supports efficient access at both ends of the
list, so our implementation is based on the use of a doubly linked list. Again, we
use the doubly linked list class, called DLinkedList, which was presented earlier in
Section 3.3.3. We place the front of the deque at the head of the linked list and the
rear of the queue at the tail. An illustration is provided in Figure 5.7.
Page 243
5.3. Double-Ended Queues219
Figure 5.7: A doubly linked list with sentinels, header and trailer. The front of our
deque is stored just after the header (“JFK”), and the back of our deque is stored
just before the trailer (“SFO”).
The definition of the resulting class, called LinkedDeque, is shown in Code
Fragment 5.21. The deque is stored in the data member D. In order to support
the size function, we also maintain the queue size in the member n. As in some
of our earlier implementations, we avoid the syntactic messiness inherent in C++
templated classes, and instead use just a type definition to define the deque’s base
element type.
typedef string Elem;// deque element type
class LinkedDeque {// deque as doubly linked list
public:
LinkedDeque();// constructor
int size() const;// number of items in the deque
bool empty() const;// is the deque empty?
const Elem& front() const throw(DequeEmpty); // the first element
const Elem& back() const throw(DequeEmpty); // the last element
void insertFront(const Elem& e);// insert new first element
void insertBack(const Elem& e);// insert new last element
void removeFront() throw(DequeEmpty); // remove first element
void removeBack() throw(DequeEmpty); // remove last element
private:// member data
DLinkedList D;// linked list of elements
int n;// number of elements
};
Code Fragment 5.21: The class structure for class LinkedDeque.
We have not bothered to provide an explicit destructor, because the DLinkedList
class provides its own destructor, which is automatically invoked when our Linked
Deque structure is destroyed.
Most of the member functions for the LinkedDeque class are straightforward
generalizations of the corresponding functions of the LinkedQueue class, so we
have omitted them. In Code Fragment 5.22, we present the implementations of
the member functions for performing insertions and removals of elements from the
deque. Observe that, in each case, we simply invoke the appropriate operation from
the underlying DLinkedList object.
Page 244
220Chapter 5. Stacks, Queues, and Deques
// insert new first element
void LinkedDeque::insertFront(const Elem& e) {
D.addFront(e);
n++;
}
// insert new last element
void LinkedDeque::insertBack(const Elem& e) {
D.addBack(e);
n++;
}
// remove first element
void LinkedDeque::removeFront() throw(DequeEmpty) {
if (empty())
throw DequeEmpty("removeFront of empty deque");
D.removeFront();
n−−;
}
// remove last element
void LinkedDeque::removeBack() throw(DequeEmpty) {
if (empty())
throw DequeEmpty("removeBack of empty deque");
D.removeBack();
n−−;
}
Code Fragment 5.22: The insertion and removal functions for LinkedDeque.
Table 5.2 shows the running times of functions in a realization of a deque by a
doubly linked list. Note that every function of the deque ADT runs in O(1) time.
Operation Time
size O(1)
empty O(1)
front, back O(1)
insertFront, insertBack O(1)
eraseFront, eraseBack O(1)
Table 5.2: Performance of a deque realized by a doubly linked list. The space usage
is O(n), where n is number of elements in the deque.
5.3.4 Adapters and the Adapter Design Pattern
An inspection of code fragments of Sections 5.1.5, 5.2.5, and 5.3.3, reveals a com
mon pattern. In each case, we have taken an existing data structure and adapted it
Page 245
5.3. Double-Ended Queues221
to be used for a special purpose. For example, in Section 5.3.3, we showed how
the DLinkedList class of Section 3.3.3 could be adapted to implement a deque. Ex
cept for the additional feature of keeping track of the number of elements, we have
simply mapped each deque operation (such as insertFront) to the corresponding
operation of DLinkedList (such as the addFront).
An adapter (also called a wrapper) is a data structure, for example, a class in
C++, that translates one interface to another. You can think of an adapter as the
software analogue to electric power plug adapters, which are often needed when
you want to plug your electric appliances into electric wall sockets in different
countries.
As an example of adaptation, observe that it is possible to implement the stack
ADT by means of a deque data structure. That is, we can translate each stack
operation to a functionally equivalent deque operation. Such a mapping is presented
in Table 5.3.
Stack Method Deque Implementation
size()size()
empty()empty()
top()front()
push(o)insertFront(o)
pop()eraseFront()
Table 5.3: Implementing a stack with a deque.
Note that, because of the deque’s symmetry, performing insertions and re
movals from the rear of the deque would have been equally efficient.
Likewise, we can develop the correspondences for the queue ADT, as shown in
Table 5.4.
Queue Method Deque Implementation
size()size()
empty()empty()
front()front()
enqueue(e)insertBack(e)
dequeue()eraseFront()
Table 5.4: Implementing a queue with a deque.
As a more concrete example of the adapter design pattern, consider the code
fragment shown in Code Fragment 5.23. In this code fragment, we present a class
DequeStack, which implements the stack ADT. It’s implementation is based on
translating each stack operation to the corresponding operation on a LinkedDeque,
which was introduced in Section 5.3.3.
Page 246
222Chapter 5. Stacks, Queues, and Deques
typedef string Elem;// element type
class DequeStack {// stack as a deque
public:
DequeStack();// constructor
int size() const;// number of elements
bool empty() const;// is the stack empty?
const Elem& top() const throw(StackEmpty); // the top element
void push(const Elem& e);// push element onto stack
void pop() throw(StackEmpty);// pop the stack
private:
LinkedDeque D;// deque of elements
};
Code Fragment 5.23: Implementation of the Stack interface by means of a deque.
The implementations of the various member functions are presented in Code
Fragment 5.24. In each case, we translate some stack operation into the corre
sponding deque operation.
DequeStack::DequeStack()// constructor
: D() { }
// number of elements
int DequeStack::size() const
{ return D.size(); }
// is the stack empty?
bool DequeStack::empty() const
{ return D.empty(); }
// the top element
const Elem& DequeStack::top() const throw(StackEmpty) {
if (empty())
throw StackEmpty("top of empty stack");
return D.front();
}
// push element onto stack
void DequeStack::push(const Elem& e)
{ D.insertFront(e); }
// pop the stack
void DequeStack::pop() throw(StackEmpty)
{
if (empty())
throw StackEmpty("pop of empty stack");
D.removeFront();
}
Code Fragment 5.24: Implementation of the Stack interface by means of a deque
Page 251
Chapter
6List and Iterator ADTs
Contents
6.1 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.1.1 The Vector Abstract Data Type . . . . . . . . . . . 228
6.1.2 A Simple Array-Based Implementation . . . . . . . . 229
6.1.3 An Extendable Array Implementation . . . . . . . . . 231
6.1.4 STL Vectors . . . . . . . . . . . . . . . . . . . . . . 236
6.2 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.2.1 Node-Based Operations and Iterators . . . . . . . . . 238
6.2.2 The List Abstract Data Type . . . . . . . . . . . . . 240
6.2.3 Doubly Linked List Implementation . . . . . . . . . . 242
6.2.4 STL Lists . . . . . . . . . . . . . . . . . . . . . . . 247
6.2.5 STL Containers and Iterators . . . . . . . . . . . . . 248
6.3 Sequences . . . . . . . . . . . . . . . . . . . . . . . . 255
6.3.1 The Sequence Abstract Data Type . . . . . . . . . . 255
6.3.2 Implementing a Sequence with a Doubly Linked List 255
6.3.3 Implementing a Sequence with an Array . . . . . . . 257
6.4 Case Study: Bubble-Sort on a Sequence . . . . . . . 259
6.4.1 The Bubble-Sort Algorithm . . . . . . . . . . . . . . 259
6.4.2 A Sequence-Based Analysis of Bubble-Sort . . . . . . 260
6.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 262
Page 252
228Chapter 6. List and Iterator ADTs
6.1 Vectors
Suppose we have a collection S of n elements stored in a certain linear order, so
that we can refer to the elements in S as first, second, third, and so on. Such a
collection is generically referred to as a list or sequence. We can uniquely refer
to each element e in S using an integer in the range [0,n − 1] that is equal to the
number of elements of S that precede e in S. The index of an element e in S is the
number of elements that are before e in S. Hence, the first element in S has index
0 and the last element has index n − 1. Also, if an element of S has index i, its
previous element (if it exists) has index i−1, and its next element (if it exists) has
index i + 1. This concept of index is related to that of the rank of an element in a
list, which is usually defined to be one more than its index; so the first element is at
rank 1, the second is at rank 2, and so on.
A sequence that supports access to its elements by their indices is called a vec
tor. Since our index definition is more consistent with the way arrays are indexed
in C++ and other common programming languages, we refer to the place where an
element is stored in a vector as its “index,” rather than its “rank.”
This concept of index is a simple yet powerful notion, since it can be used to
specify where to insert a new element into a list or where to remove an old element.
6.1.1 The Vector Abstract Data Type
A vector, also called an array list, is an ADT that supports the following funda
mental functions (in addition to the standard size() and empty() functions). In all
cases, the index parameter i is assumed to be in the range 0 ≤ i ≤ size()−1.
at(i): Return the element of V with index i; an error condition
occurs if i is out of range.
set(i,e): Replace the element at index i with e; an error condition
occurs if i is out of range.
insert(i,e): Insert a new element e into V to have index i; an error
condition occurs if i is out of range.
erase(i): Remove from V the element at index i; an error condition
occurs if i is out of range.
We do not insist that an array be used to implement a vector, so that the element
at index 0 is stored at index 0 in the array, although that is one (very natural) possi
bility. The index definition offers us a way to refer to the “place” where an element
is stored in a sequence without having to worry about the exact implementation of
that sequence. The index of an element may change when the sequence is updated,
however, as we illustrate in the following example.
Page 253
6.1. Vectors229
Example 6.1: We show below some operations on an initially empty vector V.
Operation OutputV
insert(0,7)–(7)
insert(0,4)–(4,7)
at(1)7(4,7)
insert(2,2)–(4,7,2)
at(3)“error”(4,7,2)
erase(1)–(4,2)
insert(1,5)–(4,5,2)
insert(1,3)–(4,3,5,2)
insert(4,9)–(4,3,5,2,9)
at(2)5(4,3,5,2,9)
set(3,8)–(4,3,5,8,9)
6.1.2 A Simple Array-Based Implementation
An obvious choice for implementing the vector ADT is to use a fixed size array
A, where A[i] stores the element at index i. We choose the size N of array A to be
sufficiently large, and we maintain the number n < N of elements in the vector in a
member variable.
The details of the implementation of the functions of the vector ADT are rea
sonably simple. To implement the at(i) operation, for example, we just return A[i].
The implementations of the functions insert(i,e) and erase(i) are given in Code
Fragment 6.1.
Algorithm insert(i,e):
for j = n−1,n−2,...,i do
A[ j + 1] ← A[ j]{make room for the new element}
A[i] ← e
n ← n+ 1
Algorithm erase(i):
for j = i+ 1,i+ 2,...,n−1 do
A[ j −1] ← A[ j]{fill in for the removed element}
n ← n−1
Code Fragment 6.1: Methods insert(i,e) and erase(i) in the array implementation
of the vector ADT. The member variable n stores the number of elements.
An important (and time-consuming) part of this implementation involves the
shifting of elements up or down to keep the occupied cells in the array contiguous.
These shifting operations are required to maintain our rule of always storing an
Page 254
230Chapter 6. List and Iterator ADTs
element whose list index i at index i in the array A. (See Figure 6.1.)
S
0 1 2in–1N –1
(a)
S
0 1 2in–1N –1
(b)
Figure 6.1: Array-based implementation of a vector V that is storing n elements:
(a) shifting up for an insertion at index i; (b) shifting down for a removal at index i.
The Performance of a Simple Array-Based Implementation
Table 6.1 shows the worst-case running times of the functions of a vector with n
elements realized by means of an array. Methods empty, size, at, and set clearly
run in O(1) time, but the insertion and removal functions can take much longer
than this. In particular, insert(i,e) runs in time O(n). Indeed, the worst case for this
operation occurs when i = 0, since all the existing n elements have to be shifted
forward. A similar argument applies to function erase(i), which runs in O(n) time,
because we have to shift backward n−1 elements in the worst case (i = 0). In fact,
assuming that each possible index is equally likely to be passed as an argument to
these operations, their average running time is O(n) because we have to shift n/2
elements on average.
Operation Time
size() O(1)
empty() O(1)
at(i) O(1)
set(i,e) O(1)
insert(i,e) O(n)
erase(i) O(n)
Table 6.1: Performance of a vector with n elements realized by an array. The space
usage is O(N), where N is the size of the array.
Looking more closely at insert(i,e) and erase(i), we note that they each run in
time O(n− i + 1), for only those elements at index i and higher have to be shifted
Page 255
6.1. Vectors231
up or down. Thus, inserting or removing an item at the end of a vector, using the
functions insert(n,e) and erase(n−1), take O(1) time each respectively. Moreover,
this observation has an interesting consequence for the adaptation of the vector
ADT to the deque ADT given in Section 5.3.1. If the vector ADT in this case is
implemented by means of an array as described above, then functions insertBack
and eraseBack of the deque each run in O(1) time. However, functions insertFront
and eraseFront of the deque each run in O(n) time.
Actually, with a little effort, we can produce an array-based implementation of
the vector ADT that achieves O(1) time for insertions and removals at index 0, as
well as insertions and removals at the end of the vector. Achieving this requires
that we give up on our rule that an element at index i is stored in the array at index
i, however, as we would have to use a circular array approach like the one we used
in Section 5.2 to implement a queue. We leave the details of this implementation
for an exercise (R-6.17).
6.1.3 An Extendable Array Implementation
A major weakness of the simple array implementation for the vector ADT given
in Section 6.1.2 is that it requires advance specification of a fixed capacity, N, for
the total number of elements that may be stored in the vector. If the actual number
of elements, n, of the vector is much smaller than N, then this implementation will
waste space. Worse, if n increases past N, then this implementation will crash.
Fortunately, there is a simple way to fix this major drawback.
Let us provide a means to grow the array A that stores the elements of a vec
tor V . Of course, in C++ (and most other programming languages) we cannot
actually grow the array A; its capacity is fixed at some number N, as we have al
ready observed. Instead, when an overflow occurs, that is, when n = N and function
insert is called, we perform the following steps:
1. Allocate a new array B of capacity N
2. Copy A[i] to B[i], for i = 0,...,N −1
3. Deallocate A and reassign A to point to the new array B
This array replacement strategy is known as an extendable array, for it can
be viewed as extending the end of the underlying array to make room for more
elements. (See Figure 6.2.) Intuitively, this strategy is much like that of the hermit
crab, which moves into a larger shell when it outgrows its previous one.
We give an implementation of the vector ADT using an extendable array in
Code Fragment 6.2. To avoid the complexities of templated classes, we have
adopted our earlier practice of using a type definition to specify the base element
type, which is an int in this case. The class is called ArrayVector. We leave the
details of producing a fully generic templated class as an exercise (R-6.7).
Page 256
232Chapter 6. List and Iterator ADTs
(a)(c)AA
B
(b)A
B
Figure 6.2: The three steps for “growing” an extendable array: (a) create new ar
ray B; (b) copy elements from A to B; (c) reassign A to refer to the new array and
delete the old array.
Our class definition differs slightly from the operations given in our ADT. For
example, we provide two means for accessing individual elements of the vector.
The first involves overriding the C++ array index operator (“[ ]”), and the second
is the at function. The two functions behave the same, except that the at function
performs a range test before each access. (Note the similarity with the STL vector
class given in Section 6.1.4.) If the index i is not in bounds, this function throws an
exception. Because both of these access operations return a reference, there is no
need to explicitly define a set function. Instead, we can simply use the assignment
operator. For example, the ADT function v.set(i,5) could be implemented either as
v[i] = 5 or, more safely, as v.at(i) = 5.
typedef int Elem;// base element type
class ArrayVector {
public:
ArrayVector();// constructor
int size() const;// number of elements
bool empty() const;// is vector empty?
Elem& operator[ ](int i);// element at index
Elem& at(int i) throw(IndexOutOfBounds); // element at index
void erase(int i);// remove element at index
void insert(int i, const Elem& e);// insert element at index
void reserve(int N);// reserve at least N spots
// . . . (housekeeping functions omitted)
private:
int capacity;// current array size
int n;// number of elements in vector
Elem* A;// array storing the elements
};
Code Fragment 6.2: A vector implementation using an extendable array.
The member data for class ArrayVector consists of the array storage A, the
current number n of elements in the vector, and the current storage capacity. The
class ArrayVector also provides the ADT functions insert and remove. We discuss
their implementations below. We have added a new function, called reserve, that
Page 257
6.1. Vectors233
is not part of the ADT. This function allows the user to explicitly request that the
array be expanded to a capacity of a size at least n. If the capacity is already larger
than this, then the function does nothing.
Even though we have not bothered to show them, the class also provides some
of the standard housekeeping functions. These consist of a copy constructor, an
assignment operator, and a destructor. Because this class allocates memory, their
inclusion is essential for a complete and robust class implementation. We leave
them as an exercise (R-6.6). We should also add versions of the indexing operators
that return constant references.
In Code Fragment 6.3, we present the class constructor and a number of simple
member functions. When the vector is constructed, we do not allocate any storage
and simply set A to NULL. Note that the first attempt to add an element results in
array storage being allocated.
ArrayVector::ArrayVector()// constructor
: capacity(0), n(0), A(NULL) { }
int ArrayVector::size() const// number of elements
{ return n; }
bool ArrayVector::empty() const// is vector empty?
{ return size() == 0; }
Elem& ArrayVector::operator[ ](int i)// element at index
{ return A[i]; }
// element at index (safe)
Elem& ArrayVector::at(int i) throw(IndexOutOfBounds) {
if (i < 0 | | i >= n)
throw IndexOutOfBounds("illegal index in function at()");
return A[i];
}
Code Fragment 6.3: The simple member functions for class ArrayVector.
In Code Fragment 6.4, we present the member function erase. As mentioned
above, it removes an element at index i by shifting all subsequent elements from
index i+1 to the last element of the array down by one position.
void ArrayVector::erase(int i) {// remove element at index
for (int j = i+1; j < n; j++)// shift elements down
A[j − 1] = A[j];
n−−;// one fewer element
}
Code Fragment 6.4: The member function remove for class ArrayVector.
Page 258
234Chapter 6. List and Iterator ADTs
Finally, in Code Fragment 6.5, we present the reserve and insert functions. The
reserve function first checks whether the capacity already exceeds n, in which case
nothing needs to be done. Otherwise, it allocates a new array B of the desired sizes,
copies the contents of A to B, deletes A, and makes B the current array. The insert
function first checks whether there is sufficient capacity for one more element. If
not, it sets the capacity to the maximum of 1 and twice the current capacity. Then
starting at the insertion point, it shifts elements up by one position, and stores the
new element in the desired position.
void ArrayVector::reserve(int N) {// reserve at least N spots
if (capacity >= N) return;// already big enough
Elem* B = new Elem[N];// allocate bigger array
for (int j = 0; j < n; j++)// copy contents to new array
B[j] = A[j];
if (A != NULL) delete [ ] A;// discard old array
A = B;// make B the new array
capacity = N;// set new capacity
}
void ArrayVector::insert(int i, const Elem& e) {
if (n >= capacity)// overflow?
reserve(max(1, 2 * capacity));// double array size
for (int j = n − 1; j >= i; j−−)// shift elements up
A[j+1] = A[j];
A[i] = e;// put in empty slot
n++;// one more element
}
Code Fragment 6.5: The member functions reserve and insert for class ArrayVector.
In terms of efficiency, this array replacement strategy might, at first, seem rather
slow. After all, performing just one array replacement required by an element in
sertion takes O(n) time, which is not very good. Notice, however, that, after we
perform an array replacement, our new array allows us to add n new elements to
the vector before the array must be replaced again.
This simple observation allows us to show that the running time of a series
of operations performed on an initially empty vector is proportional to the total
number of elements added. As a shorthand notation, let us refer to the insertion of
an element meant to be the last element in a vector as a “push” operation. Using a
design pattern called amortization, we show below that performing a sequence of
push operations on a vector implemented with an extendable array is quite efficient.
Proposition 6.2: Let V be a vector implemented by means of an extendable array
A, as described above. The total time to perform a series of n push operations in V ,
starting from V being empty and A having size N = 1, is O(n).
Justification: To perform this analysis, we view the computer as a coin-operated
Page 259
6.1. Vectors235
appliance, which requires the payment of one cyber-dollar for a constant amount
of computing time. When an operation is executed, we should have enough cyber
dollars available in our current “bank account” to pay for that operation’s running
time. Thus, the total amount of cyber-dollars spent for any computation is propor
tional to the total time spent on that computation. The beauty of using this analysis
method is that we can overcharge some operations in order to save up cyber-dollars
to pay for others.
Let us assume that one cyber-dollar is enough to pay for the execution of each
push operation in V, excluding the time spent for growing the array. Also, let us
assume that growing the array from size k to size 2k requires k cyber-dollars for the
time spent copying the elements. We shall charge each push operation three cyber
dollars. Thus, we overcharge each push operation that does not cause an overflow
by two cyber-dollars. Think of the two cyber-dollars profited in an insertion that
does not grow the array as being “stored” at the element inserted.
An overflow occurs when the vector V has 2i elements, for some i ≥ 0, and the
size of the array used by V is 2i. Thus, doubling the size of the array requires 2i
cyber-dollars. Fortunately, these cyber-dollars can be found at the elements stored
in cells 2i−1 through 2i − 1. (See Figure 6.3.) Note that the previous overflow
occurred when the number of elements became larger than 2i−1 for the first time,
and thus the cyber-dollars stored in cells 2i−1 through 2i − 1 were not previously
spent. Therefore, we have a valid amortization scheme in which each operation is
charged three cyber-dollars and all the computing time is paid for. That is, we can
pay for the execution of n push operations using 3n cyber-dollars.
(a)$$$$ $$$$ 10 2 3 4 5 6 7$$$$ $$$$ 10 2 3 4 5 6 7
(b)
02 3 4 5 6 7 1$ $
8 9 10 11 12 13 14 15
Figure 6.3: A series of push operations on a vector: (a) an 8-cell array is full,
with two cyber-dollars “stored” at cells 4 through 7; (b) a push operation causes
an overflow and a doubling of capacity. Copying the eight old elements to the new
array is paid for by the cyber-dollars already stored in the table; inserting the new
element is paid for by one of the cyber-dollars charged to the push operation; and
two cyber-dollars profited are stored at cell 8.
Page 260
236Chapter 6. List and Iterator ADTs
6.1.4 STL Vectors
In Section 1.5.5, we introduced the vector class of the C++ Standard Template
Library (STL). We mentioned that STL vectors behave much like standard arrays
in C++, but they are superior to standard arrays in many respects. In this section
we explore this class in greater detail.
The Standard Template Library provides C++ programmers a number of useful
built-in classes and algorithms. The classes provided by the STL are organized in
various groups. Among the most important of these groups is the set of classes
called containers. A container is a data structure that stores a collection of ob
jects. Many of the data structures that we study later in this book, such as stacks,
queues, and lists, are examples of STL containers. The class vector is perhaps the
most basic example of an STL container class. We discuss containers further in
Section 6.2.1.
The definition of class vector is given in the system include file named “vec
tor.” The vector class is part of the std namespace, so it is necessary either to use
“std::vector” or to provide an appropriate using statement. The vector class is tem
plated with the class of the individual elements. For example, the code fragment
below declares a vector containing 100 integers.
#include <vector>// provides definition of vector
using std::vector;// make vector accessible
vector<int> myVector(100);// a vector with 100 integers
We refer to the type of individual elements as the vector’s base type. Each element
is initialized to the base type’s default value, which for integers is zero.
STL vector objects behave in many respects like standard C++ arrays, but they
provide many additional features.
• As with arrays, individual elements of a vector object can be indexed using
the usual index operator (“[ ]”). Elements can also be accessed by a member
function called at. The advantage of this member function over the index
operator is that it performs range checking and generates an error exception
if the index is out of bounds.
• Unlike C++ arrays, STL vectors can be dynamically resized, and new ele
ments may be efficiently appended or removed from the end of an array.
• When an STL vector of class objects is destroyed, it automatically invokes
the destructor for each of its elements. (With C++ arrays, it is the obligation
of the programmer to do this explicitly.)
• STL vectors provide a number of useful functions that operate on entire vec
tors, not just on individual elements. This includes, for example, the ability
to copy all or part of one vector to another, the ability to compare the contents
of two arrays, and the ability to insert and erase multiple elements.
Page 261
6.1. Vectors237
Here are the principal member functions of the vector class. Let V be declared
to be an STL vector of some base type, and let e denote a single object of this same
base type. (For example, V is a vector of integers, and e is an integer.)
vector(n): Construct a vector with space for n elements; if no argu
ment is given, create an empty vector.
size(): Return the number of elements in V.
empty(): Return true if V is empty and false otherwise.
resize(n): Resize V, so that it has space for n elements.
reserve(n): Request that the allocated storage space be large enough
to hold n elements.
operator[i]: Return a reference to the ith element of V.
at(i): Same as V[i], but throw an out of range exception if i is
out of bounds, that is, if i < 0 or i ≥ V.size().
front(): Return a reference to the first element of V.
back(): Return a reference to the last element of V.
push back(e): Append a copy of the element e to the end of V, thus
increasing its size by one.
pop back(): Remove the last element of V, thus reducing its size by
one.
When the base type of an STL vector is class, all copying of elements (for
example, in push back) is performed by invoking the class’s copy constructor.
Also, when elements are destroyed (for example, by invoking the destroyer or the
pop back member function) the class’s destructor is invoked on each deleted ele
ment. STL vectors are expandable—when the current array space is exhausted, its
storage size is increased.
Although we have not discussed it here, the STL vector also supports functions
for inserting elements at arbitrary positions within the vector, and for removing
arbitrary elements of the vector. These are discussed in Section 6.1.4.
There are both similarities and differences between our ArrayVector class of
Section 6.1.3 and the STL vector class. One difference is that the STL constructor
allows for an arbitrary number of initial elements, whereas our ArrayVect con
structor always starts with an empty vector. The STL vector functions V.front()
and V.back() are equivalent to our functions V[0] and V[n−1], respectively, where
n is equal to V.size(). The STL vector functions V.push back(e) and V.pop back()
are equivalent to our ArrayVect functions V.insert(n,e) and V.remove(n − 1), re
spectively.
Page 262
238Chapter 6. List and Iterator ADTs
6.2 Lists
Using an index is not the only means of referring to the place where an element
appears in a list. If we have a list L implemented with a (singly or doubly) linked
list, then it could possibly be more natural and efficient to use a node instead of an
index as a means of identifying where to access and update a list. In this section,
define the list ADT, which abstracts the concrete linked list data structure (presented
in Sections 3.2 and 3.3) using a related position ADT that abstracts the notion of
“place” in a list.
6.2.1 Node-Based Operations and Iterators
Let L be a (singly or doubly) linked list. We would like to define functions for L
that take nodes of the list as parameters and provide nodes as return types. Such
functions could provide significant speedups over index-based functions, because
finding the index of an element in a linked list requires searching through the list
incrementally from its beginning or end, counting elements as we go.
For instance, we might want to define a hypothetical function remove(v) that
removes the element of L stored at node v of the list. Using a node as a parameter
allows us to remove an element in O(1) time by simply going directly to the place
where that node is stored and then “linking out” this node through an update of the
next and prev links of its neighbors. Similarly, in O(1) time, we could insert a new
element e into L with an operation such as insert(v,e), which specifies the node
v before which the node of the new element should be inserted. In this case, we
simply “link in” the new node.
Defining functions of a list ADT by adding such node-based operations raises
the issue of how much information we should be exposing about the implementa
tion of our list. Certainly, it is desirable for us to be able to use either a singly or
doubly linked list without revealing this detail to a user. Likewise, we do not wish
to allow a user to modify the internal structure of a list without our knowledge.
Such modifications would be possible, however, if we provided a pointer to a node
in our list in a form that allows the user to access internal data in that node (such as
the next or prev field).
To abstract and unify the different ways of storing elements in the various im
plementations of a list, we introduce a data type that abstracts the notion of the
relative position or place of an element within a list. Such an object might naturally
be called a position. Because we want this object not only to access individual
elements of a list, but also to move around in order to enumerate all the elements
of a list, we adopt the convention used in the C++ Standard Template Library, and
call it an iterator.
Page 263
6.2. Lists239
Containers and Positions
In order to safely expand the set of operations for lists, we abstract a notion of
“position” that allows us to enjoy the efficiency of doubly or singly linked list
implementations without violating object-oriented design principles. In this frame
work, we think of a list as an instance of a more general class of objects, called
a container. A container is a data structure that stores any collection of elements.
We assume that the elements of a container can be arranged in a linear order. A
position is defined to be an abstract data type that is associated with a particular
container and which supports the following function.
element(): Return a reference to the element stored at this position.
C++’s ability to overload operators provides us with an elegant alternative man
ner in which to express the element operation. In particular, we overload the deref
erencing operator (“*”), so that, given a position variable p, the associated element
can be accessed by *p, rather than p.element(). This can be used both for accessing
and modifying the element’s value.
A position is always defined in a relative manner, that is, in terms of its neigh
bors. Unless it is the first or last of the container, a position q is always “after”
some position p and “before” some position r (see Figure 6.4). A position q, which
is associated with some element e in a container, does not change, even if the index
of e changes in the container, unless we explicitly remove e. If the associated node
is removed, we say that q is invalidated. Moreover, the position q does not change
even if we replace or swap the element e stored at q with another element.
pqrsBaltimore New YorkBeijingDelhi
Figure 6.4: A list container. The positions in the current order are p, q, r, and s.
Iterators
Although a position is a useful object, it would be more useful still to be able to
navigate through the container, for example, by advancing to the next position in
the container. Such an object is called an iterator. An iterator is an extension of a
position. It supports the ability to access a node’s element, but it also provides the
ability to navigate forwards (and possibly backwards) through the container.
There are a number of ways in which to define an ADT for an iterator object.
For example, given an iterator object p, we could define an operation p.next(),
which returns an iterator that refers to the node just after p in the container. Be
cause of C++’s ability to overload operators, there is a more elegant way to do
Page 264
240Chapter 6. List and Iterator ADTs
this by overloading the increment operator (“++”). In particular, the operation
++p advances p to the next position of the container. By repeatedly applying this
operation, we can step through all the elements of the container. For some imple
mentations of containers, such as a doubly linked list, navigation may be possible
both forwards and backwards. If so, we can also overload the decrement operator
(“– –”) to move the iterator to the previous position in the container.
In addition to navigating through the container, we need some way of initial
izing an iterator to the first node of a container and determining whether it has
gone beyond the end of the container. To do this, we assume that each container
provides two special iterator values, begin and end. The beginning iterator refers
to the first position of the container. We think of the ending iterator as referring
to an imaginary position that lies just after the last node of the container. Given
a container object L, the operation L.begin() returns an instance of the beginning
iterator for L, and the operation L.end() returns an instance of the ending iterator.
(See Figure 6.5.)
Baltimore New YorkBeijingDelhi
L.begin()L.end()
Figure 6.5: The special iterators L.begin() and L.end() for a list L.
In order to enumerate all the elements of a given container L, we define an iter
ator p whose value is initialized to L.begin(). The associated element is accessed
using *p. We can enumerate all of the elements of the container by advancing p to
the next node using the operation ++p. We repeat this until p is equal to L.end(),
which means that we have fallen off the end of the list.
6.2.2 The List Abstract Data Type
Using the concept of an iterator to encapsulate the idea of “node” in a list, we can
define another type of sequence ADT, called simply the list ADT. In addition to the
above functions, we include the generic functions size and empty with the usual
meanings. This ADT supports the following functions for a list L and an iterator p
for this list.
begin(): Return an iterator referring to the first element of L; same
as end() if L is empty.
end(): Return an iterator referring to an imaginary element just
after the last element of L.
insertFront(e): Insert a new element e into L as the first element.
Page 265
6.2. Lists241
insertBack(e): Insert a new element e into L as the last element.
insert(p,e): Insert a new element e into L before position p in L.
eraseFront(): Remove the first element of L.
eraseBack(): Remove the last element of L.
erase(p): Remove from L the element at position p; invalidates p
as a position.
The functions insertFront(e) and insertBack(e) are provided as a convenience,
since they are equivalent to insert(L.begin(),e) and insert(L.end(),e), respectively.
Similarly, eraseFront and eraseBack can be performed by the more general func
tion erase.
An error condition occurs if an invalid position is passed as an argument to one
of the list operations. Reasons for a position p to be invalid include:
• p was never initialized or was set to a position in a different list
• p was previously removed from the list
• p results from an illegal operation, such as attempting to perform ++p,
where p = L.end(), that is, attempting to access a position beyond the end
position
We do not check for these errors in our implementation. Instead, it is the re
sponsibility of the programmer to be sure that only legal positions are used.
Example 6.3: We show a series of operations for an initially empty list L below.
We use variables p and q to denote different positions, and we show the object
currently stored at such a position in parentheses in the Output column.
Operation OutputL
insertFront(8)–(8)
p = begin()p : (8)(8)
insertBack(5)–(8,5)
q = p; ++ q q : (5)(8,5)
p == begin()true(8,5)
insert(q,3)–(8,3,5)
*q = 7–(8,3,7)
insertFront(9)–(9,8,3,7)
eraseBack()–(9,8,3)
erase(p)–(9,3)
eraseFront()–(3)
The list ADT, with its built-in notion of position, is useful in a number of set
tings. For example, a program that models several people playing a game of cards
could model each person’s hand as a list. Since most people like to keep cards of
the same suit together, inserting and removing cards from a person’s hand could
Page 266
242Chapter 6. List and Iterator ADTs
be implemented using the functions of the list ADT, with the positions being de
termined by a natural ordering of the suits. Likewise, a simple text editor embeds
the notion of positional insertion and removal, since such editors typically perform
all updates relative to a cursor, which represents the current position in the list of
characters of text being edited.
6.2.3 Doubly Linked List Implementation
There are a number of different ways to implement our list ADT in C++. Probably
the most natural and efficient way is to use a doubly linked list, similar to the one
we introduced in Section 3.3. Recall that our doubly linked list structure is based
on two sentinel nodes, called the header and trailer. These are created when the
list is first constructed. The other elements of the list are inserted between these
sentinels.
Following our usual practice, in order to keep the code simple, we sacrifice gen
erality by forgoing the use of class templates. Instead, we provide a type definition
Elem, which is the base element type of the list. We leave the details of producing
a fully generic templated class as an exercise (R-6.11).
Before defining the class, which we call NodeList, we define two important
structures. The first represents a node of the list and the other represents an iterator
for the list. Both of these objects are defined as nested classes within NodeList.
Since users of the class access nodes exclusively through iterators, the node is de
clared a private member of NodeList, and the iterator is a public member.
The node object is called Node and is presented in Code Fragment 6.6. This
is a simple C++ structure, which has only (public) data members, consisting of the
node’s element, a link to the previous node of the list, and a link to the next node
of the list. Since it is declared to be private to NodeList, its members are accessible
only within NodeList.
struct Node {// a node of the list
Elem elem;// element value
Node* prev;// previous in list
Node* next;// next in list
};
Code Fragment 6.6: The declaration of a node of a doubly linked list.
Our iterator object is called Iterator. To users of class NodeList, it can be ac
cessed by the qualified type name NodeList::Iterator. Its definition, which is pre
sented in Code Fragment 6.7, is placed in the public part of NodeList. An element
associated with an iterator can be accessed by overloading the dereferencing oper
ator (“*”). In order to make it possible to compare iterator objects, we overload the
Page 267
6.2. Lists243
equality and inequality operators (“==” and “!=”). We provide the ability to move
forward or backward in the list by providing the increment and decrement operators
(“++” and “– –”). We declare NodeList to be a friend, so that it may access the
private members of Iterator. The private data member consists of a pointer v to the
associated node of the list. We also provide a private constructor, which initializes
the node pointer. (The constructor is private so that only NodeList is allowed to
create new iterators.)
class Iterator {// an iterator for the list
public:
Elem& operator*();// reference to the element
bool operator==(const Iterator& p) const; // compare positions
bool operator!=(const Iterator& p) const;
Iterator& operator++();// move to next position
Iterator& operator−−();// move to previous position
friend class NodeList;// give NodeList access
private:
Node* v;// pointer to the node
Iterator(Node* u);// create from node
};
Code Fragment 6.7: Class Iterator, realizing an iterator for a doubly linked list.
In Code Fragment 6.8 we present the implementations of the member functions
for the Iterator class. These all follow directly from the definitions given earlier.
NodeList::Iterator::Iterator(Node* u)// constructor from Node*
{ v = u; }
Elem& NodeList::Iterator::operator*()// reference to the element
{ return v−>elem; }
// compare positions
bool NodeList::Iterator::operator==(const Iterator& p) const
{ return v == p.v; }
bool NodeList::Iterator::operator!=(const Iterator& p) const
{ return v != p.v; }
// move to next position
NodeList::Iterator& NodeList::Iterator::operator++()
{ v = v−>next; return *this; }
// move to previous position
NodeList::Iterator& NodeList::Iterator::operator−−()
{ v = v−>prev; return *this; }
Code Fragment 6.8: Implementations of the Iterator member functions.
Page 268
244Chapter 6. List and Iterator ADTs
To keep the code simple, we have not implemented any error checking. We
assume that the functions of Code Fragment 6.8 are defined outside the class body.
Because of this, when referring to the nested class Iterator, we need to apply the
scope resolution operator, as in NodeList::Iterator, so the compiler knows that we
are referring to the iterator type associated with NodeList. Observe that the incre
ment and decrement operators not only update the position, but they also return a
reference to the updated position. This makes it possible to use the result of the
increment operation, as in “q = ++p.”
Having defined the supporting structures Node and Iterator, let us now present
the declaration of class NodeList, which is given in Code Fragment 6.9. The class
declaration begins by inserting the Node and Iterator definitions from Code Frag
ments 6.6 and 6.7. This is followed by the public members, that consist of a simple
default constructor and the members of the list ADT. We have omitted the standard
housekeeping functions from our class definition. These include the class destruc
tor, a copy constructor, and an assignment operator. The definition of the destructor
is important, since this class allocates memory, so it is necessary to delete this
memory when an object of this type is destroyed. We leave the implementation of
these housekeeping functions as an exercise (R-6.12).
typedef int Elem;// list base element type
class NodeList {// node-based list
private:
// insert Node declaration here. . .
public:
// insert Iterator declaration here. . .
public:
NodeList();// default constructor
int size() const;// list size
bool empty() const;// is the list empty?
Iterator begin() const;// beginning position
Iterator end() const;// (just beyond) last position
void insertFront(const Elem& e);// insert at front
void insertBack(const Elem& e);// insert at rear
void insert(const Iterator& p, const Elem& e); // insert e before p
void eraseFront();// remove first
void eraseBack();// remove last
void erase(const Iterator& p);// remove p
// housekeeping functions omitted. . .
private:// data members
int n;// number of items
Node* header;// head-of-list sentinel
Node* trailer;// tail-of-list sentinel
};
Code Fragment 6.9: Class NodeList realizing the C++-based list ADT.
Page 269
6.2. Lists245
The private data members include pointers to the header and trailer sentinel
nodes. In order to implement the function size efficiently, we also provide a variable
n, which stores the number of elements in the list.
Next, let us see how the various functions of class NodeList are implemented.
In Code Fragment 6.10, we begin by presenting a number of simple functions,
including the constructor, the size and empty functions, and the begin and end
functions. The constructor creates an initially empty list by setting n to zero, then
allocating the header and trailer nodes and linking them together. The function be
gin returns the position of the first node of the list, which is the node just following
the header sentinel, and the function end returns the position of the trailer. As de
sired, this is the position following the last element of the list. In both cases, we are
invoking the private constructor declared within class Iterator. We are allowed to
do so because NodeList is a friend of Iterator.
NodeList::NodeList() {// constructor
n = 0;// initially empty
header = new Node;// create sentinels
trailer = new Node;
header−>next = trailer;// have them point to each other
trailer−>prev = header;
} i
ni
nt NodeList::size() const// list size
{ return n; }
bool NodeList::empty() const// is the list empty?
{ return (n == 0); }
NodeList::Iterator NodeList::begin() const // begin position is first item
{ return Iterator(header−>next); }
NodeList::Iterator NodeList::end() const// end position is just beyond last
{ return Iterator(trailer); }
Code Fragment 6.10: Implementations of a number of simple member functions of
class NodeList.
Let us next see how to implement insertion. There are three different public
insertion functions, insert, insertFront, and insertBack, which are shown in Code
Fragment 6.11. The function insert(p,e) performs insertion into the doubly linked
list using the same approach that we was explained in Section 3.3. In particular, let
w be a pointer to p’s node, let u be a pointer to p’s predecessor. We create a new
node v, and link it before w and after u. Finally, we increment n to indicate that
there is one additional element in the list.
Page 270
246Chapter 6. List and Iterator ADTs
// insert e before p
void NodeList::insert(const NodeList::Iterator& p, const Elem& e) {
Node* w = p.v;// p’s node
Node* u = w−>prev;// p’s predecessor
Node* v = new Node;// new node to insert
v−>elem = e;
v−>next = w; w−>prev = v;// link in v before w
v−>prev = u; u−>next = v;// link in v after u
n++;
}
void NodeList::insertFront(const Elem& e) // insert at front
{ insert(begin(), e); }
void NodeList::insertBack(const Elem& e) // insert at rear
{ insert(end(), e); }
Code Fragment 6.11: Implementations of the insertion functions of class NodeList.
The function insertFront invokes insert on the beginning of the list, and the
function insertBack invokes insert on the list’s trailer.
Finally, in Code Fragment 6.12 we present the implementation of the erase
function, which removes a node from the list. Again, our approach follows directly
from the method described in Section 3.3 for removal of a node from a doubly
linked list. Let v be a pointer to the node to be deleted, and let w be its successor
and u be its predecessor. We unlink v by linking u and w to each other. Once v has
been unlinked from the list, we need to return its allocated storage to the system in
order to avoid any memory leaks. Finally, we decrement the number of elements in
the list.
void NodeList::erase(const Iterator& p) { // remove p
Node* v = p.v;// node to remove
Node* w = v−>next;// successor
Node* u = v−>prev;// predecessor
u−>next = w; w−>prev = u;// unlink p
delete v;// delete this node
n−−;// one fewer element
}
void NodeList::eraseFront()// remove first
{ erase(begin()); }
void NodeList::eraseBack()// remove last
{ erase(−−end()); }
Code Fragment 6.12: Implementations of the function erase of class NodeList.
Page 271
6.2. Lists247
There are number of other enhancements that could be added to our imple
mentation of NodeList, such as better error checking, a more sophisticated suite of
constructors, and a post-increment operator for the iterator class.
You might wonder why we chose to define the iterator function end to return an
imaginary position that lies just beyond the end of the list, rather than the last node
of the list. This choice offers a number of advantages. First, it is well defined, even
if the list is empty. Second, the function insert(p,e) can be used to insert a new
element at any position of the list. In particular, it is possible to insert an element
at the end of the list by invoking insert(end(),e). If we had instead defined end to
return the last position of the list, this would not be possible, and the only way to
insert an element at the end of the list would be through the insertBack function.
Observe that our implementation is quite efficient with respect to both time and
space. All of the operations of the list ADT run in time O(1). (The only exceptions
to this are the omitted housekeeping functions, the destructor, copy constructor, and
assignment operator. They require O(n) time, where n is the number of elements
in the list.) The space used by the data structure is proportional to the number of
elements in the list.
6.2.4 STL Lists
The C++ Standard Template Library provides an implementation of a list, which is
called list. Like the STL vector, the STL list is an example of an STL container. As
in our implementation of class NodeList, the STL list is implemented as a doubly
linked list.
In order to define an object to be of type list, it is necessary to first include the
appropriate system definition file, which is simply called “list.” As with the STL
vector, the list class is a member of the std namespace, it is necessary either to
preface references to it with the namespace resolution operator, as in “std::list”, or
to provide an appropriate using statement. The list class is templated with the base
type of the individual elements. For example, the code fragment below declares a
list of floats. By default, the initial list is empty.
#include <list>
using std::list;// make list accessible
list<float> myList;// an empty list of floats
Below is a list of the principal member functions of the list class. Let L be
declared to be an STL list of some base type, and let x denote a single object of this
same base type. (For example, L is a list of integers, and e is an integer.)
list(n): Construct a list with n elements; if no argument list is
given, an empty list is created.
size(): Return the number of elements in L.
Page 272
248Chapter 6. List and Iterator ADTs
empty(): Return true if L is empty and false otherwise.
front(): Return a reference to the first element of L.
back(): Return a reference to the last element of L.
push front(e): Insert a copy of e at the beginning of L.
push back(e): Insert a copy of e at the end of L.
pop front(): Remove the fist element of L.
pop back(): Remove the last element of L.
The functions push front and push back are the STL equivalents of our func
tions insertFront and insertBack, respectively, of our list ADT. Similarly, the func
tions pop front and pop back are equivalent to the respective functions eraseFront
and eraseBack.
Note that, when the base type of an STL vector is class object, all copying of
elements (for example, in push back) is performed by invoking the base class’s
copy constructor. Whenever elements are destroyed (for example, by invoking the
destroyer or the pop back member function) the class’s destructor is invoked on
each deleted element.
6.2.5 STL Containers and Iterators
In order to develop a fuller understanding of STL vectors and lists, it is necessary
to understand the concepts of STL containers and iterators. Recall that a container
is a data structure that stores a collection of elements. The STL provides a variety
of different container classes, many of which are discussed later.
STL ContainerDescription
vectorVector
dequeDouble ended queue
listList
stackLast-in, first-out stack
queueFirst-in, first-out queue
priority queuePriority queue
set (and multiset)Set (and multiset)
map (and multimap) Map (and multi-key map)
Different containers organize their elements in different ways, and hence sup
port different methods for accessing individual elements. STL iterators provide a
relatively uniform method for accessing and enumerating the elements stored in
containers.
Before introducing how iterators work for STL containers, let us begin with a
simple function that sums the elements of an STL vector, denoted by V, shown in
Page 273
6.2. Lists249
Code Fragment 6.13. The elements are accessed in the standard manner through
the indexing operator.
int vectorSum1(const vector<int>& V) {
int sum = 0;
for (int i = 0; i < V.size(); i++)
sum += V[i];
return sum;
}
Code Fragment 6.13: A simple C++ function that sums the entries of an STL vector.
This particular method of iterating through the elements of a vector should be
quite familiar by now. Unfortunately, this method would not be applicable to other
types of containers, because it relies on the fact that the elements of a vector can
be accessed efficiently through indexing. This is not true for all containers, such as
lists. What we desire is a uniform mechanism for accessing elements.
STL Iterators
Every STL container class defines a special associated class called an iterator. As
mentioned earlier, an iterator is an object that specifies a position within a container
and which is endowed with the ability to navigate to other positions. If p is an
iterator that refers to some position within a container, then *p yields a reference to
the associated element.
Advancing to the next element of the container is done by incrementing the
iterator. For example, either ++p or p++ advances p to point to the next ele
ment of the container. The former returns the updated value of the iterator, and
the latter returns its original value. (In our implementation of an iterator for class
NodeList in Code Fragment 6.7, we defined only the preincrement operator, but the
postincrement operator would be an easy extension. See Exercise R-6.13.)
Each STL container class provides two member functions, begin and end, each
of which returns an iterator for this container. The first returns an iterator that points
to the first element of the container, and the second returns an iterator that can be
thought of as pointing to an imaginary element just beyond the last element of the
container. An example for the case of lists was shown in Figure 6.5, and an example
of how this works for the STL vector is shown in Figure 6.6.
V [0]V.begin()V.end()
V [1] V [2]V [n − 1]
Figure 6.6: The special iterators V.begin() and V.end() for an STL vector V .
Page 274
250Chapter 6. List and Iterator ADTs
Using Iterators
Let us see how we can use iterators to enumerate the elements of an STL container
C. Suppose, for example, that C is of type vector<int>, that is, it is an STL list of
integers. The associated iterator type is denoted “vector<int>::iterator.” In general,
if C is an STL container of some type cont and the base type is of type base, then
the iterator type would be denoted “cont<base>::iterator.”
For example, Code Fragment 6.14 demonstrates how to sum the elements of an
STL vector V using an iterator. We begin by providing a type definition to the itera
tor type, called Iterator. We then create a loop, which is controlled by an iterator p.
We start with V.begin(), and we terminate when p reaches V.end(). Although this
approach is less direct than the approach based on indexing individual elements, it
has the advantage that it can be applied to any STL container class, not just vectors.
int vectorSum2(vector<int> V) {
typedef vector<int>::iterator Iterator;// iterator type
int sum = 0;
for (Iterator p = V.begin(); p != V.end(); ++p)
sum += *p;
return sum;
}
Code Fragment 6.14: Using an iterator to sum the elements of an STL vector.
Different containers provide iterators with different capabilities. Most STL
containers (including lists, sets, and maps) provide the ability to move not only
forwards, but backwards as well. For such containers the decrement operators – –p
and p– – are also defined for their iterators. This is called a bidirectional iterator.
A few STL containers (including vectors and deques) support the additional
feature of allowing the addition and subtraction of an integer. For example, for
such an iterator, p, the value p + 3 references the element three positions after p in
the container. This is called a random-access iterator.
As with pointers, care is needed in the use of iterators. For example, it is up to
the programmer to be sure that an iterator points to a valid element of the container
before attempting to dereference it. Attempting to dereference an invalid iterator
can result in your program aborting. As mentioned earlier, iterators can be invalid
for various reasons. For example, an iterator becomes invalid if the position that it
refers to is deleted.
Const Iterators
Observe that in Code Fragment 6.14, we passed the vector V into the function
by value (recall Section 1.4). This can be quite inefficient, because the system
constructs a complete copy of the actual argument. Since our function does not
Page 275
6.2. Lists251
modify V, the best approach would be to declare the argument to be a constant
reference instead, that is, “const vector<int>&.”
A problem arises, however, if we declare an iterator for such a vector. Many
STL implementations generate an error message if we attempt to use a regular iter
ator with a constant vector reference, since such an iterator may lead to an attempt
to modify the vector’s contents.
The solution is a special read-only iterator, called a const iterator. When using
a const iterator, it is possible to read the values of the container by dereferencing the
iterator, but it is not possible to modify the container’s values. For example, if p is a
const iterator, it is possible to read the value of *p, but you cannot assign it a value.
The const iterator type for our vector type is denoted “vector<int>::const iterator.”
We make use of the typedef command to rename this lengthy definition to the more
concise ConstIterator. The final code fragment is presented in Code Fragment 6.15.
int vectorSum3(const vector<int>& V) {
typedef vector<int>::const iterator ConstIterator; // iterator type
int sum = 0;
for (ConstIterator p = V.begin(); p != V.end(); ++p)
sum += *p;
return sum;
}
Code Fragment 6.15: Using a constant iterator to sum the elements of a vector.
STL Iterator-Based Container Functions
STL iterators offer a flexible and uniform way to access the elements of STL con
tainers. Many of the member functions and predefined algorithms that work with
STL containers use iterators as their arguments. Here are a few of the member
functions of the STL vector class that use iterators as arguments. Let V be an STL
vector of some given base type, and let e be an object of this base type. Let p and
q be iterators over this base type, both drawn from the same container.
vector(p,q): Construct a vector by iterating between p and q, copying
each of these elements into the new vector.
assign(p,q): Delete the contents of V, and assigns its new contents
by iterating between p and q and copying each of these
elements into V.
insert(p,e): Insert a copy of e just prior to the position given by iter
ator p and shifts the subsequent elements one position to
the right.
erase(p): Remove and destroy the element of V at the position
given by p and shifts the subsequent elements one po
Page 276
252Chapter 6. List and Iterator ADTs
sition to the left.
erase(p,q): Iterate between p and q, removing and destroying all
these elements and shifting subsequent elements to the
left to fill the gap.
clear(): Delete all these elements of V .
When presenting a range of iterator values (as we have done above with the
constructor V (p,q), assign(p,q), and erase(p,q)), the iterator range is understood
to start with p and end just prior to q. Borrowing from interval notation in math
ematics, this iterator range is often expressed as [p,q), implying that p is included
in the range, but q is not. This convention holds whenever dealing with iterator
ranges.
Note that the vector member functions insert and erase move elements around
in the vector. They should be used with care, because they can be quite slow. For
example, inserting or erasing an element at the beginning of a vector causes all the
later elements of the vector to be shifted one position.
The above functions are also defined for the STL list and the STL deque (but,
of course, the constructors are named differently). Since the list is defined as a dou
bly linked list, there is no need to shift elements when performing insert or erase.
These three STL containers (vector, list, and deque) are called sequence contain
ers, because they explicitly store elements in sequential order. The STL containers
set, multiset, map, and multimap support all of the above functions except assign.
They are called associated containers, because elements are typically accessed by
providing an associated key value. We discuss them in Chapter 9.
It is worthwhile noting that, in the constructor and assignment functions, the
iterators p and q do not need to be drawn from the same type of container as V , as
long as the container they are drawn from has the same base type. For example,
suppose that L is an STL list container of integers. We can create a copy of L in the
form of an STL vector V as follows:
list<int> L;// an STL list of integers
// . . .
vector<int> V(L.begin(), L.end());// initialize V to be a copy of L
The iterator-based form of the constructor is quite handy, since it provides an
easy way to initialize the contents of an STL container from a standard C++ array.
Here, we make use of a low-level feature of C++, which is inherited from its prede
cessor, the C programming language. Recall from Section 1.1.3 that a C++ array A
is represented as a pointer to its first element A[0]. In addition, A+1 points to A[1],
A + 2 points to A[2], and generally A + i points to A[i].
Addressing the elements of an array in this manner is called pointer arithmetic.
It is generally frowned upon as poor programming practice, but in this instance it
Page 277
6.2. Lists253
provides an elegant way to initialize a vector from an C++ array, as follows.
int A[ ] = {2, 5, −3, 8, 6};// a C++ array of 5 integers
vector<int> V(A, A+5);// V = (2, 5, -3, 8, 6)
Even though the pointers A and A + 5 are not STL iterators, through the magic of
pointer arithmetic, they essentially behave as though they were. This same trick
can be used to initialize any of the STL sequence containers.
STL Vectors and Algorithms
In addition to the above member functions for STL vectors, the STL also provides
a number of algorithms that operate on containers in general, and vectors in partic
ular. To access these functions, use the include statement “#include <algorithm>.”
Let p and q be iterators over some base type, and let e denote an object of this base
type. As above, these operations apply to the iterator range [p,q), which starts at p
and ends just prior to q.
sort(p,q): Sort the elements in the range from p to q in ascending
order. It is assumed that less-than operator (“<”) is de
fined for the base type.
random shuffle(p,q): Rearrange the elements in the range from p to q in ran
dom order.
reverse(p,q): Reverse the elements in the range from p to q.
find(p,q,e): Return an iterator to the first element in the range from p
to q that is equal to e; if e is not found, q is returned.
min element(p,q): Return an iterator to the minimum element in the range
from p to q.
max element(p,q): Return an iterator to the maximum element in the range
from p to q.
for each(p,q, f): Apply the function f the elements in the range from p to
q.
For example, to sort an entire vector V, we would use sort(V.begin(),V.end()).
To sort just the first 10 elements, we could use sort(V.begin(),V.begin()+10).
All of the above functions are supported for the STL deque. All of the above
functions, except sort and random shuffle are supported for the STL list.
An Illustrative Example
In Code Fragment 6.16, we provide a short example program of the functional
ity of the STL vector class. The program begins with the necessary “#include”
Page 278
254Chapter 6. List and Iterator ADTs
statements. We include <vector>, for the definitions of vectors and iterators, and
<algorithm>, for the definitions of sort and random shuffle.
We first initialize a standard C++ array containing six integers, and we then use
the iterator-based constructor to create a six-element vector containing these values.
We show how to use the member functions size, pop back, push back, front, and
back. Observe that popping decreases the array size and pushing increases the array
size. We then show how to use iterator arithmetic to sort a portion of the vector, in
this case, the first four elements. The call to the erase member function removes
two of the last four elements of the vector. After the removal, the remaining two
elements at the end have been shifted forward to fill the empty positions.
Next, we demonstrate how to generate a vector of characters. We apply the
function random shuffle to permute the elements of the vector randomly. Finally,
we show how to use the member function insert, to insert a character at the begin
ning of the vector. Observe how the other elements are shifted to the right.
#include <cstdlib>// provides EXIT SUCCESS
#include <iostream>// I/O definitions
#include <vector>// provides vector
#include <algorithm>// for sort, random shuffle
using namespace std;// make std:: accessible
int main () {
int a[ ] = {17, 12, 33, 15, 62, 45};
vector<int> v(a, a + 6);// v: 17 12 33 15 62 45
cout << v.size() << endl;// outputs: 6
v.pop back();// v: 17 12 33 15 62
cout << v.size() << endl;// outputs: 5
v.push back(19);// v: 17 12 33 15 62 19
cout << v.front() << " " << v.back() << endl; // outputs: 17 19
sort(v.begin(), v.begin() + 4);// v: (12 15 17 33) 62 19
v.erase(v.end() − 4, v.end() − 2);// v: 12 15 62 19
cout << v.size() << endl;// outputs: 4
char b[ ] = {’b’, ’r’, ’a’, ’v’, ’o’};
vector<char> w(b, b + 5);// w: b r a v o
random shuffle(w.begin(), w.end());// w: o v r a b
w.insert(w.begin(), ’s’);// w: s o v r a b
for (vector<char>::iterator p = w.begin(); p != w.end(); ++p)
cout << *p << " ";// outputs: s o v r a b
cout << endl;
return EXIT SUCCESS;
}
Code Fragment 6.16: An example of the use of the STL vector and iterators.
Page 279
6.3. Sequences255
6.3 Sequences
In this section, we define an abstract data type that generalizes the vector and list
ADTs. This ADT therefore provides access to its elements using both indices and
positions, and is a versatile data structure for a wide variety of applications.
6.3.1 The Sequence Abstract Data Type
A sequence is an ADT that supports all the functions of the list ADT (discussed in
Section 6.2), but it also provides functions for accessing elements by their index,
as we did in the vector ADT (discussed in Section 6.1). The interface consists of
the operations of the list ADT, plus the following two “bridging” functions, which
provide connections between indices and positions.
atIndex(i): Return the position of the element at index i.
indexOf(p): Return the index of the element at position p.
6.3.2 Implementing a Sequence with a Doubly Linked List
One possible implementation of a sequence, of course, is with a doubly linked list.
By doing so, all of the functions of the list ADT can be easily implemented to run
in O(1) time each. The functions atIndex and indexOf from the vector ADT can
also be implemented with a doubly linked list, though in a less efficient manner.
In particular, if we want the functions from the list ADT to run efficiently (using
position objects to indicate where accesses and updates should occur), then we
can no longer explicitly store the indices of elements in the sequence. Hence, to
perform the operation atIndex(i), we must perform link “hopping” from one of the
ends of the list until we locate the node storing the element at index i. As a slight
optimization, we could start hopping from the closest end of the sequence, which
would achieve a running time of
O(min(i + 1,n−i)).
This is still O(n) in the worst case (when i is near the middle index), but it would
be more efficient in applications where many calls to atIndex(i) are expected to
involve index values that are significantly closer to either end of the list. In our
implementation, we just use the simple approach of walking from the front of the
list, and we leave the two-sided solution as an exercise (R-6.14).
In Code Fragment 6.17, we present a definition of a class NodeSequence,
which implements the sequence ADT. Observe that, because a sequence extends
the definition of a list, we have inherited our class by extending the NodeList class
Page 280
256Chapter 6. List and Iterator ADTs
that was introduced in Section 6.2. We simply add definitions of our bridging
functions. Through the magic of inheritance, users of our class NodeSequence
have access to all the members of the NodeList class, including its nested class,
NodeList::Iterator.
class NodeSequence : public NodeList {
public:
Iterator atIndex(int i) const;// get position from index
int indexOf(const Iterator& p) const;// get index from position
};
Code Fragment 6.17: The definition of class NodeSequence, which implements the
sequence ADT using a doubly linked list.
Next, in Code Fragment 6.18, we show the implementations of the atIndex and
indexOf member functions. The function atIndex(i) hops i positions to the right,
starting at the beginning, and returns the resulting position. The function indexOf
hops through the list until finding the position that matches the given position p.
Observe that the conditional “q != p” uses the overloaded comparison operator for
positions defined in Code Fragment 6.8.
// get position from index
NodeSequence::Iterator NodeSequence::atIndex(int i) const {
Iterator p = begin();
for (int j = 0; j < i; j++) ++p;
return p;
}
// get index from position
int NodeSequence::indexOf(const Iterator& p) const {
Iterator q = begin();
int j = 0;
while (q != p) {// until finding p
++q; ++j;// advance and count hops
}
return j;
}
Code Fragment 6.18: Definition of the functions atIndex and indexOf of class Node
Sequence.
Both of these functions are quite fragile, and are likely to abort if their argu
ments are not in bounds. A more careful implementation of atIndex would first
check that the argument i lies in the range from 0 to n − 1, where n is the size of
the sequence. The function indexOf should check that it does not run past the end
of the sequence. In either case, an appropriate exception should be thrown.
Page 281
6.3. Sequences257
The worst-case running times of both of the functions atIndex and indexOf are
O(n), where n is the size of the list. Although this is not very efficient, we may
take consolation in the fact that all the other operations of the list ADT run in time
O(1). A natural alternative approach would be to implement the sequence ADT
using an array. Although we could now provide very efficient implementations of
atIndex and indexOf, the insertion and removal operations of the list ADT would
now require O(n) time. Thus, neither solution is perfect under all circumstances.
We explore this alternative in the next section.
6.3.3 Implementing a Sequence with an Array
Suppose we want to implement a sequence S by storing each element e of S in
a cell A[i] of an array A. We can define a position object p to hold an index i
and a reference to array A, as member variables. We can then implement function
element(p) by simply returning a reference to A[i]. A major drawback with this ap
proach, however, is that the cells in A have no way to reference their corresponding
positions. For example, after performing an insertFront operation, the elements
have been shifted to new positions, but we have no way of informing the exist
ing positions for S that the associated positions of their elements have changed.
(Remember that positions in a sequence are defined relative to their neighboring
positions, not their ranks.) Hence, if we are going to implement a general sequence
with an array, we need to find a different approach.
Consider an alternate solution in which, instead of storing the elements of S
in array A, we store a pointer to a new kind of position object in each cell of A.
Each new position object p stores a pair consisting of the index i and the element
e associated with p. We can easily scan through the array to update the i value
associated with each position whose rank changes as the result of an insertion or
deletion. An example is shown in Figure 6.7, where a new element containing BWI
is inserted at index 1 of an existing sequence. After the insertion, the elements PVD
and SFO are shifted to the right, so we increment the index value associated with
their index-element pairs.
0 JFK 1 PVD 2 SFO
A
0 1 2 3N-10 1 2 3N-10 JFK 1 BWI 2 PVD 3 SFO
A
Figure 6.7: An array-based implementation of the sequence ADT.
Page 282
258Chapter 6. List and Iterator ADTs
In this array-based implementation of a sequence, the functions insertFront,
insert, and erase take O(n) time because we have to shift position objects to make
room for the new position or to fill in the hole created by the removal of the old
position (just as in the insert and remove functions based on rank). All the other
position-based functions take O(1) time.
Note that we can use an array in a circular fashion, as we did for implementing a
queue (see Section 5.2.4). With a little work, we can then perform functions insert
Front in O(1) time. Note that functions insert and erase still take O(n) time. Now,
the worst case occurs when the element to be inserted or removed has rank ⌊n/2⌋.
Table 6.2 compares the running times of the implementations of the general
sequence ADT, by means of a circular array and a doubly linked list.
OperationsCircular Array List
size, emptyO(1)O(1)
atIndex, indexOfO(1)O(n)
begin, endO(1)O(1)
*p, ++p, – –pO(1)O(1)
insertFront, insertBackO(1)O(1)
insert, eraseO(n)O(1)
Table 6.2: Comparison of the running times of the functions of a sequence imple
mented with either an array (used in a circular fashion) or a doubly linked list. We
denote with n the number of elements in the sequence at the time the operation is
performed. The space usage is O(n) for the doubly linked list implementation, and
O(N) for the array implementation, where N is the size of the array.
Summarizing this table, we see that the array-based implementation is superior
to the linked-list implementation on the rank-based access operations, atIndex and
indexOf. It is equal in performance to the linked-list implementation on all the
other access operations. Regarding update operations, the linked-list implementa
tion beats the array-based implementation in the position-based update operations,
insert and erase. For update operations insertFront and insertBack, the two imple
mentations have comparable performance.
Considering space usage, note that an array requires O(N) space, where N is
the size of the array (unless we utilize an extendable array), while a doubly linked
list uses O(n) space, where n is the number of elements in the sequence. Since n is
less than or equal to N, this implies that the asymptotic space usage of a linked-list
implementation is superior to that of a fixed-size array, although there is a small
constant factor overhead that is larger for linked lists, since arrays do not need links
to maintain the ordering of their cells.
Page 283
6.4. Case Study: Bubble-Sort on a Sequence259
6.4 Case Study: Bubble-Sort on a Sequence
In this section, we illustrate the use of the sequence ADT and its implementation
trade-offs with sample C++ functions using the well-known bubble-sort algorithm.
6.4.1 The Bubble-Sort Algorithm
Consider a sequence of n elements such that any two elements in the sequence
can be compared according to an order relation (for example, companies compared
by revenue, states compared by population, or words compared lexicographically).
The sorting problem is to reorder the sequence so that the elements are in non
decreasing order. The bubble-sort algorithm (see Figure 6.8) solves this problem
by performing a series of passes over the sequence. In each pass, the elements
are scanned by increasing rank, from rank 0 to the end of the sequence. At each
position in a pass, an element is compared with its neighbor, and if these two con
secutive elements are found to be in the wrong relative order (that is, the preceding
element is larger than the succeeding one), then the two elements are swapped. The
sequence is sorted by completing n such passes.
passswapssequence
(5,7,2,6,9,3)
1st 7 ↔ 2 7 ↔ 6 9 ↔ 3 (5,2,6,7,3,9)
2nd5 ↔ 2 7 ↔ 3(2,5,6,3,7,9)
3rd6 ↔ 3(2,5,3,6,7,9)
4th5 ↔ 3(2,3,5,6,7,9)
Figure 6.8: The bubble-sort algorithm on a sequence of integers. For each pass, the
swaps performed and the sequence after the pass are shown.
The bubble-sort algorithm has the following properties:
• In the first pass, once the largest element is reached, it keeps on being swapped
until it gets to the last position of the sequence.
• In the second pass, once the second largest element is reached, it keeps on
being swapped until it gets to the second-to-last position of the sequence.
• In general, at the end of the ith pass, the right-most i elements of the sequence
(that is, those at indices from n−1 down to n−i) are in final position.
The last property implies that it is correct to limit the number of passes made
by a bubble-sort on an n-element sequence to n. Moreover, it allows the ith pass to
be limited to the first n−i + 1 elements of the sequence.
Page 284
260Chapter 6. List and Iterator ADTs
6.4.2 A Sequence-Based Analysis of Bubble-Sort
Assume that the implementation of the sequence is such that the accesses to ele
ments and the swaps of elements performed by bubble-sort take O(1) time each.
That is, the running time of the ith pass is O(n − i + 1). We have that the overall
running time of bubble-sort is
On∑i=1(n−i+ 1)!.
We can rewrite the sum inside the big-Oh notation as
n+ (n−1) +···+ 2 + 1 =n∑i=1i.
By Proposition 4.3, we have
n∑i=1i =n(n+ 1)
2.
Thus, bubble-sort runs in O(n2) time, provided that accesses and swaps can each
be implemented in O(1) time. As we see in future chapters, this performance for
sorting is quite inefficient. We discuss the bubble-sort algorithm here. Our aim is
to demonstrate, not as an example of a good sorting algorithm.
Code Fragments 6.19 and 6.20 present two implementations of bubble-sort on
a sequence of integers. The parameter S is of type Sequence, but we do not specify
whether it is a node-based or array-based implementation. The two bubble-sort
implementations differ in the preferred choice of functions to access and modify
the sequence. The first is based on accessing elements by their index. We use the
function atIndex to access the two elements of interest.
Since function bubbleSort1 accesses elements only through the index-based
interface functions atIndex, this implementation is suitable only for the array-based
implementation of the sequence, for which atIndex takes O(1) time. Given such an
array-based sequence, this bubble-sort implementation runs in O(n2) time.
On the other hand, if we had used our node-based implementation of the se
quence, each atIndex call would take O(n) time in the worst case. Since this func
tion is called with each iteration of the inner loop, the entire function would run in
O(n3) worst-case time, which is quite slow if n is large.
In contrast to bubbleSort1, our second function, bubbleSort2, accesses the el
ements entirely through the use of iterators. The iterators prec and succ play the
roles that indices j−1 and j play, respectively, in bubbleSort1. Observe that when
we first enter the inner loop of bubbleSort1, the value of j −1 is 0, that is, it refers
to the first element of the sequence. This is why we initialize prec to the beginning
Page 285
6.4. Case Study: Bubble-Sort on a Sequence261
void bubbleSort1(Sequence& S) {// bubble-sort by indices
int n = S.size();
for (int i = 0; i < n; i++) {// i-th pass
for (int j = 1; j < n−i; j++) {
Sequence::Iterator prec = S.atIndex(j−1);// predecessor
Sequence::Iterator succ = S.atIndex(j);// successor
if (*prec > *succ) {// swap if out of order
int tmp = *prec; *prec = *succ; *succ = tmp;
}
}
}
} Code Fragment 6.19: A C++ implementations of bubble-sort based on indices.
of the sequence before entering the inner loop. Whenever we reenter the inner loop,
we initialize succ to prec and then immediately increment it. Thus, succ refers to
the position immediately after prec. Before resuming the loop, we increment prec.
void bubbleSort2(Sequence& S) {// bubble-sort by positions
int n = S.size();
for (int i = 0; i < n; i++) {// i-th pass
Sequence::Iterator prec = S.begin();// predecessor
for (int j = 1; j < n−i; j++) {
Sequence::Iterator succ = prec;
++succ;// successor
if (*prec > *succ) {// swap if out of order
int tmp = *prec; *prec = *succ; *succ = tmp;
} ++prec;// advance predecessor
}
}
}
Code Fragment 6.20: Two C++ implementations of bubble-sort.
Since the iterator increment operator takes O(1) time in either the array-based
or node-based implementation of a sequence, this second implementation of bubble
sort would run in O(n2) worst-case time, irrespective of the manner in which the
sequence was implemented.
The two bubble-sort implementations given above show the importance of pro
viding efficient implementations of ADTs. Nevertheless, in spite of its implementa
tion simplicity, computing researchers generally feel that the bubble-sort algorithm
is not a good sorting method, because, even if implemented in the best possible
way, it still takes quadratic time. Indeed, there are much more efficient sorting
algorithms that run in O(nlogn) time. We explore these in Chapters 8 and 11.
Page 291
Chapter
7Trees
Contents
7.1 General Trees . . . . . . . . . . . . . . . . . . . . . . 268
7.1.1 Tree Definitions and Properties . . . . . . . . . . . . 269
7.1.2 Tree Functions . . . . . . . . . . . . . . . . . . . . . 272
7.1.3 A C++ Tree Interface . . . . . . . . . . . . . . . . . 273
7.1.4 A Linked Structure for General Trees . . . . . . . . . 274
7.2 Tree Traversal Algorithms . . . . . . . . . . . . . . . . 275
7.2.1 Depth and Height . . . . . . . . . . . . . . . . . . . 275
7.2.2 Preorder Traversal . . . . . . . . . . . . . . . . . . . 278
7.2.3 Postorder Traversal . . . . . . . . . . . . . . . . . . 281
7.3 Binary Trees . . . . . . . . . . . . . . . . . . . . . . . 284
7.3.1 The Binary Tree ADT . . . . . . . . . . . . . . . . . 285
7.3.2 A C++ Binary Tree Interface . . . . . . . . . . . . . 286
7.3.3 Properties of Binary Trees . . . . . . . . . . . . . . 287
7.3.4 A Linked Structure for Binary Trees . . . . . . . . . 289
7.3.5 A Vector-Based Structure for Binary Trees . . . . . . 295
7.3.6 Traversals of a Binary Tree . . . . . . . . . . . . . . 297
7.3.7 The Template Function Pattern . . . . . . . . . . . 303
7.3.8 Representing General Trees with Binary Trees . . . . 309
7.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 310
Page 292
268Chapter 7. Trees
7.1 General Trees
Productivity experts say that breakthroughs come by thinking “nonlinearly.” In
this chapter, we discuss one of the most important nonlinear data structures in
computing—trees. Tree structures are indeed a breakthrough in data organization,
for they allow us to implement a host of algorithms much faster than when using
linear data structures, such as lists, vectors, and sequences. Trees also provide a
natural organization for data, and consequently have become ubiquitous structures
in file systems, graphical user interfaces, databases, Web sites, and other computer
systems.
It is not always clear what productivity experts mean by “nonlinear” thinking,
but when we say that trees are “nonlinear,” we are referring to an organizational
relationship that is richer than the simple “before” and “after” relationships be
tween objects in sequences. The relationships in a tree are hierarchical, with some
objects being “above” and some “below” others. Actually, the main terminology
for tree data structures comes from family trees, with the terms “parent,” “child,”
“ancestor,” and “descendant” being the most common words used to describe rela
tionships. We show an example of a family tree in Figure 7.1.
Figure 7.1: A family tree showing some descendants of Abraham, as recorded in
Genesis, chapters 25–36.
Page 293
7.1. General Trees269
7.1.1 Tree Definitions and Properties
A tree is an abstract data type that stores elements hierarchically. With the excep
tion of the top element, each element in a tree has a parent element and zero or
more children elements. A tree is usually visualized by placing elements inside
ovals or rectangles, and by drawing the connections between parents and children
with straight lines. (See Figure 7.2.) We typically call the top element the root
of the tree, but it is drawn as the highest element, with the other elements being
connected below (just the opposite of a botanical tree).
Figure 7.2: A tree with 17 nodes representing the organizational structure of a fic
titious corporation. Electronics R’Us is stored at the root. The children of the root
store R&D, Sales, Purchasing, and Manufacturing. The internal nodes store Sales,
International, Overseas, Electronics R’Us, and Manufacturing.
Formal Tree Definition
Formally, we define tree T to be a set of nodes storing elements in a parent-child
relationship with the following properties:
• If T is nonempty, it has a special node, called the root of T, that has no
parent.
• Each node v of T different from the root has a unique parent node w; every
node with parent w is a child of w.
Note that according to our definition, a tree can be empty, meaning that it doesn’t
have any nodes. This convention also allows us to define a tree recursively, such
that a tree T is either empty or consists of a node r, called the root of T, and a
(possibly empty) set of trees whose roots are the children of r.
Page 294
270Chapter 7. Trees
Other Node Relationships
Two nodes that are children of the same parent are siblings. A node v is external
if v has no children. A node v is internal if it has one or more children. External
nodes are also known as leaves.
Example 7.1: In most operating systems, files are organized hierarchically into
nested directories (also called folders), which are presented to the user in the form
of a tree. (See Figure 7.3.) More specifically, the internal nodes of the tree are
associated with directories and the external nodes are associated with regular files.
In the UNIX and Linux operating systems, the root of the tree is appropriately
called the “root directory,” and is represented by the symbol “/.”
Figure 7.3: Tree representing a portion of a file system.
A node u is an ancestor of a node v if u = v or u is an ancestor of the parent
of v. Conversely, we say that a node v is a descendent of a node u if u is an ancestor
of v. For example, in Figure 7.3, cs252/ is an ancestor of papers/, and pr3 is a
descendent of cs016/. The subtree of T rooted at a node v is the tree consisting of
all the descendents of v in T (including v itself). In Figure 7.3, the subtree rooted at
cs016/ consists of the nodes cs016/, grades, homeworks/, programs/, hw1, hw2,
hw3, pr1, pr2, and pr3.
Page 295
7.1. General Trees271
Edges and Paths in Trees
An edge of tree T is a pair of nodes (u,v) such that u is the parent of v, or vice
versa. A path of T is a sequence of nodes such that any two consecutive nodes in
the sequence form an edge. For example, the tree in Figure 7.3 contains the path
(cs252/, projects/, demos/, market).
Example 7.2: When using single inheritance, the inheritance relation between
classes in a C++ program forms a tree. The base class is the root of the tree.
Ordered Trees
A tree is ordered if there is a linear ordering defined for the children of each node;
that is, we can identify children of a node as being the first, second, third, and so
on. Such an ordering is determined by how the tree is to be used, and is usually in
dicated by drawing the tree with siblings arranged from left to right, corresponding
to their linear relationship. Ordered trees typically indicate the linear order rela
tionship existing between siblings by listing them in a sequence or iterator in the
correct order.
Example 7.3: A structured document, such as a book, is hierarchically organized
as a tree whose internal nodes are chapters, sections, and subsections, and whose
external nodes are paragraphs, tables, figures, the bibliography, and so on. (See
Figure 7.4.) The root of the tree corresponds to the book itself. We could, in fact,
consider expanding the tree further to show paragraphs consisting of sentences,
sentences consisting of words, and words consisting of characters. In any case,
such a tree is an example of an ordered tree, because there is a well-defined ordering
among the children of each node.
Figure 7.4: An ordered tree associated with a book.
Page 296
272Chapter 7. Trees
7.1.2 Tree Functions
The tree ADT stores elements at the nodes of the tree. Because nodes are internal
aspects of our implementation, we do not allow access to them directly. Instead,
each node of the tree is associated with a position object, which provides public
access to nodes. For this reason, when discussing the public interfaces of functions
of our ADT, we use the notation p (rather than v) to clarify that the argument to
the function is a position and not a node. But, given the tight connection between
these two objects, we often blur the distinction between them, and use the terms
“position” and “node” interchangeably for trees.
As we did with positions for lists in Chapter 6, we exploit C++’s ability to
overload the dereferencing operator (“*”) to access the element associated with a
position. Given a position variable p, the associated element is accessed by *p.
This can be used both for reading and modifying the element’s value.
It is useful to store collections of positions. For example, the children of a node
in a tree can be presented to the user as such a list. We define position list, to be a
list whose elements are tree positions.
The real power of a tree position arises from its ability to access the neighboring
elements of the tree. Given a position p of tree T , we define the following:
p.parent(): Return the parent of p; an error occurs if p is the root.
p.children(): Return a position list containing the children of node p.
p.isRoot(): Return true if p is the root and false otherwise.
p.isExternal(): Return true if p is external and false otherwise.
If a tree T is ordered, then the list provided by p.children() provides access to the
children of p in order. If p is an external node, then p.children() returns an empty
list. If we wanted, we could also provide a function p.isInternal(), which would
simply return the complement of p.isExternal().
The tree itself provides the following functions. The first two, size and empty,
are just the standard functions that we defined for the other container types we al
ready saw. The function root yields the position of the root and positions produces
a list containing all the tree’s nodes.
size(): Return the number of nodes in the tree.
empty(): Return true if the tree is empty and false otherwise.
root(): Return a position for the tree’s root; an error occurs if the
tree is empty.
positions(): Return a position list of all the nodes of the tree.
We have not defined any specialized update functions for a tree here. Instead,
we prefer to describe different tree update functions in conjunction with specific
applications of trees in subsequent chapters. In fact, we can imagine several kinds
of tree update operations beyond those given in this book.
Page 297
7.1. General Trees273
7.1.3 A C++ Tree Interface
Let us present an informal C++ interface for the tree ADT. We begin by presenting
an informal C++ interface for the class Position, which represents a position in a
tree. This is given in Code Fragment 7.1.
template <typename E>// base element type
class Position<E> {// a node position
public:
E& operator*();// get element
Position parent() const;// get parent
PositionList children() const;// get node’s children
bool isRoot() const;// root node?
bool isExternal() const;// external node?
};
Code Fragment 7.1: An informal interface for a position in a tree (not a complete
C++ class).
We have provided a version of the derferencing operator (“*”) that returns a
standard (readable and writable) reference. (For simplicity, we did not provide a
version that returns a constant reference, but this would be an easy addition.)
Next, in Code Fragment 7.2, we present our informal C++ interface for a tree.
To keep the interface as simple as possible, we ignore error processing; hence, we
do not declare any exceptions to be thrown.
template <typename E>// base element type
class Tree<E> {
public:// public types
class Position;// a node position
class PositionList;// a list of positions
public:// public functions
int size() const;// number of nodes
bool empty() const;// is tree empty?
Position root() const;// get the root
PositionList positions() const;// get positions of all nodes
};
Code Fragment 7.2: An informal interface for the tree ADT (not a complete class).
Although we have not formally defined an interface for the class PositionList,
we may assume that it satisfies the standard list ADT as given in Chapter 6. In
our code examples, we assume that PositionList is implemented as an STL list of
objects of type Position, or more concretely, “std::list<Position>.” In particular, we
assume that PositionList provides an iterator type, which we simply call Iterator in
our later examples.
Page 298
274Chapter 7. Trees
7.1.4 A Linked Structure for General Trees
A natural way to realize a tree T is to use a linked structure, where we represent
each node of T by a position object p (see Figure 7.5(a)) with the following fields:
a reference to the node’s element, a link to the node’s parent, and some kind of
collection (for example, a list or array) to store links to the node’s children. If p
is the root of T, then the parent field of p is NULL. Also, we store a reference to
the root of T and the number of nodes of T in internal variables. This structure is
schematically illustrated in Figure 7.5(b).
elementparent
childrenContainerPVD
ATLBWIJFKLAX
(a)(b)
Figure 7.5: The linked structure for a general tree: (a) the node structure; (b) the
portion of the data structure associated with a node and its children.
Table 7.1 summarizes the performance of the linked-structure implementation
of a tree. The analysis is left as an exercise (C-7.27), but we note that, by using a
container to store the children of each node p, we can implement the children(p)
function by using the iterator for the container to enumerate its elements.
Operation Time
isRoot, isExternal O(1)
parent O(1)
children(p) O(cp)
size, empty O(1)
root O(1)
positions O(n)
Table 7.1: Running times of the functions of an n-node linked tree structure. Let cp
denote the number of children of a node p. The space usage is O(n).
Page 299
7.2. Tree Traversal Algorithms275
7.2 Tree Traversal Algorithms
In this section, we present algorithms for performing traversal computations on a
tree by accessing it through the tree ADT functions.
7.2.1 Depth and Height
Let p be a node of a tree T . The depth of p is the number of ancestors of p, exclud
ing p itself. For example, in the tree of Figure 7.2, the node storing International
has depth 2. Note that this definition implies that the depth of the root of T is 0.
The depth of p’s node can also be recursively defined as follows:
• If p is the root, then the depth of p is 0
• Otherwise, the depth of p is one plus the depth of the parent of p
Based on the above definition, the recursive algorithm depth(T, p) shown in
Code Fragment 7.3, computes the depth of a node referenced by position p of T by
calling itself recursively on the parent of p, and adding 1 to the value returned.
Algorithm depth(T, p):
if p.isRoot() then
return 0
else
return 1 + depth(T, p.parent())
Code Fragment 7.3: An algorithm to compute the depth of a node p in a tree T .
A simple C++ implementation of algorithm depth is shown in Code Fragment 7.4.
int depth(const Tree& T, const Position& p) {
if (p.isRoot())
return 0;// root has depth 0
else
return 1 + depth(T, p.parent());// 1 + (depth of parent)
}
Code Fragment 7.4: A C++ implementation of the algorithm of Code Fragment 7.3.
The running time of algorithm depth(T, p) is O(dp), where dp denotes the
depth of the node p in the tree T , because the algorithm performs a constant-time
recursive step for each ancestor of p. Thus, in the worst case, the depth algorithm
runs in O(n) time, where n is the total number of nodes in the tree T , since some
nodes may have this depth in T . Although such a running time is a function of
the input size, it is more accurate to characterize the running time in terms of the
parameter dp, since it is often much smaller than n.
Page 300
276Chapter 7. Trees
The height of a node p in a tree T is also defined recursively.
• If p is external, then the height of p is 0
• Otherwise, the height of p is one plus the maximum height of a child of p
The height of a tree T is the height of the root of T . For example, the tree of
Figure 7.2 has height 4. In addition, height can also be viewed as follows.
Proposition 7.4: The height of a tree is equal to the maximum depth of its exter
nal nodes.
We leave the justification of this fact to an exercise (R-7.7). Based on this
proposition, we present an algorithm, height1, for computing the height of a tree T .
It is shown in Code Fragment 7.5. It enumerates all the nodes in the tree and invokes
function depth (Code Fragment 7.3) to compute the depth of each external node.
Algorithm height1(T ):
h = 0
for each p ∈ T.positions() do
if p.isExternal() then
h = max(h,depth(T, p))
return h
Code Fragment 7.5: Algorithm height1(T ) for computing the height of a tree T
based on computing the maximum depth of the external nodes.
The C++ implementation of this algorithm is shown in Code Fragment 7.6. We
assume that Iterator is the iterator class for PositionList. Given such an iterator q,
we can access the associated position as *q.
int height1(const Tree& T) {
int h = 0;
PositionList nodes = T.positions();// list of all nodes
for (Iterator q = nodes.begin(); q != nodes.end(); ++q) {
if (q−>isExternal())
h = max(h, depth(T, *q));// get max depth among leaves
}
return h;
}
Code Fragment 7.6: A C++ implementation of the function height1.
Unfortunately, algorithm height1 is not very efficient. Since height1 calls algo
rithm depth(p) on each external node p of T , the running time of height1 is given
by O(n+∑p(1+dp)), where n is the number of nodes of T , dp is the depth of node
p, and E is the set of external nodes of T . In the worst case, the sum ∑p(1 + dp)
is proportional to n2. (See Exercise C-7.8.) Thus, algorithm height1 runs in O(n2)
time.
Page 301
7.2. Tree Traversal Algorithms277
Algorithm height2, shown in Code Fragment 7.7 and implemented in C++ in
Code Fragment 7.8, computes the height of tree T in a more efficient manner by
using the recursive definition of height.
Algorithm height2(T, p):
if p.isExternal() then
return 0
else
h = 0
for each q ∈ p.children() do
h = max(h,height2(T,q))
return 1 + h
Code Fragment 7.7: A more efficient algorithm for computing the height of the
subtree of tree T rooted at a node p.
int height2(const Tree& T, const Position& p) {
if (p.isExternal()) return 0;// leaf has height 0
int h = 0;
PositionList ch = p.children();// list of children
for (Iterator q = ch.begin(); q != ch.end(); ++q)
h = max(h, height2(T, *q));
return 1 + h;// 1 + max height of children
}
Code Fragment 7.8: Method height2 written in C++.
Algorithm height2 is more efficient than height1 (from Code Fragment 7.5).
The algorithm is recursive, and, if it is initially called on the root of T, it will
eventually be called on each node of T. Thus, we can determine the running time
of this method by summing, over all the nodes, the amount of time spent at each
node (on the nonrecursive part). Processing each node in children(p) takes O(cp)
time, where cp denotes the number of children of node p. Also, the while loop
has c
pp iterations and each iteration of the loop takes O(1) time plus the time for
the recursive call on a child of p. Thus, algorithm height2 spends O(1 + cp) time
at each node p, and its running time is O(∑p(1 + cp)). In order to complete the
analysis, we make use of the following property.
Proposition 7.5: Let T be a tree with n nodes, and let cp denote the number of
children of a node p of T. Then ∑pcp = n− 1.
Justification: Each node of T, with the exception of the root, is a child of another
node, and thus contributes one unit to the above sum.
By Proposition 7.5, the running time of algorithm height2, when called on the
root of T, is O(n), where n is the number of nodes of T.
Page 302
278Chapter 7. Trees
7.2.2 Preorder Traversal
A traversal of a tree T is a systematic way of accessing, or “visiting,” all the nodes
of T. In this section, we present a basic traversal scheme for trees, called pre
order traversal. In the next section, we study another basic traversal scheme, called
postorder traversal.
In a preorder traversal of a tree T, the root of T is visited first and then the
subtrees rooted at its children are traversed recursively. If the tree is ordered, then
the subtrees are traversed according to the order of the children. The specific action
associated with the “visit” of a node depends on the application of this traversal, and
could involve anything from incrementing a counter to performing some complex
computation for this node. The pseudo-code for the preorder traversal of the subtree
rooted at a node referenced by position p is shown in Code Fragment 7.9. We
initially invoke this routine with the call preorder(T,T.root()).
Algorithm preorder(T, p):
perform the “visit” action for node p
for each child q of p do
recursively traverse the subtree rooted at q by calling preorder(T,q)
Code Fragment 7.9: Algorithm preorder for performing the preorder traversal of the
subtree of a tree T rooted at a node p.
The preorder traversal algorithm is useful for producing a linear ordering of
the nodes of a tree where parents must always come before their children in the
ordering. Such orderings have several different applications. We explore a simple
instance of such an application in the next example.
Paper
Title Abstract § 1§ 2§ 3 References
§ 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2
Figure 7.6: Preorder traversal of an ordered tree, where the children of each node
are ordered from left to right.
Page 303
7.2. Tree Traversal Algorithms279
Example 7.6: The preorder traversal of the tree associated with a document, as in
Example 7.3, examines an entire document sequentially, from beginning to end. If
the external nodes are removed before the traversal, then the traversal examines the
table of contents of the document. (See Figure 7.6.)
The preorder traversal is also an efficient way to access all the nodes of a tree.
To justify this, let us consider the running time of the preorder traversal of a tree
T with n nodes under the assumption that visiting a node takes O(1) time. The
analysis of the preorder traversal algorithm is actually similar to that of algorithm
height2 (Code Fragment 7.8), given in Section 7.2.1. At each node p, the nonre
cursive part of the preorder traversal algorithm requires time O(1 + cp), where cp
is the number of children of p. Thus, by Proposition 7.5, the overall running time
of the preorder traversal of T is O(n).
Algorithm preorderPrint(T, p), implemented in C++ in Code Fragment 7.10,
performs a preorder printing of the subtree of a node p of T, that is, it performs the
preorder traversal of the subtree rooted at p and prints the element stored at a node
when the node is visited. Recall that, for an ordered tree T, function T.children(p)
returns an iterator that accesses the children of p in order. We assume that Iterator
is this iterator type. Given an iterator q, the associated position is given by *q.
void preorderPrint(const Tree& T, const Position& p) {
cout << *p;// print element
PositionList ch = p.children();// list of children
for (Iterator q = ch.begin(); q != ch.end(); ++q) {
cout << " ";
preorderPrint(T, *q);
}
}
Code Fragment 7.10: Method preorderPrint(T, p) that performs a preorder printing
of the elements in the subtree associated with position p of T.
There is an interesting variation of the preorderPrint function that outputs a
different representation of an entire tree. The parenthetic string representation
P(T) of tree T is recursively defined as follows. If T consists of a single node
referenced by a position p, then
P(T) = *p.
Otherwise,
P(T) = *p+ "("+ P(T1)+ P(T2)+ ··· + P(Tk) + ")",
where p is the root position of T and T1,T2,...,Tk are the subtrees rooted at the
children of p, which are given in order if T is an ordered tree.
Page 304
280Chapter 7. Trees
Note that the definition of P(T) is recursive. Also, we are using “+” here
to denote string concatenation. (Recall the string type from Section 1.1.3.) The
parenthetic representation of the tree of Figure 7.2 is shown in Figure 7.7.
Electronics R’Us (
R&D
Sales (
Domestic
International (
Canada
S.America
Overseas ( Africa Europe Asia Australia ) ) )
Purchasing
Manufacturing ( TV CD Tuner ) )
Figure 7.7: Parenthetic representation of the tree of Figure 7.2. Indentation, line
breaks, and spaces have been added for clarity.
Note that, technically speaking, there are some computations that occur be
tween and after the recursive calls at a node’s children in the above algorithm. We
still consider this algorithm to be a preorder traversal, however, since the primary
action of printing a node’s contents occurs prior to the recursive calls.
The C++ function parenPrint, shown in Code Fragment 7.11, is a variation of
function preorderPrint (Code Fragment 7.10). It implements the definition given
above to output a parenthetic string representation of a tree T. It first prints the ele
ment associated with each node. For each internal node, we first print “(”, followed
by the parenthetical representation of each of its children, followed by “)”.
void parenPrint(const Tree& T, const Position& p) {
cout << *p;// print node’s element
if (!p.isExternal()) {
PositionList ch = p.children();// list of children
cout << "( ";// open
for (Iterator q = ch.begin(); q != ch.end(); ++q) {
if (q != ch.begin()) cout << " ";// print separator
parenPrint(T, *q);// visit the next child
}
cout << " )";// close
}
}
Code Fragment 7.11: A C++ implementation of algorithm parenPrint.
We explore a modification of Code Fragment 7.11 in Exercise R-7.10, to dis
play a tree in a fashion more closely matching that given in Figure 7.7.
Page 305
7.2. Tree Traversal Algorithms281
7.2.3 Postorder Traversal
Another important tree traversal algorithm is the postorder traversal. This algo
rithm can be viewed as the opposite of the preorder traversal, because it recursively
traverses the subtrees rooted at the children of the root first, and then visits the root.
It is similar to the preorder traversal, however, in that we use it to solve a particular
problem by specializing an action associated with the “visit” of a node p. Still,
as with the preorder traversal, if the tree is ordered, we make recursive calls for
the children of a node p according to their specified order. Pseudo-code for the
postorder traversal is given in Code Fragment 7.12.
Algorithm postorder(T, p):
for each child q of p do
recursively traverse the subtree rooted at q by calling postorder(T,q)
perform the “visit” action for node p
Code Fragment 7.12: Algorithm postorder for performing the postorder traversal of
the subtree of a tree T rooted at a node p.
The name of the postorder traversal comes from the fact that this traversal
method visits a node p after it has visited all the other nodes in the subtree rooted
at p. (See Figure 7.8.)
Paper
Title Abstract § 1§ 2§ 3 References
§ 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2
Figure 7.8: Postorder traversal of the ordered tree of Figure 7.6.
The analysis of the running time of a postorder traversal is analogous to that of
a preorder traversal. (See Section 7.2.2.) The total time spent in the nonrecursive
portions of the algorithm is proportional to the time spent visiting the children of
each node in the tree. Thus, a postorder traversal of a tree T with n nodes takes
O(n) time, assuming that visiting each node takes O(1) time. That is, the postorder
traversal runs in linear time.
In Code Fragment 7.13, we present a C++ function postorderPrint which per
Page 306
282Chapter 7. Trees
forms a postorder traversal of a tree T. This function prints the element stored at a
node when it is visited.
void postorderPrint(const Tree& T, const Position& p) {
PositionList ch = p.children();// list of children
for (Iterator q = ch.begin(); q != ch.end(); ++q) {
postorderPrint(T, *q);
cout << " ";
}
cout << *p;// print element
}
Code Fragment 7.13: The function postorderPrint(T, p), which prints the elements
of the subtree of position p of T.
The postorder traversal method is useful for solving problems where we wish
to compute some property for each node p in a tree, but computing that property
for p requires that we have already computed that same property for p’s children.
Such an application is illustrated in the following example.
Example 7.7: Consider a file-system tree T, where external nodes represent files
and internal nodes represent directories (Example 7.1). Suppose we want to com
pute the disk space used by a directory, which is recursively given by the sum of
the following (see Figure 7.9):
• The size of the directory itself
• The sizes of the files in the directory
• The space used by the children directories
Figure 7.9: The tree of Figure 7.3 representing a file system, showing the name and
size of the associated file/directory inside each node, and the disk space used by the
associated directory above each internal node.
Page 307
7.2. Tree Traversal Algorithms283
This computation can be done with a postorder traversal of tree T. After the
subtrees of an internal node p have been traversed, we compute the space used by
p by adding the sizes of the directory p itself and of the files contained in p, to
the space used by each internal child of p, which was computed by the recursive
postorder traversals of the children of p.
Motivated by Example 7.7, algorithm diskSpace, which is presented in Code
Fragment 7.14, performs a postorder traversal of a file-system tree T, printing the
name and disk space used by the directory associated with each internal node of
T. When called on the root of tree T, diskSpace runs in time O(n), where n is the
number of nodes of the tree, provided the auxiliary functions name(p) and size(p)
take O(1) time.
int diskSpace(const Tree& T, const Position& p) {
int s = size(p);// start with size of p
if (!p.isExternal()) {// if p is internal
PositionList ch = p.children();// list of p’s children
for (Iterator q = ch.begin(); q != ch.end(); ++q)
s += diskSpace(T, *q);// sum the space of subtrees
cout << name(p) << ": " << s << endl; // print summary
}
return s;
}
Code Fragment 7.14: The function diskSpace, which prints the name and disk space
used by the directory associated with p, for each internal node p of a file-system
tree T. This function calls the auxiliary functions name and size, which should be
defined to return the name and size of the file/directory associated with a node.
Other Kinds of Traversals
Preorder traversal is useful when we want to perform an action for a node and then
recursively perform that action for its children, and postorder traversal is useful
when we want to first perform an action on the descendents of a node and then
perform that action on the node.
Although the preorder and postorder traversals are common ways of visiting
the nodes of a tree, we can also imagine other traversals. For example, we could
traverse a tree so that we visit all the nodes at depth d before we visit the nodes at
depth d+1. Such a traversal, called a breadth-first traversal, could be implemented
using a queue, whereas the preorder and postorder traversals use a stack. (This stack
is implicit in our use of recursion to describe these functions, but we could make
this use explicit, as well, to avoid recursion.) In addition, binary trees, which we
discuss next, support an additional traversal method known as the inorder traversal.
Page 308
284Chapter 7. Trees
7.3 Binary Trees
A binary tree is an ordered tree in which every node has at most two children.
1. Every node has at most two children.
2. Each child node is labeled as being either a left child or a right child.
3. A left child precedes a right child in the ordering of children of a node.
The subtree rooted at a left or right child of an internal node is called the node’s
left subtree or right subtree, respectively. A binary tree is proper if each node has
either zero or two children. Some people also refer to such trees as being full binary
trees. Thus, in a proper binary tree, every internal node has exactly two children.
A binary tree that is not proper is improper.
Example 7.8: An important class of binary trees arises in contexts where we wish
to represent a number of different outcomes that can result from answering a series
of yes-or-no questions. Each internal node is associated with a question. Starting at
the root, we go to the left or right child of the current node, depending on whether
the answer to the question is “Yes” or “No.” With each decision, we follow an
edge from a parent to a child, eventually tracing a path in the tree from the root
to an external node. Such binary trees are known as decision trees, because each
external node p in such a tree represents a decision of what to do if the questions
associated with p’s ancestors are answered in a way that leads to p. A decision
tree is a proper binary tree. Figure 7.10 illustrates a decision tree that provides
recommendations to a prospective investor.
Figure 7.10: A decision tree providing investment advice.
Page 309
7.3. Binary Trees285
Example 7.9: An arithmetic expression can be represented by a tree whose exter
nal nodes are associated with variables or constants, and whose internal nodes are
associated with one of the operators +, −,−, ×, and /. (See Figure 7.11.) Each node
in such a tree has a value associated with it.
• If a node is external, then its value is that of its variable or constant.
• If a node is internal, then its value is defined by applying its operation to the
values of its children.
Such an arithmetic-expression tree is a proper binary tree, since each of the oper
ators +, −,−, ×, and / take exactly two operands. Of course, if we were to allow
for unary operators, like negation (−), as in “−x,” then we could have an improper
binary tree.
Figure 7.11: A binary tree representing an arithmetic expression. This tree repre
sents the expression ((((3+1)×3)/((9−5)+2))−((3×(7−4))+6)). The value
associated with the internal node labeled “/” is 2.
A Recursive Binary Tree Definition
Incidentally, we can also define a binary tree in a recursive way such that a binary
tree is either empty or consists of:
• A node r, called the root of T and storing an element
• A binary tree, called the left subtree of T
• A binary tree, called the right subtree of T
We discuss some of the specialized topics for binary trees below.
7.3.1 The Binary Tree ADT
In this section, we introduce an abstract data type for a binary tree. As with our
earlier tree ADT, each node of the tree stores an element and is associated with a
Page 310
286Chapter 7. Trees
position object, which provides public access to nodes. By overloading the deref
erencing operator, the element associated with a position p can be accessed by *p.
In addition, a position p supports the following operations.
p.left(): Return the left child of p; an error condition occurs if p
is an external node.
p.right(): Return the right child of p; an error condition occurs if p
is an external node.
p.parent(): Return the parent of p; an error occurs if p is the root.
p.isRoot(): Return true if p is the root and false otherwise.
p.isExternal(): Return true if p is external and false otherwise.
The tree itself provides the same operations as the standard tree ADT. Recall
that a position list is a list of tree positions.
size(): Return the number of nodes in the tree.
empty(): Return true if the tree is empty and false otherwise.
root(): Return a position for the tree’s root; an error occurs if the
tree is empty.
positions(): Return a position list of all the nodes of the tree.
As in Section 7.1.2 for the tree ADT, we do not define specialized update functions
for binary trees, but we consider them later.
7.3.2 A C++ Binary Tree Interface
Let us present an informal C++ interface for the binary tree ADT. We begin in
Code Fragment 7.15 by presenting an informal C++ interface for the class Posi
tion, which represents a position in a tree. It differs from the tree interface of Sec
tion 7.1.3 by replacing the tree member function children with the two functions
left and right.
template <typename E>// base element type
class Position<E> {// a node position
public:
E& operator*();// get element
Position left() const;// get left child
Position right() const;// get right child
Position parent() const;// get parent
bool isRoot() const;// root of tree?
bool isExternal() const;// an external node?
};
Code Fragment 7.15: An informal interface for the binary tree ADT (not a complete
C++ class).
Page 311
7.3. Binary Trees287
Next, in Code Fragment 7.16, we present an informal C++ interface for a binary
tree. To keep the interface as simple as possible, we have ignored error processing,
and hence we do not declare any exceptions to be thrown.
template <typename E>// base element type
class BinaryTree<E> {// binary tree
public:// public types
class Position;// a node position
class PositionList;// a list of positions
public:// member functions
int size() const;// number of nodes
bool empty() const;// is tree empty?
Position root() const;// get the root
PositionList positions() const;// list of nodes
};
Code Fragment 7.16: An informal interface for the binary tree ADT (not a complete
C++ class).
Although we have not formally defined an interface for the class PositionList,
we may assume that it satisfies the standard list ADT as given in Chapter 6. In
our code examples, we assume that PositionList is implemented as an STL list of
objects of type Position.
7.3.3 Properties of Binary Trees
Binary trees have several interesting properties dealing with relationships between
their heights and number of nodes. We denote the set of all nodes of a tree T, at the
same depth d, as the level d of T. In a binary tree, level 0 has one node (the root),
level 1 has, at most, two nodes (the children of the root), level 2 has, at most, four
nodes, and so on. (See Figure 7.12.) In general, level d has, at most, 2d nodes.
We can see that the maximum number of nodes on the levels of a binary tree
grows exponentially as we go down the tree. From this simple observation, we can
derive the following properties relating the height of a binary T to its number of
nodes. A detailed justification of these properties is left as an exercise (R-7.16).
Proposition 7.10: Let T be a nonempty binary tree, and let n, nE, nI and h denote
the number of nodes, number of external nodes, number of internal nodes, and
height of T, respectively. Then T has the following properties:
1. h+1 ≤ n ≤ 2h+1 − 1
2. 1 ≤ nE ≤ 2h
3. h ≤ nI ≤ 2h − 1
4. log(n+1)− 1 ≤ h ≤ n− 1
Also, if T is proper, then it has the following properties:
Page 312
288Chapter 7. Trees
1. 2h+1 ≤ n ≤ 2h+1 −1
2. h+1 ≤ nE ≤ 2h
3. h ≤ nI ≤ 2h −1
4. log(n+1)−1 ≤ h ≤ (n−1)/2
Figure 7.12: Maximum number of nodes in the levels of a binary tree.
In addition to the binary tree properties above, we also have the following re
lationship between the number of internal nodes and external nodes in a proper
binary tree.
Proposition 7.11: In a nonempty proper binary tree T , the number of external
nodes is one more than the number of internal nodes.
Justification: We can see this using an argument based on induction. If the
tree consists of a single root node, then clearly we have one external node and no
internal nodes, so the proposition holds.
If, on the other hand, we have two or more, then the root has two subtrees. Since
the subtrees are smaller than the original tree, we may assume that they satisfy the
proposition. Thus, each subtree has one more external node than internal nodes.
Between the two of them, there are two more external nodes than internal nodes.
But, the root of the tree is an internal node. When we consider the root and both
subtrees together, the difference between the number of external and internal nodes
is 2−1 = 1, which is just what we want.
Note that the above relationship does not hold, in general, for improper binary
trees and nonbinary trees, although there are other interesting relationships that can
hold as we explore in an exercise (C-7.9).
Page 313
7.3. Binary Trees289
7.3.4 A Linked Structure for Binary Trees
In this section, we present an implementation of a binary tree T as a linked struc
ture, called LinkedBinaryTree. We represent each node v of T by a node object
storing the associated element and pointers to its parent and two children. (See
Figure 7.13.) For simplicity, we assume the tree is proper, meaning that each node
has either zero or two children.
Figure 7.13: A node in a linked data structure for representing a binary tree.
In Figure 7.14, we show a linked structure representation of a binary tree. The
structure stores the tree’s size, that is, the number of nodes in the tree, and a pointer
to the root of the tree. The rest of the structure consists of the nodes linked together
appropriately. If v is the root of T, then the pointer to the parent node is NULL, and
if v is an external node, then the pointers to the children of v are NULL.
Figure 7.14: An example of a linked data structure for representing a binary tree.
Page 314
290Chapter 7. Trees
We begin by defining the basic constituents that make up the LinkedBinaryTree
class. The most basic entity is the structure Node, shown in Code Fragment 7.17,
that represents a node of the tree.
struct Node {// a node of the tree
Elem elt;// element value
Node* par;// parent
Node* left;// left child
Node* right;// right child
Node() : elt(), par(NULL), left(NULL), right(NULL) { } // constructor
};
Code Fragment 7.17: Structure Node implementing a node of a binary tree. It is
nested in the protected section of class BinaryTree.
Although all its members are public, class Node is declared within the protected
section of the LinkedBinaryTree class. Thus, it is not publicly accessible. Each
node has a member variable elt, which contains the associated element, and pointers
par, left, and right, which point to the associated relatives.
Next, we define the public class Position in Code Fragment 7.18. Its data mem
ber consists of a pointer v to a node of the tree. Access to the node’s element
is provided by overloading the dereferencing operator (“*”). We declare Linked
BinaryTree to be a friend, providing it access to the private data.
class Position {// position in the tree
private:
Node* v;// pointer to the node
public:
Position(Node* v = NULL) : v( v) { }// constructor
Elem& operator*()// get element
{ return v−>elt; }
Position left() const// get left child
{ return Position(v−>left); }
Position right() const// get right child
{ return Position(v−>right); }
Position parent() const// get parent
{ return Position(v−>par); }
bool isRoot() const// root of the tree?
{ return v−>par == NULL; }
bool isExternal() const// an external node?
{ return v−>left == NULL && v−>right == NULL; }
friend class LinkedBinaryTree;// give tree access
};
typedef std::list<Position> PositionList;// list of positions
Code Fragment 7.18: Class Position implementing a position in a binary tree. It is
nested in the public section of class LinkedBinaryTree.
Page 315
7.3. Binary Trees291
Most of the functions of class Position simply involve accessing the appropriate
members of the Node structure. We have also included a declaration of the class
PositionList, as an STL list of positions. This is used to represent collections of
nodes. To keep the code simple, we have omitted error checking, and, rather than
using templates, we simply provide a type definition for the base element type,
called Elem. (See Exercise P-7.2.)
We present the major part of the class LinkedBinaryTree in Code Fragment 7.19.
The class declaration begins by inserting the above declarations of Node and Po
sition. This is followed by a declaration of the public members, local utility func
tions, and the private member data. We have omitted housekeeping functions, such
as a destructor, assignment operator, and copy constructor.
typedef int Elem;// base element type
class LinkedBinaryTree {
protected:
// insert Node declaration here. . .
public:
// insert Position declaration here. . .
public:
LinkedBinaryTree();// constructor
int size() const;// number of nodes
bool empty() const;// is tree empty?
Position root() const;// get the root
PositionList positions() const;// list of nodes
void addRoot();// add root to empty tree
void expandExternal(const Position& p);// expand external node
Position removeAboveExternal(const Position& p); // remove p and parent
// housekeeping functions omitted. . .
protected:// local utilities
void preorder(Node* v, PositionList& pl) const; // preorder utility
private:
Node* root;// pointer to the root
int n;// number of nodes
};
Code Fragment 7.19: Implementation of a LinkedBinaryTree class.
The private data for class LinkedBinaryTree consists of a pointer root to the
root node and a variable n, containing the number of nodes in the tree. (We added
the underscore to the name root to avoid a name conflict with the member function
root.) In addition to the functions of the ADT, we have introduced a few update
functions, addRoot, expandExternal, and removeAboveExternal, which provide
the means to build and modify trees. They are discussed below. We define a utility
function preorder, which is used in the implementation of the function positions.
In Code Fragment 7.20, we present the definitions of the constructor and sim
Page 316
292Chapter 7. Trees
pler member functions of class LinkedBinaryTree. The function addRoot assumes
that the tree is empty, and it creates a single root node. (It should not be invoked if
the tree is nonempty, since otherwise a memory leak results.)
LinkedBinaryTree::LinkedBinaryTree()// constructor
: root(NULL), n(0) { }
int LinkedBinaryTree::size() const// number of nodes
{ return n; }
bool LinkedBinaryTree::empty() const// is tree empty?
{ return size() == 0; }
LinkedBinaryTree::Position LinkedBinaryTree::root() const // get the root
{ return Position( root); }
void LinkedBinaryTree::addRoot()// add root to empty tree
{ root = new Node; n = 1; }
Code Fragment 7.20: Simple member functions for class LinkedBinaryTree.
Binary Tree Update Functions
In addition to the BinaryTree interface functions and addRoot, the class LinkedBi
naryTree also includes the following update functions given a position p. The first
is used for adding nodes to the tree and the second is used for removing nodes.
expandExternal(p): Transform p from an external node into an internal node
by creating two new external nodes and making them the
left and right children of p, respectively; an error condi
tion occurs if p is an internal node.
removeAboveExternal(p): Remove the external node p together with its parent q,
replacing q with the sibling of p (see Figure 7.15, where
p’s node is w and q’s node is v); an error condition occurs
if p is an internal node or p is the root.
(a)(b)(c)
Figure 7.15: Operation removeAboveExternal(p), which removes the external node
w to which p refers and its parent node v.
Page 317
7.3. Binary Trees293
The function expandExternal(p) is shown in Code Fragment 7.21. Letting v
be p’s associated node, it creates two new nodes. One becomes v’s left child and
the other becomes v’s right child. The constructor for Node initializes the node’s
pointers to NULL, so we need only update the new node’s parent links.
// expand external node
void LinkedBinaryTree::expandExternal(const Position& p) {
Node* v = p.v;// p’s node
v−>left = new Node;// add a new left child
v−>left−>par = v;// v is its parent
v−>right = new Node;// and a new right child
v−>right−>par = v;// v is its parent
n += 2;// two more nodes
}
Code Fragment 7.21: The function expandExternal(p) of class LinkedBinaryTree.
The function removeAboveExternal(p) is shown in Code Fragment 7.22. Let
w be p’s associated node and let v be its parent. We assume that w is external and
is not the root. There are two cases. If w is a child of the root, removing w and its
parent (the root) causes w’s sibling to become the tree’s new root. If not, we replace
w’s parent with w’s sibling. This involves finding w’s grandparent and determining
whether v is the grandparent’s left or right child. Depending on which, we set the
link for the appropriate child of the grandparent. After unlinking w and v, we delete
these nodes. Finally, we update the number of nodes in the tree.
LinkedBinaryTree::Position// remove p and parent
LinkedBinaryTree::removeAboveExternal(const Position& p) {
Node* w = p.v; Node* v = w−>par;// get p’s node and parent
Node* sib = (w == v−>left ? v−>right : v−>left);
if (v == root) {// child of root?
root = sib;// . . .make sibling root
sib−>par = NULL;
} else {
Node* gpar = v−>par;// w’s grandparent
if (v == gpar−>left) gpar−>left = sib;// replace parent by sib
else gpar−>right = sib;
sib−>par = gpar;
}
delete w; delete v;// delete removed nodes
n −= 2;// two fewer nodes
return Position(sib);
}
Code Fragment 7.22: An implementation of the function removeAboveExternal(p).
Page 318
294Chapter 7. Trees
The function positions is shown in Code Fragment 7.23. It invokes the utility
function preorder, which traverses the tree and stores the node positions in an STL
vector.
// list of all nodes
LinkedBinaryTree::PositionList LinkedBinaryTree::positions() const {
PositionList pl;
preorder( root, pl);// preorder traversal
return PositionList(pl);// return resulting list
}
// preorder traversal
void LinkedBinaryTree::preorder(Node* v, PositionList& pl) const {
pl.push back(Position(v));// add this node
if (v−>left != NULL)// traverse left subtree
preorder(v−>left, pl);
if (v−>right != NULL)// traverse right subtree
preorder(v−>right, pl);
}
Code Fragment 7.23: An implementation of the function positions.
We have omitted the housekeeping functions (the destructor, copy constructor,
and assignment operator). We leave these as exercises (Exercise C-7.22), but they
also involve performing a traversal of the tree.
Performance of the LinkedBinaryTree Implementation
Let us now analyze the running times of the functions of class LinkedBinaryTree,
which uses a linked structure representation.
• Each of the position functions left, right, parent, isRoot, and isExternal takes
O(1) time.
• By accessing the member variable n, which stores the number of nodes of T,
functions size and empty each run in O(1) time.
• The accessor function root runs in O(1) time.
• The update functions expandExternal and removeAboveExternal visit only
a constant number of nodes, so they both run in O(1) time.
• Function positions is implemented by performing a preorder traversal, which
takes O(n) time. (We discuss three different binary-tree traversals in Sec
tion 7.3.6. Any of these suffice.) The nodes visited by the traversal are each
added in O(1) time to an STL list. Thus, function positions takes O(n) time.
Table 7.2 summarizes the performance of this implementation of a binary tree.
There is an object of class Node (Code Fragment 7.17) for each node of tree T.
Thus, the overall space requirement is O(n).
Page 319
7.3. Binary Trees295
Operation Time
left, right, parent, isExternal, isRoot O(1)
size, empty O(1)
root O(1)
expandExternal, removeAboveExternal O(1)
positions O(n)
Table 7.2: Running times for the functions of an n-node binary tree implemented
with a linked structure. The space usage is O(n).
7.3.5 A Vector-Based Structure for Binary Trees
A simple structure for representing a binary tree T is based on a way of numbering
the nodes of T. For every node v of T, let f(v) be the integer defined as follows:
• If v is the root of T, then f(v) = 1
• If v is the left child of node u, then f(v) = 2 f(u)
• If v is the right child of node u, then f(v) = 2 f(u)+ 1
The numbering function f is known as a level numbering of the nodes in a binary
tree T, because it numbers the nodes on each level of T in increasing order from
left to right, although it may skip some numbers. (See Figure 7.16.)
The level numbering function f suggests a representation of a binary tree T
by means of a vector S, such that node v of T is associated with the element of
S at rank f(v). (See Figure 7.17.) Typically, we realize the vector S by means
of an extendable array. (See Section 6.1.3.) Such an implementation is simple
and efficient, for we can use it to easily perform the functions root, parent, left,
right, sibling, isExternal, and isRoot by using simple arithmetic operations on the
numbers f(v) associated with each node v involved in the operation. That is, each
position object v is simply a “wrapper” for the index f(v) into the vector S. We
leave the details of such implementations as a simple exercise (R-7.26).
Let n be the number of nodes of T, and let fM be the maximum value of f(v)
over all the nodes of T. The vector S has size N = fM + 1, since the element of S at
index 0 is not associated with any node of T. Also, S will have, in general, a number
of empty elements that do not refer to existing nodes of T. For a tree of height h,
N = O(2h). In the worst case, this can be as high as 2n − 1. The justification is
left as an exercise (R-7.24). In Section 8.3, we discuss a class of binary trees called
“heaps,” for which N = n+1. Thus, in spite of the worst-case space usage, there are
applications for which the array-list representation of a binary tree is space efficient.
Still, for general binary trees, the exponential worst-case space requirement of this
representation is prohibitive.
Page 320
296Chapter 7. Trees
Table 7.3 summarizes the running times of the functions of a binary tree imple
mented with a vector. We do not include any tree update functions here. The vector
implementation of a binary tree is a fast and easy way of realizing the binary-tree
ADT, but it can be very space inefficient if the height of the tree is large.
(a)
(b)
Figure 7.16: Binary tree level numbering: (a) general scheme; (b) an example.
Figure 7.17: Representation of a binary tree T by means of a vector S.
Page 321
7.3. Binary Trees297
Operation Time
left, right, parent, isExternal, isRoot O(1)
size, empty O(1)
root O(1)
expandExternal, removeAboveExternal O(1)
positions O(n)
Table 7.3: Running times for a binary tree T implemented with a vector S. We
denote the number of nodes of T with n, and N denotes the size of S. The space
usage is O(N), which is O(2n) in the worst case.
7.3.6 Traversals of a Binary Tree
As with general trees, binary-tree computations often involve traversals.
Preorder Traversal of a Binary Tree
Since any binary tree can also be viewed as a general tree, the preorder traversal
for general trees (Code Fragment 7.9) can be applied to any binary tree. We can
simplify the algorithm in the case of a binary-tree traversal, however, as we show
in Code Fragment 7.24. (Also see Code Fragment 7.23.)
Algorithm binaryPreorder(T, p):
perform the “visit” action for node p
if p is an internal node then
binaryPreorder(T, p.left()){recursively traverse left subtree}
binaryPreorder(T, p.right()){recursively traverse right subtree}
Code Fragment 7.24: Algorithm binaryPreorder, which performs the preorder
traversal of the subtree of a binary tree T rooted at node p.
For example, a preorder traversal of the binary tree shown in Figure 7.14 visits
the nodes in the order 〈LAX,BWI,ATL,JFK,PVD〉. As is the case for general
trees, there are many applications of the preorder traversal for binary trees.
Postorder Traversal of a Binary Tree
Analogously, the postorder traversal for general trees (Code Fragment 7.12) can be
specialized for binary trees as shown in Code Fragment 7.25.
A postorder traversal of the binary tree shown in Figure 7.14 visits the nodes in
the order 〈ATL,JFK,BWI,PVD,LAX〉.
Page 322
298Chapter 7. Trees
Algorithm binaryPostorder(T, p):
if p is an internal node then
binaryPostorder(T, p.left()){recursively traverse left subtree}
binaryPostorder(T, p.right()){recursively traverse right subtree}
perform the “visit” action for the node p
Code Fragment 7.25: Algorithm binaryPostorder for performing the postorder
traversal of the subtree of a binary tree T rooted at node p.
Evaluating an Arithmetic Expression
The postorder traversal of a binary tree can be used to solve the expression eval
uation problem. In this problem, we are given an arithmetic-expression tree, that
is, a binary tree where each external node has a value associated with it and each
internal node has an arithmetic operation associated with it (see Example 7.9), and
we want to compute the value of the arithmetic expression represented by the tree.
Algorithm evaluateExpression, given in Code Fragment 7.26, evaluates the ex
pression associated with the subtree rooted at a node p of an arithmetic-expression
tree T by performing a postorder traversal of T starting at p. In this case, the “visit”
action consists of performing a single arithmetic operation.
Algorithm evaluateExpression(T, p):
if p is an internal node then
x ← evaluateExpression(T, p.left())
y ← evaluateExpression(T, p.right())
Let ◦ be the operator associated with p
return x ◦y
else
return the value stored at p
Code Fragment 7.26: Algorithm evaluateExpression for evaluating the expression
represented by the subtree of an arithmetic-expression tree T rooted at node p.
The expression-tree evaluation application of the postorder traversal provides
an O(n)-time algorithm for evaluating an arithmetic expression represented by a
binary tree with n nodes. Indeed, like the general postorder traversal, the postorder
traversal for binary trees can be applied to other “bottom-up” evaluation problems
(such as the size computation given in Example 7.7) as well. The specialization
of the postorder traversal for binary trees simplifies that for general trees, however,
because we use the left and right functions to avoid a loop that iterates through the
children of an internal node.
Interestingly, the specialization of the general preorder and postorder traversal
Page 323
7.3. Binary Trees299
methods to binary trees suggests a third traversal in a binary tree that is different
from both the preorder and postorder traversals. We explore this third kind of
traversal for binary trees in the next subsection.
Inorder Traversal of a Binary Tree
An additional traversal method for a binary tree is the inorder traversal. In this
traversal, we visit a node between the recursive traversals of its left and right sub
trees. The inorder traversal of the subtree rooted at a node p in a binary tree T is
given in Code Fragment 7.27.
Algorithm inorder(T, p):
if p is an internal node then
inorder(T, p.left()){recursively traverse left subtree}
perform the “visit” action for node p
if p is an internal node then
inorder(T, p.right()){recursively traverse right subtree}
Code Fragment 7.27: Algorithm inorder for performing the inorder traversal of the
subtree of a binary tree T rooted at a node p.
For example, an inorder traversal of the binary tree shown in Figure 7.14 visits
the nodes in the order 〈ATL,BWI,JFK,LAX,PVD〉. The inorder traversal of a
binary tree T can be informally viewed as visiting the nodes of T “from left to
right.” Indeed, for every node p, the inorder traversal visits p after all the nodes
in the left subtree of p and before all the nodes in the right subtree of p. (See
Figure 7.18.)
Figure 7.18: Inorder traversal of a binary tree.
Page 324
300Chapter 7. Trees
Binary Search Trees
Let S be a set whose elements have an order relation. For example, S could be a set
of integers. A binary search tree for S is a proper binary tree T such that:
• Each internal node p of T stores an element of S, denoted with x(p)
• For each internal node p of T , the elements stored in the left subtree of p are
less than or equal to x(p) and the elements stored in the right subtree of p are
greater than or equal to x(p)
• The external nodes of T do not store any element
An inorder traversal of the internal nodes of a binary search tree T visits the
elements in nondecreasing order. (See Figure 7.19.)
We can use a binary search tree T to locate an element with a certain value x
by traversing down the tree T . At each internal node we compare the value of the
current node to our search element x. If the answer to the question is “smaller,”
then the search continues in the left subtree. If the answer is “equal,” then the
search terminates successfully. If the answer is “greater,” then the search continues
in the right subtree. Finally, if we reach an external node (which is empty), then the
search terminates unsuccessfully. (See Figure 7.19.)
Note that the time for searching in a binary search tree T is proportional to the
height of T . Recall from Proposition 7.10 that the height of a tree with n nodes
can be as small as O(logn) or as large as Ω(n). Thus, binary search trees are
most efficient when they have small height. We illustrate an example search in a
binary search tree in Figure 7.19. We study binary search trees in more detail in
Section 10.1.
Figure 7.19: A binary search tree storing integers. The blue solid path is traversed
when searching (successfully) for 36. The blue dashed path is traversed when
searching (unsuccessfully) for 70.
Page 325
7.3. Binary Trees301
Using Inorder Traversal for Tree Drawing
The inorder traversal can also be applied to the problem of computing a drawing of
a binary tree. We can draw a binary tree T with an algorithm that assigns x- and
y-coordinates to a node p of T using the following two rules (see Figure 7.20).
• x(p) is the number of nodes visited before p in the inorder traversal of T.
• y(p) is the depth of p in T.
In this application, we take the convention common in computer graphics that x
coordinates increase left to right and y-coordinates increase top to bottom. So the
origin is in the upper left corner of the computer screen.
Figure 7.20: The inorder drawing algorithm for a binary tree.
The Euler Tour Traversal of a Binary Tree
The tree-traversal algorithms we have discussed so far are all forms of iterators.
Each traversal visits the nodes of a tree in a certain order, and is guaranteed to visit
each node exactly once. We can unify the tree-traversal algorithms given above into
a single framework, however, by relaxing the requirement that each node be visited
exactly once. The resulting traversal method is called the Euler tour traversal,
which we study next. The advantage of this traversal is that it allows for more
general kinds of algorithms to be expressed easily.
The Euler tour traversal of a binary tree T can be informally defined as a “walk”
around T, where we start by going from the root toward its left child, viewing the
edges of T as being “walls” that we always keep to our left. (See Figure 7.21.)
Each node p of T is encountered three times by the Euler tour:
• “On the left” (before the Euler tour of p’s left subtree)
• “From below” (between the Euler tours of p’s two subtrees)
• “On the right” (after the Euler tour of p’s right subtree)
If p is external, then these three “visits” actually all happen at the same time.
Page 326
302Chapter 7. Trees
Figure 7.21: Euler tour traversal of a binary tree.
We give pseudo-code for the Euler tour of the subtree rooted at a node p in
Code Fragment 7.28.
Algorithm eulerTour(T, p):
perform the action for visiting node p on the left
if p is an internal node then
recursively tour the left subtree of p by calling eulerTour(T, p.left())
perform the action for visiting node p from below
if p is an internal node then
recursively tour the right subtree of p by calling eulerTour(T, p.right())
perform the action for visiting node p on the right
Code Fragment 7.28: Algorithm eulerTour for computing the Euler tour traversal
of the subtree of a binary tree T rooted at a node p.
The preorder traversal of a binary tree is equivalent to an Euler tour traversal
in which each node has an associated “visit” action occur only when it is encoun
tered on the left. Likewise, the inorder and postorder traversals of a binary tree are
equivalent to an Euler tour, where each node has an associated “visit” action occur
only when it is encountered from below or on the right, respectively.
The Euler tour traversal extends the preorder, inorder, and postorder traversals,
but it can also perform other kinds of traversals. For example, suppose we wish
to compute the number of descendants of each node p in an n node binary tree T .
We start an Euler tour by initializing a counter to 0, and then increment the counter
each time we visit a node on the left. To determine the number of descendants of
a node p, we compute the difference between the values of the counter when p is
visited on the left and when it is visited on the right, and add 1. This simple rule
gives us the number of descendants of p, because each node in the subtree rooted
Page 327
7.3. Binary Trees303
at p is counted between p’s visit on the left and p’s visit on the right. Therefore, we
have an O(n)-time method for computing the number of descendants of each node
in T .
The running time of the Euler tour traversal is easy to analyze, assuming that
visiting a node takes O(1) time. Namely, in each traversal, we spend a constant
amount of time at each node of the tree during the traversal, so the overall running
time is O(n) for an n node tree.
Another application of the Euler tour traversal is to print a fully parenthesized
arithmetic expression from its expression tree (Example 7.9). Algorithm printEx
pression, shown in Code Fragment 7.29, accomplishes this task by performing the
following actions in an Euler tour:
• “On the left” action: if the node is internal, print “(”
• “From below” action: print the value or operator stored at the node
• “On the right” action: if the node is internal, print “)”
Algorithm printExpression(T, p):
if p.isExternal() then
print the value stored at p
else
print “(”
printExpression(T, p.left())
print the operator stored at p
printExpression(T, p.right())
print “)”
Code Fragment 7.29: An algorithm for printing the arithmetic expression associated
with the subtree of an arithmetic-expression tree T rooted at p.
7.3.7 The Template Function Pattern
The tree traversal functions described above are actually examples of an interesting
object-oriented software design pattern, the template function pattern. This is not
to be confused with templated classes or functions in C++, but the principal is
similar. The template function pattern describes a generic computation mechanism
that can be specialized for a particular application by redefining certain steps.
Euler Tour with the Template Function Pattern
Following the template function pattern, we can design an algorithm, template
EulerTour, that implements a generic Euler tour traversal of a binary tree. When
Page 328
304Chapter 7. Trees
called on a node p, function templateEulerTour calls several other auxiliary func
tions at different phases of the traversal. First of all, it creates a three-element
structure r to store the result of the computation calling auxiliary function initRe
sult. Next, if p is an external node, templateEulerTour calls auxiliary function
visitExternal, else (p is an internal node) templateEulerTour executes the follow
ing steps:
• Calls auxiliary function visitLeft, which performs the computations associ
ated with encountering the node on the left
• Recursively calls itself on the left child
• Calls auxiliary function visitBelow, which performs the computations asso
ciated with encountering the node from below
• Recursively calls itself on the right subtree
• Calls auxiliary function visitRight, which performs the computations associ
ated with encountering the node on the right
Finally, templateEulerTour returns the result of the computation by calling aux
iliary function returnResult. Function templateEulerTour can be viewed as a tem
plate or “skeleton” of an Euler tour. (See Code Fragment 7.30.)
Algorithm templateEulerTour(T, p):
r ← initResult()
if p.isExternal() then
r.finalResult ← visitExternal(T, p,r)
else
visitLeft(T, p,r)
r.leftResult ← templateEulerTour(T, p.left())
visitBelow(T, p,r)
r.rightResult ← templateEulerTour(T, p.right())
visitRight(T, p,r)
return returnResult(r)
Code Fragment 7.30: Function templateEulerTour for computing a generic Euler
tour traversal of the subtree of a binary tree T rooted at a node p, following the
template function pattern. This function calls the functions initResult, visitExter
nal, visitLeft, visitBelow, visitRight, and returnResult.
In an object-oriented context, we can then write a class EulerTour that:
• Contains function templateEulerTour
• Contains all the auxiliary functions called by templateEulerTour as empty
place holders (that is, with no instructions or returning NULL)
• Contains a function execute that calls templateEulerTour(T,T.root())
Page 329
7.3. Binary Trees305
Class EulerTour itself does not perform any useful computation. However, we
can extend it with the inheritance mechanism and override the empty functions to
do useful tasks.
Template Function Examples
As a first example, we can evaluate the expression associated with an arithmetic
expression tree (see Example 7.9) by writing a new class EvaluateExpression that:
• Extends class EulerTour
• Overrides function initResult by returning an array of three numbers
• Overrides function visitExternal by returning the value stored at the node
• Overrides function visitRight by combining r.leftResult and r.rightResult with
the operator stored at the node, and setting r.finalResult equal to the result of
the operation
• Overrides function returnResult by returning r.finalResult
This approach should be compared with the direct implementation of the algo
rithm shown in Code Fragment 7.26.
As a second example, we can print the expression associated with an arithmetic
expression tree (see Example 7.9) using a new class PrintExpression that:
• Extends class EulerTour
• Overrides function visitExternal by printing the value of the variable or con
stant associated with the node
• Overrides function visitLeft by printing “(”
• Overrides function visitBelow by printing the operator associated with the
node
• Overrides function visitRight by printing “)”
This approach should be compared with the direct implementation of the algo
rithm shown in Code Fragment 7.29.
C++ Implementation
A complete C++ implementation of the generic EulerTour class and of its spe
cializations EvaluateExpressionTour and PrintExpressionTour are shown in Code
Fragments 7.31 through 7.34. These are based on a linked binary tree implementa
tion.
We begin by defining a local structure Result with fields leftResult, rightResult,
and finalResult, which store the intermediate results of the tour. In order to avoid
typing lengthy qualified type names, we give two type definitions, BinaryTree and
Position, for the tree and a position in the tree, respectively. The only data member
is a pointer to the binary tree. We provide a simple function, called initialize, that
sets this pointer to an existing binary tree. The remaining functions are protected,
Page 330
306Chapter 7. Trees
since they are not invoked directly, but rather by the derived classes, which produce
the desired specialized behavior.
template <typename E, typename R> // element and result types
class EulerTour {// a template for Euler tour
protected:
struct Result {// stores tour results
R leftResult;// result from left subtree
R rightResult;// result from right subtree
R finalResult;// combined result
};
typedef BinaryTree<E> BinaryTree;// the tree
typedef typename BinaryTree::Position Position; // a position in the tree
protected:// data member
const BinaryTree* tree;// pointer to the tree
public:
void initialize(const BinaryTree& T)// initialize
{ tree = &T; }
protected:// local utilities
int eulerTour(const Position& p) const; // perform the Euler tour
// functions given by subclasses
virtual void visitExternal(const Position& p, Result& r) const {}
virtual void visitLeft(const Position& p, Result& r) const {}
virtual void visitBelow(const Position& p, Result& r) const {}
virtual void visitRight(const Position& p, Result& r) const {}
Result initResult() const { return Result(); }
int result(const Result& r) const { return r.finalResult; }
};
Code Fragment 7.31: Class EulerTour defining a generic Euler tour of a binary tree.
This class realizes the template function pattern and must be specialized in order to
generate an interesting computation.
Next, in Code Fragment 7.32, we present the principal traversal function, called
eulerTour. This recursive function performs an Euler traversal on the tree and in
vokes the appropriate functions as it goes. If run on the generic Euler tree, noth
ing interesting would result, because these functions (as defined in Code Frag
ment 7.31) do nothing. It is up to the derived functions to provide more interesting
definitions for these generic functions.
In Code Fragment 7.33, we present our first example of a derived class us
ing the template pattern, called EvaluateExpressionTour. It evaluates an integer
arithmetic-expression tree. We assume that each external node of an expression
tree provides a function called value, which returns the value associated with this
node. We assume that each internal node of an expression tree provides a function
called operation, which performs the operation associated with this node to the two
operands arising from its left and right subtrees, and returns the result.
Page 331
7.3. Binary Trees307
template <typename E, typename R> // do the tour
int EulerTour<E, R>::eulerTour(const Position& p) const {
Result r = initResult();
if (p.isExternal()) {// external node
visitExternal(p, r);
} else {// internal node
visitLeft(p, r);
r.leftResult = eulerTour(p.left());// recurse on left
visitBelow(p, r);
r.rightResult = eulerTour(p.right());// recurse on right
visitRight(p, r);
}
return result(r);
}
Code Fragment 7.32: The principal member function eulerTour, which recursively
traverses the tree and accumulates the results.
Using these two functions, we can evaluate the expression recursively as we
traverse the tree. The main entry point is the function execute, which initializes the
tree, invokes the recursive Euler tour starting at the root, and prints the final result.
For example, given the expression tree of Figure 7.21, this procedure would output
the string “The value is: -13”.
template <typename E, typename R>
class EvaluateExpressionTour : public EulerTour<E, R> {
protected:// shortcut type names
typedef typename EulerTour<E, R>::BinaryTree BinaryTree;
typedef typename EulerTour<E, R>::Position Position;
typedef typename EulerTour<E, R>::Result Result;
public:
void execute(const BinaryTree& T) {// execute the tour
initialize(T);
std::cout << "The value is: " << eulerTour(T.root()) << "\n";
}
protected:// leaf: return value
virtual void visitExternal(const Position& p, Result& r) const
{ r.finalResult = (*p).value(); }
// internal: do operation
virtual void visitRight(const Position& p, Result& r) const
{ r.finalResult = (*p).operation(r.leftResult, r.rightResult); }
};
Code Fragment 7.33: Implementation of class EvaluateExpressionTour which
specializes EulerTour to evaluate the expression associated with an arithmetic
expression tree.
Page 332
308Chapter 7. Trees
Finally, in Code Fragment 7.34, we present a second example of a derived class,
called PrintExpressionTour. In contrast to the previous function, which evaluates
the value of an expression tree, this one prints the expression. We assume that each
node of an expression tree provides a function called print. For each external node,
this function prints the value associated with this node. For each internal node,
this function prints the operator, for example, printing “+” for addition or “*” for
multiplication.
template <typename E, typename R>
class PrintExpressionTour : public EulerTour<E, R> {
protected: // . . .same type name shortcuts as in EvaluateExpressionTour
public:
void execute(const BinaryTree& T) {// execute the tour
initialize(T);
cout << "Expression: "; eulerTour(T.root()); cout << endl;
}
protected:// leaf: print value
virtual void visitExternal(const Position& p, Result& r) const
{ (*p).print(); }
// left: open new expression
virtual void visitLeft(const Position& p, Result& r) const
{ cout << "("; }
// below: print operator
virtual void visitBelow(const Position& p, Result& r) const
{ (*p).print(); }
// right: close expression
virtual void visitRight(const Position& p, Result& r) const
{ cout << ")"; }
};
Code Fragment 7.34: A class that prints an arithmetic-expression tree.
When entering a subtree, the function visitLeft has been overridden to print “(”
and on exiting a subtree, the function visitRight has been overridden to print “).”
The main entry point is the function execute, which initializes the tree, and invokes
the recursive Euler tour starting at the root. When combined, these functions print
the entire expression (albeit with lots of redundant parentheses). For example, given
the expression tree of Figure 7.21, this procedure would output the following string.
((((3 + 1) * 3) / ((9 - 5) + 2)) - ((3 * (7 - 4)) + 6))
Page 333
7.3. Binary Trees309
7.3.8 Representing General Trees with Binary Trees
An alternative representation of a general tree T is obtained by transforming T into
a binary tree T′. (See Figure 7.22.) We assume that either T is ordered or that it
has been arbitrarily ordered. The transformation is as follows:
• For each node u of T, there is an internal node u′ of T′ associated with u
• If u is an external node of T and does not have a sibling immediately follow
ing it, then the children of u′ in T′ are external nodes
• If u is an internal node of T and v is the first child of u in T, then v′ is the left
child of u′ in T
• If node v has a sibling w immediately following it, then w′ is the right child
of v′ in T′
Note that the external nodes of T′ are not associated with nodes of T, and serve
only as place holders (hence, may even be null).
(a)(b)
Figure 7.22: Representation of a tree by means of a binary tree: (a) tree T; (b)
binary tree T′ associated with T. The dashed edges connect nodes of T′ associated
with sibling nodes of T.
It is easy to maintain the correspondence between T and T′, and to express
operations in T in terms of corresponding operations in T′. Intuitively, we can
think of the correspondence in terms of a conversion of T into T′ that takes each
set of siblings {v1,v2,...,vk} in T with parent v and replaces it with a chain of right
children rooted at v1, which then becomes the left child of v.
Page 334
310Chapter 7. Trees
7.4 Exercises
For help with exercises, please visit the web site, www.wiley.com/college/goodrich.
Reinforcement
R-7.1 Describe an algorithm for counting the number of left external nodes in a
binary tree, using the Binary tree ADT.
R-7.2 The following questions refer to the tree of Figure 7.3.
a. Which node is the root?
b. What are the internal nodes?
c. How many descendents does node cs016/ have?
d. How many ancestors does node cs016/ have?
e. What are the siblings of node homeworks/?
f. Which nodes are in the subtree rooted at node projects/?
g. What is the depth of node papers/?
h. What is the height of the tree?
R-7.3 Find the value of the arithmetic expression associated with each subtree
of the binary tree of Figure 7.11.
R-7.4 Let T be an n-node improper binary tree (that is, each internal node has
one or two children). Describe how to represent T by means of a proper
binary tree T′ with O(n) nodes.
R-7.5 What are the minimum and maximum number of internal and external
nodes in an improper binary tree with n nodes?
R-7.6 Show a tree achieving the worst-case running time for algorithm depth.
R-7.7 Give a justification of Proposition 7.4.
R-7.8 What is the running time of algorithm height2(T,v) (Code Fragment 7.7)
when called on a node v distinct from the root of T?
R-7.9 Let T be the tree of Figure 7.3.
a. Give the output of preorderPrint(T,T.root()) (Code Fragment 7.10).
b. Give the output of parenPrint(T,T.root())(Code Fragment cod:paren:Print).
R-7.10 Describe a modification to the parenPrint function given in Code Frag
ment 7.11, so that it uses the size function for string objects to output the
parenthetic representation of a tree with line breaks and spaces added to
display the tree in a text window that is 80 characters wide.
Page 335
7.4. Exercises311
R-7.11 Draw an arithmetic-expression tree that has four external nodes, storing
the numbers 1, 5, 6, and 7 (with each number stored in a distinct external
node, but not necessarily in this order), and has three internal nodes, each
storing an operator from the set {+,−,×,/}, so that the value of the root
is 21. The operators may return and act on fractions, and an operator may
be used more than once.
R-7.12 Let T be an ordered tree with more than one node. Is it possible that the
preorder traversal of T visits the nodes in the same order as the postorder
traversal of T? If so, give an example; otherwise, argue why this cannot
occur. Likewise, is it possible that the preorder traversal of T visits the
nodes in the reverse order of the postorder traversal of T? If so, give an
example; otherwise, argue why this cannot occur.
R-7.13 Answer the previous question for the case when T is a proper binary tree
with more than one node.
R-7.14 Let T be a tree with n nodes. What is the running time of the function
parenPrint(T,T.root())? (See Code Fragment 7.11.)
R-7.15 Draw a (single) binary tree T, such that:
• Each internal node of T stores a single character
• A preorder traversal of T yields EXAMFUN
• An inorder traversal of T yields MAFXUEN
R-7.16 Answer the following questions so as to justify Proposition 7.10.
a. What is the minimum number of external nodes for a binary tree
with height h? Justify your answer.
b. What is the maximum number of external nodes for a binary tree
with height h? Justify your answer.
c. Let T be a binary tree with height h and n nodes. Show that
log(n+1)− 1 ≤ h ≤ (n− 1)/2.
d. For which values of n and h can the above lower and upper bounds
on h be attained with equality?
R-7.17 Describe a generalization of the Euler tour traversal of trees such that each
internal node has three children. Describe how you could use this traversal
to compute the height of each node in such a tree.
R-7.18 Modify the C++ function preorderPrint, given in Code Fragment 7.10, so
that it will print the strings associated with the nodes of a tree one per line,
and indented proportionally to the depth of the node.
R-7.19 Let T be the tree of Figure 7.3. Draw, as best as you can, the output of the
algorithm postorderPrint(T,T.root()) (Code Fragment 7.13).
Page 336
312Chapter 7. Trees
R-7.20 Let T be the tree of Figure 7.9. Compute, in terms of the values given
in this figure, the output of algorithm diskSpace(T,T.root()). (See Code
Fragment 7.14.)
R-7.21 Let T be the binary tree of Figure 7.11.
a. Give the output of preorderPrint(T,T.root()) (Code Fragment 7.10).
b. Give the output of the function printExpression(T,T.root()) (Code
Fragment 7.29).
R-7.22 Describe, in pseudo-code, an algorithm for computing the number of de
scendents of each node of a binary tree. The algorithm should be based
on the Euler tour traversal.
R-7.23 Let T be a (possibly improper) binary tree with n nodes, and let D be the
sum of the depths of all the external nodes of T. Show that if T has the
minimum number of external nodes possible, then D is O(n) and if T has
the maximum number of external nodes possible, then D is O(nlogn).
R-7.24 Let T be a binary tree with n nodes, and let f be the level numbering of
the nodes of T as given in Section 7.3.5.
a. Show that, for every node v of T, f (v) ≤ 2n −1.
b. Show an example of a binary tree with seven nodes that attains the
above upper bound on f (v) for some node v.
R-7.25 Draw the binary tree representation of the following arithmetic expres
sion: “(((5 + 2)∗(2−1))/((2 + 9)+ ((7−2)−1))∗8) .”
R-7.26 Let T be a binary tree with n nodes that is realized with a vector, S, and let
f be the level numbering of the nodes in T as given in Section 7.3.5. Give
pseudo-code descriptions of each of the functions root, parent, leftChild,
rightChild, isExternal, and isRoot.
R-7.27 Show how to use the Euler tour traversal to compute the level number,
defined in Section 7.3.5, of each node in a binary tree T.
Creativity
C-7.1 Show that there are more than 2n different potentially improper binary
trees with n internal nodes, where two trees are considered different if
they can be drawn as different looking trees.
C-7.2 Describe an efficient algorithm for converting a fully balanced string of
parentheses into an equivalent tree. The tree associated with such a string
is defined recursively. The outer-most pair of balanced parentheses is as
sociated with the root and each substring inside this pair, defined by the
substring between two balanced parentheses, is associated with a subtree
of this root.
Page 337
7.4. Exercises313
C-7.3 For each node v in a tree T , let pre(v) be the rank of v in a preorder
traversal of T , let post(v) be the rank of v in a postorder traversal of T , let
depth(v) be the depth of v, and let desc(v) be the number of descendents
of v, not counting v itself. Derive a formula defining post(v) in terms of
desc(v), depth(v), and pre(v), for each node v in T .
C-7.4 Let T be a tree whose nodes store strings. Give an algorithm that computes
and prints, for every internal node v of T , the string stored at v and the
height of the subtree rooted at v.
C-7.5 Design algorithms for the following operations for a binary tree T .
• preorderNext(v): return the node visited after node v in a preorder
traversal of T .
• inorderNext(v): return the node visited after node v in an inorder
traversal of T .
• postorderNext(v): return the node visited after node v in a postorder
traversal of T .
What are the worst-case running times of your algorithms?
C-7.6 Give an O(n)-time algorithm for computing the depth of all the nodes of
a tree T , where n is the number of nodes of T .
C-7.7 The indented parenthetic representation of a tree T is a variation of the
parenthetic representation of T (see Figure 7.7) that uses indentation and
line breaks as illustrated in Figure 7.23. Give an algorithm that prints this
representation of a tree.
Sales (
Domestic
International (
Canada
S. America
Overseas (
Africa
Europe
Asia
Australia
)
)
)
(a)(b)
Figure 7.23: (a) Tree T ; (b) indented parenthetic representation of T .
C-7.8 Let T be a (possibly improper) binary tree with n nodes, and let D be the
sum of the depths of all the external nodes of T . Describe a configuration
for T such that D is Ω(n2). Such a tree would be the worst case for the
asymptotic running time of Algorithm height1 (Code Fragment 7.6).
Page 338
314Chapter 7. Trees
C-7.9 For a tree T, let nI denote the number of its internal nodes, and let nE
denote the number of its external nodes. Show that if every internal node
in T has exactly 3 children, then nE = 2nI +1.
C-7.10 The update operations expandExternal and removeAboveExternal do not
permit the creation of an improper binary tree. Give pseudo-code descrip
tions for alternate update operations suitable for improper binary trees.
You may need to define new query operations as well.
C-7.11 The balance factor of an internal node v of a binary tree is the difference
between the heights of the right and left subtrees of v. Show how to spe
cialize the Euler tour traversal of Section 7.3.7 to print the balance factors
of all the nodes of a binary tree.
C-7.12 Two ordered trees T′ and T′′ are said to be isomorphic if one of the fol
lowing holds:
• Both T′ and T′′ consist of a single node
• Both T′ and T′′ have the same number k of subtrees, and the ith
subtree of T′ is isomorphic to the ith subtree of T′′, for i = 1,...,k.
Design an algorithm that tests whether two given ordered trees are iso
morphic. What is the running time of your algorithm?
C-7.13 Extend the concept of an Euler tour to an ordered tree that is not necessar
ily a binary tree.
C-7.14 As mentioned in Exercise C-5.8, postfix notation is an unambiguous way
of writing an arithmetic expression without parentheses. It is defined so
that if “(exp1)◦(exp2)” is a normal (infix) fully parenthesized expression
with operation “◦,” then its postfix equivalent is “pexp1 pexp2◦,” where
pexp1 is the postfix version of exp1 and pexp2 is the postfix version of
exp2. The postfix version of a single number of variables is just that num
ber or variable. So, for example, the postfix version of the infix expression
“((5+2)∗(8−3))/4” is “5 2 + 8 3 − ∗ 4 /.” Give an efficient algorithm,
that when given an expression tree, outputs the expression in postfix nota
tion.
C-7.15 Given a proper binary tree T, define the reflection of T to be the binary
tree T′ such that each node v in T is also in T′, but the left child of v in T
is v’s right child in T′ and the right child of v in T is v’s left child in T′.
Show that a preorder traversal of a proper binary tree T is the same as the
postorder traversal of T’s reflection, but in reverse order.
C-7.16 Algorithm preorderDraw draws a binary tree T by assigning x- and y
coordinates to each node v such that x(v) is the number of nodes preceding
v in the preorder traversal of T and y(v) is the depth of v in T. Algorithm
postorderDraw is similar to preorderDraw but assigns x-coordinates using
a postorder traversal.
Page 339
7.4. Exercises315
a. Show that the drawing of T produced by preorderDraw has no pairs
of crossing edges.
b. Redraw the binary tree of Figure 7.20 using preorderDraw.
c. Show that the drawing of T produced by postorderDraw has no pairs
of crossing edges.
d. Redraw the binary tree of Figure 7.20 using postorderDraw.
C-7.17 Let a visit action in the Euler tour traversal be denoted by a pair (v,a),
where v is the visited node and a is one of left, below, or right. Design an
algorithm for performing operation tourNext(v,a), which returns the visit
action (w,b) following (v,a). What is the worst-case running time of your
algorithm?
C-7.18 Algorithm preorderDraw draws a binary tree T by assigning x- and y
coordinates to each node v as follows:
• Set x(v) equal to the number of nodes preceding v in the preorder
traversal of T.
• Set y(v) equal to the depth of v in T.
a. Show that the drawing of T produced by algorithm preorderDraw
has no pairs of crossing edges.
b. Use algorithm preorderDraw to redraw the binary tree shown in Fig
ure 7.20.
c. Use algorithm postorderDraw, which is similar to preorderDraw but
assigns x-coordinates using a postorder traversal, to redraw the bi
nary tree of Figure 7.20.
C-7.19 Design an algorithm for drawing general trees that generalizes the inorder
traversal approach for drawing binary trees.
C-7.20 Consider a variation of the linked data structure for binary trees where
each node object has pointers to the node objects of the children but not to
the node object of the parent. Describe an implementation of the functions
of a binary tree with this data structure and analyze the time complexity
for these functions.
C-7.21 Design an alternative implementation of the linked data structure for bi
nary trees using a class for nodes that specializes into subclasses for an
internal node, an external node, and the root node.
C-7.22 Provide the missing housekeeping functions (destructor, copy constructor,
and assignment operator) for the class LinkedBinaryTree given in Code
Fragment 7.19.
C-7.23 Our linked binary tree implementation given in Code Fragment 7.19 as
sumes that the tree is proper. Design an alternative implementation of the
linked data structure for a general (possibly improper) binary tree.
Page 340
316Chapter 7. Trees
C-7.24 Let T be a tree with n nodes. Define the lowest common ancestor (LCA)
between two nodes v and w as the lowest node in T that has both v and
w as descendents (where we allow a node to be a descendent of itself).
Given two nodes v and w, describe an efficient algorithm for finding the
LCA of v and w. What is the running time of your method?
C-7.25 Let T be a tree with n nodes, and, for any node v in T, let dv denote
the depth of v in T. The distance between two nodes v and w in T is
dv+dw −2du, where u is the LCA u of v and w (as defined in the previous
exercise). The diameter of T is the maximum distance between two nodes
in T. Describe an efficient algorithm for finding the diameter of T. What
is the running time of your method?
C-7.26 Suppose each node v of a binary tree T is labeled with its value f(v) in
a level numbering of T. Design a fast method for determining f(u) for
the lowest common ancestor (LCA), u, of two nodes v and w in T, given
f(v) and f(w). You do not need to find node u, just compute its level
numbering label.
C-7.27 Justify Table 7.1, summarizing the running time of the functions of a tree
represented with a linked structure, by providing, for each function, a de
scription of its implementation, and an analysis of its running time.
C-7.28 Describe efficient implementations of the expandExternal and remove
AboveExternal binary tree update functions, described in Section 7.3.4,
for the case when the binary tree is implemented using a vector S, where
S is realized using an expandable array. Your functions should work even
for null external nodes, assuming we represent such a node as a wrapper
object storing an index to an empty or nonexistent cell in S. What are the
worst-case running times of these functions? What is the running time
of removeAboveExternal if the internal node removed has only external
node children?
C-7.29 Describe a nonrecursive method for evaluating a binary tree representing
an arithmetic expression.
C-7.30 Let T be a binary tree with n nodes. Define a Roman node to be a node
v in T, such that the number of descendents in v’s left subtree differ from
the number of descendants in v’s right subtree by at most 5. Describe
a linear-time method for finding each node v of T, such that v is not a
Roman node, but all of v descendants are Roman nodes.
C-7.31 Let T′ be the binary tree representing a tree T. (See Section 7.3.8.)
a. Is a preorder traversal of T′ equivalent to a preorder traversal of T?
b. Is a postorder traversal of T′ equivalent to a postorder traversal of T?
c. Is an inorder traversal of T′ equivalent to some well-structured traver
sal of T?
Page 341
7.4. Exercises317
C-7.32 Describe a nonrecursive method for performing an Euler tour traversal of
a binary tree that runs in linear time and does not use a stack.
(Hint: You can tell which visit action to perform at a node by taking note
of where you are coming from.)
C-7.33 Describe, in pseudo-code, a nonrecursive method for performing an in
order traversal of a binary tree in linear time.
(Hint: Use a stack.)
C-7.34 Let T be a binary tree with n nodes (T may or may not be realized with a
vector). Give a linear-time method that uses the functions of the Binary
Tree interface to traverse the nodes of T by increasing values of the level
numbering function f given in Section 7.3.5. This traversal is known as
the level order traversal.
(Hint: Use a queue.)
C-7.35 The path length of a tree T is the sum of the depths of all the nodes in T.
Describe a linear-time method for computing the path length of a tree T
(which is not necessarily binary).
C-7.36 Define the internal path length, I(T), of a tree T, to be the sum of the
depths of all the internal nodes in T. Likewise, define the external path
length, E(T), of a tree T, to be the sum of the depths of all the external
nodes in T. Show that if T is a binary tree with n internal nodes, then
E(T) = I(T)+ 2n.
(Hint: Use the fact that we can build T from a single root node via a series
of n expandExternal operations.)
Projects
P-7.1 Write a program that takes as input a rooted tree T and a node v of T and
converts T to another tree with the same set of node adjacencies but now
rooted at v.
P-7.2 Give a fully generic implementation of the class LinkedBinaryTree using
class templates and taking into account error conditions.
P-7.3 Implement the binary tree ADT using a vector.
P-7.4 Implement the binary tree ADT using a linked structure.
P-7.5 Write a program that draws a binary tree.
P-7.6 Write a program that draws a general tree.
P-7.7 Write a program that can input and display a person’s family tree.
P-7.8 Implement the binary tree representation of the tree ADT. You may reuse
the LinkedBinaryTree implementation of a binary tree.
Page 342
318Chapter 7. Trees
P-7.9 A slicing floorplan is a decomposition of a rectangle with horizontal and
vertical sides using horizontal and vertical cuts (see Figure 7.24(a)). A
slicing floorplan can be represented by a binary tree, called a slicing tree,
whose internal nodes represent the cuts, and whose external nodes repre
sent the basic rectangles into which the floorplan is decomposed by the
cuts (see Figure 7.24(b)). The compaction problem is defined as follows.
Assume that each basic rectangle of a slicing floorplan is assigned a min
imum width w and a minimum height h. The compaction problem is to
find the smallest possible height and width for each rectangle of the slic
ing floorplan that is compatible with the minimum dimensions of the basic
rectangles. Namely, this problem requires the assignment of values h(v)
and w(v) to each node v of the slicing tree, such that
w(v) =wif v is an external node whose basic rect
angle has minimum width w
max(w(w),w(z))if v is an internal node associated with a
horizontal cut with left child w and right
child z
w(w) + w(z)if v is an internal node associated with
a vertical cut with left child w and right
child z
h(v) =hif v is an external node whose basic rectle has minimum height hangle has minimum height h
h(w) + h(z)if v is an internal node associated with a
horizontal cut with left child w and right
child z
max(h(w),h(z))if v is an internal node associated with
a vertical cut with left child w and right
child z
Design a data structure for slicing floorplans that supports the following
operations:
• Create a floorplan consisting of a single basic rectangle
• Decompose a basic rectangle by means of a horizontal cut
• Decompose a basic rectangle by means of a vertical cut
• Assign minimum height and width to a basic rectangle
• Draw the slicing tree associated with the floorplan
• Compact the floorplan
• Draw the compacted floorplan
Page 343
7.4. Exercises319
(a)(b)
Figure 7.24: (a) Slicing floorplan; (b) slicing tree associated with the floorplan.
P-7.10 Write a program that takes, as input, a fully parenthesized, arithmetic ex
pression and converts it to a binary expression tree. Your program should
display the tree in some way and also print the value associated with the
root. For an additional challenge, allow for the leaves to store variables
of the form x1, x2, x3, and so on, which are initially 0 and which can be
updated interactively by your program, with the corresponding update in
the printed value of the root of the expression tree.
P-7.11 Write a program that can play Tic-Tac-Toe effectively. (See Section 3.1.3.)
To do this, you will need to create a game tree T , which is a tree where
each node corresponds to a game configuration, which, in this case, is
a representation of the tic-tac-toe board. The root node corresponds to
the initial configuration. For each internal node v in T , the children of v
correspond to the game states we can reach from v’s game state in a single
legal move for the appropriate player, A (the first player) or B (the second
player). Nodes at even depths correspond to moves for A and nodes at
odd depths correspond to moves for B. External nodes are either final
game states or are at a depth beyond which we don’t want to explore. We
score each external node with a value that indicates how good this state
is for player A. In large games, like chess, we have to use a heuristic
scoring function, but for small games, like tic-tac-toe, we can construct
the entire game tree and score external nodes as +1, 0, −1, indicating
whether player A has a win, draw, or lose in that configuration. A good
algorithm for choosing moves is minimax. In this algorithm, we assign a
score to each internal node v in T , such that if v represents A’s turn, we
compute v’s score as the maximum of the scores of v’s children (which
corresponds to A’s optimal play from v). If an internal node v represents
B’s turn, then we compute v’s score as the minimum of the scores of v’s
children (which corresponds to B’s optimal play from v).
Page 344
320Chapter 7. Trees
Chapter Notes
Our use of the position abstraction derives from the position and node abstractions intro
duced by Aho, Hopcroft, and Ullman [5]. Discussions of the classic preorder, inorder,
and postorder tree traversal methods can be found in Knuth’s Fundamental Algorithms
book [56]. The Euler tour traversal technique comes from the parallel algorithms commu
nity, as it is introduced by Tarjan and Vishkin [93] and is discussed by J´aJ´a [49] and by
Karp and Ramachandran [53]. The algorithm for drawing a tree is generally considered
to be a part of the “folklore” of graph drawing algorithms. The reader interested in graph
drawing is referred to the book by Di Battista, Eades, Tamassia and Tollis [28] and the
survey by Tamassia and Liotta [92]. The puzzler in Exercise R-7.11 was communicated by
Micha Sharir.
Page 345
Chapter
8 Heaps and Priority Queues
Contents
8.1 The Priority Queue Abstract Data Type . . . . . . . . 322
8.1.1 Keys, Priorities, and Total Order Relations . . . . . . 322
8.1.2 Comparators . . . . . . . . . . . . . . . . . . . . . . 324
8.1.3 The Priority Queue ADT . . . . . . . . . . . . . . . 327
8.1.4 A C++ Priority Queue Interface . . . . . . . . . . . . 328
8.1.5 Sorting with a Priority Queue . . . . . . . . . . . . . 329
8.1.6 The STL priority queue Class . . . . . . . . . . . . . 330
8.2 Implementing a Priority Queue with a List . . . . . . 331
8.2.1 A C++ Priority Queue Implementation using a List . 333
8.2.2 Selection-Sort and Insertion-Sort . . . . . . . . . . . 335
8.3 Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
8.3.1 The Heap Data Structure . . . . . . . . . . . . . . . 337
8.3.2 Complete Binary Trees and Their Representation . . 340
8.3.3 Implementing a Priority Queue with a Heap . . . . . 344
8.3.4 C++ Implementation . . . . . . . . . . . . . . . . . 349
8.3.5 Heap-Sort . . . . . . . . . . . . . . . . . . . . . . . 351
8.3.6 Bottom-Up Heap Construction ⋆ . . . . . . . . . . 353
8.4 Adaptable Priority Queues . . . . . . . . . . . . . . . 357
8.4.1 A List-Based Implementation . . . . . . . . . . . . . 358
8.4.2 Location-Aware Entries . . . . . . . . . . . . . . . . 360
8.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 361
Page 346
322Chapter 8. Heaps and Priority Queues
8.1 The Priority Queue Abstract Data Type
A priority queue is an abstract data type for storing a collection of prioritized ele
ments that supports arbitrary element insertion but supports removal of elements in
order of priority, that is, the element with first priority can be removed at any time.
This ADT is fundamentally different from the position-based data structures such
as stacks, queues, deques, lists, and even trees, we discussed in previous chapters.
These other data structures store elements at specific positions, which are often
positions in a linear arrangement of the elements determined by the insertion and
deletion operations performed. The priority queue ADT stores elements according
to their priorities, and has no external notion of “position.”
8.1.1 Keys, Priorities, and Total Order Relations
Applications commonly require comparing and ranking objects according to pa
rameters or properties, called “keys,” that are assigned to each object in a collec
tion. Formally, we define a key to be an object that is assigned to an element as a
specific attribute for that element and that can be used to identify, rank, or weigh
that element. Note that the key is assigned to an element, typically by a user or ap
plication; hence, a key might represent a property that an element did not originally
possess.
The key an application assigns to an element is not necessarily unique, however,
and an application may even change an element’s key if it needs to. For example,
we can compare companies by earnings or by number of employees; hence, either
of these parameters can be used as a key for a company, depending on the infor
mation we wish to extract. Likewise, we can compare restaurants by a critic’s food
quality rating or by average entr´ee price. To achieve the most generality then, we
allow a key to be of any type that is appropriate for a particular application.
As in the examples above, the key used for comparisons is often more than
a single numerical value, such as price, length, weight, or speed. That is, a key
can sometimes be a more complex property that cannot be quantified with a single
number. For example, the priority of standby passengers is usually determined by
taking into account a host of different factors, including frequent-flyer status, the
fare paid, and check-in time. In some applications, the key for an object is data
extracted from the object itself (for example, it might be a member variable storing
the list price of a book, or the weight of a car). In other applications, the key is not
part of the object but is externally generated by the application (for example, the
quality rating given to a stock by a financial analyst, or the priority assigned to a
standby passenger by a gate agent).
Page 347
8.1. The Priority Queue Abstract Data Type323
Comparing Keys with Total Orders
A priority queue needs a comparison rule that never contradicts itself. In order for
a comparison rule, which we denote by ≤, to be robust in this way, it must define
a total order relation, which is to say that the comparison rule is defined for every
pair of keys and it must satisfy the following properties:
• Reflexive property : k ≤ k
• Antisymmetric property: if k1 ≤ k2 and k2 ≤ k1, then k1 = k2
• Transitive property: if k1 ≤ k2 and k2 ≤ k3, then k1 ≤ k3
Any comparison rule, ≤, that satisfies these three properties never leads to a
comparison contradiction. In fact, such a rule defines a linear ordering relationship
among a set of keys. If a finite collection of keys has a total order defined for it, then
the notion of the smallest key, kmin, is well defined as the key, such that kmin ≤ k,
for any other key k in our collection.
A priority queue is a container of elements, each associated with a key. The
name “priority queue” comes from the fact that keys determine the “priority” used
to pick elements to be removed. The fundamental functions of a priority queue P
are as follows:
insert(e): Insert the element e (with an implicit associated key value)
into P.
min(): Return an element of P with the smallest associated key
value, that is, an element whose key is less than or equal
to that of every other element in P.
removeMin(): Remove from P the element min().
Note that more than one element can have the same key, which is why we were
careful to define removeMin to remove not just any minimum element, but the
same element returned by min. Some people refer to the removeMin function as
extractMin.
There are many applications where operations insert and removeMin play an
important role. We consider such an application in the example that follows.
Example 8.1: Suppose a certain flight is fully booked an hour prior to departure.
Because of the possibility of cancellations, the airline maintains a priority queue of
standby passengers hoping to get a seat. The priority of each passenger is deter
mined by the fare paid, the frequent-flyer status, and the time when the passenger is
inserted into the priority queue. When a passenger requests to fly standby, the asso
ciated passenger object is inserted into the priority queue with an insert operation.
Shortly before the flight departure, if seats become available (for example, due to
last-minute cancellations), the airline repeatedly removes a standby passenger with
first priority from the priority queue, using a combination of min and removeMin
operations, and lets this person board.
Page 348
324Chapter 8. Heaps and Priority Queues
8.1.2 Comparators
An important issue in the priority queue ADT that we have so far left undefined
is how to specify the total order relation for comparing the keys associated with
each element. There are a number of ways of doing this, each having its particular
advantages and disadvantages.
The most direct solution is to implement a different priority queue based on
the element type and the manner of comparing elements. While this approach is
arguably simple, it is certainly not very general, since it would require that we
make many copies of essentially the same code. Maintaining multiple copies of the
nearly equivalent code is messy and error prone.
A better approach would be to design the priority queue as a templated class,
where the element type is specified by an abstract template argument, say E. We
assume that each concrete class that could serve as an element of our priority queue
provides a means for comparing two objects of type E. This could be done in
many ways. Perhaps we require that each object of type E provides a function
called comp that compares two objects of type E and determines which is larger.
Perhaps we require that the programmer defines a function that overloads the C++
comparison operator “<” for two objects of type E. (Recall Section 1.4.2 for a
discussion of operator overloading). In C++ jargon this is called a function object.
Let us consider a more concrete example. Suppose that class Point2D defines a
two-dimensional point. It has two public member functions, getX and getY, which
access its x and y coordinates, respectively. We could define a lexicographical less
than operator as follows. If the x coordinates differ we use their relative values;
otherwise, we use the relative values of the y coordinates.
bool operator<(const Point2D& p, const Point2D& q) {
if (p.getX() == q.getX())return p.getY() < q.getY();
elsereturn p.getX() < q.getX();
}
This approach of overloading the relational operators is general enough for
many situations, but it relies on the assumption that objects of the same type are
always compared in the same way. There are situations, however, where it is de
sirable to apply different comparisons to objects of the same type. Consider the
following examples.
Example 8.2: There are at least two ways of comparing the C++ character strings,
"4" and "12"". In the lexicographic ordering, which is an extension of the alpha
betic ordering to character strings, we have "4" > "12".". But if we interpret these
strings as integers, then "4" < "12"..
Page 349
8.1. The Priority Queue Abstract Data Type325
Example 8.3: A geometric algorithm may compare points p and q in two-dimen
sional space, by their x-coordinate (that is, p ≤ q if px ≤ qx), to sort them from left
to right, while another algorithm may compare them by their y-coordinate (that is,
p ≤ q if py ≤ qy), to sort them from bottom to top. In principle, there is nothing
pertaining to the concept of a point that says whether points should be compared
by x- or y-coordinates. Also, many other ways of comparing points can be defined
(for example, we can compare the distances of p and q from the origin).
There are a couple of ways to achieve the goal of independence of element
type and comparison method. The most general approach, called the composition
method, is based on defining each entry of our priority queue to be a pair (e,k),
consisting of an element e and a key k. The element part stores the data, and the
key part stores the information that defines the priority ordering. Each key object
defines its own comparison function. By changing the key class, we can change the
way in which the queue is ordered. This approach is very general, because the key
part does not need to depend on the data present in the element part. We study this
approach in greater detail in Chapter 9.
The approach that we use is a bit simpler than the composition method. It is
based on defining a special object, called a comparator, whose job is to provide a
definition of the comparison function between any two elements. This can be done
in various ways. In C++, a comparator for element type E can be implemented as
a class that defines a single function whose job is to compare two objects of type
E. One way to do this is to overload the “()” operator. The resulting function takes
two arguments, a and b, and returns a boolean whose value is true if a < b. For
example, if “isLess” is the name of our comparator object, the comparison function
is invoked using the following operation:
isLess(a,b): Return true if a < b and false otherwise.
It might seem at first that defining just a less-than function is rather limited, but
note that it is possible to derive all the other relational operators by combining less
than comparisons with other boolean operators. For example, we can test whether
a and b are equal with (!isLess(a,b) && !isLess(b, a)). (See Exercise R-8.3.)
Defining and Using Comparator Objects
Let us consider a more concrete example of a comparator class. As mentioned in
the above example, let us suppose that we have defined a class structure, called
Point2D, for storing a two-dimensional point. In Code Fragment 8.1, we present
two comparators. The comparator LeftRight implements a left-to-right order by
comparing the x-coordinates of the points, and the comparator BottomTop imple
ments a bottom-to-top order by comparing the y-coordinates of the points.
To use these comparators, we would declare two objects, one of each type.
Let us call them leftRight and bottomTop. Observe that these objects store no
Page 350
326Chapter 8. Heaps and Priority Queues
class LeftRight {// a left-right comparator
public:
bool operator()(const Point2D& p, const Point2D& q) const
{ return p.getX() < q.getX(); }
};
class BottomTop {// a bottom-top comparator
public:
bool operator()(const Point2D& p, const Point2D& q) const
{ return p.getY() < q.getY(); }
};
Code Fragment 8.1: Two comparator classes for comparing points. The first imple
ments a left-to-right order and the second implements a bottom-to-top order.
data members. They are used solely for the purposes of specifying a particular
comparison operator. Given two objects p and q, each of type Point2D, to test
whether p is to the left of q, we would invoke leftRight(p,q), and to test whether p
is below q, we would invoke bottomTop(p,q). Each invokes the “()” operator for
the corresponding class.
Next, let us see how we can use our comparators to implement two different be
haviors. Consider the generic function printSmaller shown in Code Fragment 8.2.
It prints the smaller of its two arguments. The function definition is templated by
the element type E and the comparator type C. The comparator class is assumed
to implement a less-than function for two objects of type E. The function is given
three arguments, the two elements p and q to be compared and an instance isLess of
a comparator for these elements. The function invokes the comparator to determine
which element is smaller, and then prints this value.
template <typename E, typename C> // element type and comparator
void printSmaller(const E& p, const E& q, const C& isLess) {
cout << (isLess(p, q) ? p : q) << endl; // print the smaller of p and q
}
Code Fragment 8.2: A generic function that prints the smaller of two elements,
given a comparator for these elements.
Finally, let us see how we can apply our function on two points. The code
is shown in Code Fragment 8.3. We declare to points p and q and initialize their
coordinates. (We have not presented the class definition for Point2D, but let us
assume that the constructor is given the x- and y-coordinates, and we have provided
an output operator.) We then declare two comparator objects, one for a left-to-right
ordering and the other for a bottom-to-top ordering. Finally, we invoke the function
printSmaller on the two points, changing only the comparator objects in each case.
Observe that, depending on which comparator is provided, the call to the func
Page 351
8.1. The Priority Queue Abstract Data Type327
Point2D p(1.3, 5.7), q(2.5, 0.6);// two points
LeftRight leftRight;// a left-right comparator
BottomTop bottomTop;// a bottom-top comparator
printSmaller(p, q, leftRight);// outputs: (1.3, 5.7)
printSmaller(p, q, bottomTop);// outputs: (2.5, 0.6)
Code Fragment 8.3: The use of two comparators to implement different behaviors
from the function printSmaller.
tion isLess in function printSmaller invokes either the “()” operator of class Left
Right or BottomTop. In this way, we obtain the desired result, two different be
haviors for the same two-dimensional point class.
Through the use of comparators, a programmer can write a general priority
queue implementation that works correctly in a wide variety of contexts. In par
ticular, the priority queues presented in this chapter are generic classes that are
templated by two types, the element E and the comparator C.
The comparator approach is a bit less general than the composition method,
because the comparator bases its decisions on the contents of the elements them
selves. In the composition method, the key may contain information that is not part
of the element object. The comparator approach has the advantage of being sim
pler, since we can insert elements directly into our priority queue without creating
element-key pairs. Furthermore, in Exercise R-8.4 we show that there is no real
loss of generality in using comparators.
8.1.3 The Priority Queue ADT
Having described the priority queue abstract data type at an intuitive level, we now
describe it in more detail. As an ADT, a priority queue P supports the following
functions:
size(): Return the number of elements in P.
empty(): Return true if P is empty and false otherwise.
insert(e): Insert a new element e into P.
min(): Return a reference to an element of P with the smallest
associated key value (but do not remove it); an error con
dition occurs if the priority queue is empty.
removeMin(): Remove from P the element referenced by min(); an er
ror condition occurs if the priority queue is empty.
As mentioned above, the primary functions of the priority queue ADT are the
insert, min, and removeMin operations. The other functions, size and empty, are
generic collection operations. Note that we allow a priority queue to have multiple
entries with the same key.
Page 352
328Chapter 8. Heaps and Priority Queues
Example 8.4: The following table shows a series of operations and their effects
on an initially empty priority queue P. Each element consists of an integer, which
we assume to be sorted according to the natural ordering of the integers. Note that
each call to min returns a reference to an entry in the queue, not the actual value.
Although the “Priority Queue” column shows the items in sorted order, the priority
queue need not store elements in this order.
Operation Output Priority Queue
insert(5)–{5}
insert(9)–{5,9}
insert(2)–{2,5,9}
insert(7)–{2,5,7,9}
min()[2]{2,5,7,9}
removeMin()–{5,7,9}
size()3{5,7,9}
min()[5]{5,7,9}
removeMin()–{7,9}
removeMin()–{9}
removeMin()–{}
empty()true {}
removeMin() “error” {}
8.1.4 A C++ Priority Queue Interface
Before discussing specific implementations of the priority queue, we first define
an informal C++ interface for a priority queue in Code Fragment 8.4. It is not a
complete C++ class, just a declaration of the public functions.
template <typename E, typename C>// element and comparator
class PriorityQueue {// priority-queue interface
public:
int size() const;// number of elements
bool isEmpty() const;// is the queue empty?
void insert(const E& e);// insert element
const E& min() const throw(QueueEmpty);// minimum element
void removeMin() throw(QueueEmpty);// remove minimum
};
Code Fragment 8.4: An informal PriorityQueue interface (not a complete class).
Although the comparator type C is included as a template argument, it does not
appear in the public interface. Of course, its value is relevant to any concrete imple
mentation. Observe that the function min returns a constant reference to the element
Page 353
8.1. The Priority Queue Abstract Data Type329
in the queue, which means that its value may be read and copied but not modified.
This is important because otherwise a user of the class might inadvertently modify
the element’s associated key value, and this could corrupt the integrity of the data
structure. The member functions size, empty, and min are all declared to be const,
which informs the compiler that they do not alter the contents of the queue.
An error condition occurs if either of the functions min or removeMin is called
on an empty priority queue. This is signaled by throwing an exception of type
QueueEmpty. Its definition is similar to others we have seen. (See Code Frag
ment 5.2.)
8.1.5 Sorting with a Priority Queue
Another important application of a priority queue is sorting, where we are given a
collection L of n elements that can be compared according to a total order relation,
and we want to rearrange them in increasing order (or at least in nondecreasing
order if there are ties). The algorithm for sorting L with a priority queue Q, called
PriorityQueueSort, is quite simple and consists of the following two phases:
1. In the first phase, we put the elements of L into an initially empty priority
queue P through a series of n insert operations, one for each element.
2. In the second phase, we extract the elements from P in nondecreasing order
by means of a series of n combinations of min and removeMin operations,
putting them back into L in order.
Pseudo-code for this algorithm is given in Code Fragment 8.5. It assumes that
L is given as an STL list, but the code can be adapted to other containers.
Algorithm PriorityQueueSort(L,P):
Input: An STL list L of n elements and a priority queue, P, that compares
elements using a total order relation
Output: The sorted list L
while !L.empty() do
e ← L.front
L.pop front(){remove an element e from the list}
P.insert(e){. . . and it to the priority queue}
while !P.empty() do
e ← P.min()
P.removeMin(){remove the smallest element e from the queue}
L.push back(e){. . . and append it to the back of L}
Code Fragment 8.5: Algorithm PriorityQueueSort, which sorts an STL list L with
the aid of a priority queue P.
Page 354
330Chapter 8. Heaps and Priority Queues
The algorithm works correctly for any priority queue P, no matter how P is
implemented. However, the running time of the algorithm is determined by the
running times of operations insert, min, and removeMin, which do depend on how
P is implemented. Indeed, PriorityQueueSort should be considered more a sorting
“scheme” than a sorting “algorithm,” because it does not specify how the priority
queue P is implemented. The PriorityQueueSort scheme is the paradigm of several
popular sorting algorithms, including selection-sort, insertion-sort, and heap-sort,
which we discuss in this chapter.
8.1.6 The STL priority queue Class
The C++ Standard Template Library (STL) provides an implementation of a pri
ority queue, called priority queue. As with the other STL classes we have seen,
such as stacks and queues, the STL priority queue is an example of a container.
In order to declare an object of type priority queue, it is necessary to first include
the definition file, which is called “queue.” As with other STL objects, the pri
ority queue is part of the std namespace, and hence it is necessary either to use
“std::priority queue” or to provide an appropriate “using” statement.
The priority queue class is templated with three parameters: the base type of
the elements, the underlying STL container in which the priority queue is stored,
and the comparator object. Only the first template argument is required. The second
parameter (the underlying container) defaults to the STL vector. The third param
eter (the comparator) defaults to using the standard C++ less-than operator (“<”).
The STL priority queue uses comparators in the same manner as we defined in Sec
tion 8.1.2. In particular, a comparator is a class that overrides the “()” operator in
order to define a boolean function that implements the less-than operator.
The code fragment below defines two STL priority queues. The first stores
integers. The second stores two-dimensional points under the left-to-right ordering
(recall Section 8.1.2).
#include <queue>
using namespace std;// make std accessible
priority queue<int> p1;// a priority queue of integers
// a priority queue of points with left-to-right order
priority queue<Point2D, vector<Point2D>, LeftRight> p2;
The principal member functions of the STL priority queue are given below. Let
p be declared to be an STL priority queue, and let e denote a single object whose
type is the same as the base type of the priority queue. (For example, p is a priority
queue of integers, and e is an integer.)
Page 355
8.2. Implementing a Priority Queue with a List331
size(): Return the number of elements in the priority queue.
empty(): Return true if the priority queue is empty and false oth
erwise.
push(e): Insert e in the priority queue.
top(): Return a constant reference to the largest element of the
priority queue.
pop(): Remove the element at the top of the priority queue.
Other than the differences in function names, the most significant difference
between our interface and the STL priority queue is that the functions top and pop
access the largest item in the queue according to priority order, rather than the
smallest. An example of the usage of the STL priority queue is shown in Code
Fragment 8.6.
priority queue<Point2D, vector<Point2D>, LeftRight> p2;
p2.push( Point2D(8.5, 4.6) );// add three points to p2
p2.push( Point2D(1.3, 5.7) );
p2.push( Point2D(2.5, 0.6) );
cout << p2.top() << endl; p2.pop();// output: (8.5, 4.6)
cout << p2.top() << endl; p2.pop();// output: (2.5, 0.6)
cout << p2.top() << endl; p2.pop();// output: (1.3, 5.7)
Code Fragment 8.6: An example of the use of the STL priority queue.
Of course, it is possible to simulate the same behavior as our priority queue by
defining the comparator object so that it implements the greater-than relation rather
than the less-than relation. This effectively reverses all order relations, and thus
the top function would instead return the smallest element, just as function min
does in our interface. Note that the STL priority queue does not perform any error
checking.
8.2 Implementing a Priority Queue with a List
In this section, we show how to implement a priority queue by storing its elements
in an STL list. (Recall this data structure from Section 6.2.4.) We consider two
realizations, depending on whether we sort the elements of the list.
Implementation with an Unsorted List
Let us first consider the implementation of a priority queue P by an unsorted doubly
linked list L. A simple way to perform the operation insert(e) on P is by adding
each new element at the end of L by executing the function L.push back(e). This
implementation of insert takes O(1) time.
Page 356
332Chapter 8. Heaps and Priority Queues
Since the insertion does not consider key values, the resulting list L is unsorted.
As a consequence, in order to perform either of the operations min or removeMin
on P, we must inspect all the entries of the list to find one with the minimum key
value. Thus, functions min and removeMin take O(n) time each, where n is the
number of elements in P at the time the function is executed. Moreover, each of
these functions runs in time proportional to n even in the best case, since they each
require searching the entire list to find the smallest element. Using the notation of
Section 4.2.3, we can say that these functions run in Θ(n) time. We implement
functions size and empty by simply returning the output of the corresponding func
tions executed on list L. Thus, by using an unsorted list to implement a priority
queue, we achieve constant-time insertion, but linear-time search and removal.
Implementation with a Sorted List
An alternative implementation of a priority queue P also uses a list L, except that
this time let us store the elements sorted by their key values. Specifically, we repre
sent the priority queue P by using a list L of elements sorted by nondecreasing key
values, which means that the first element of L has the smallest key.
We can implement function min in this case by accessing the element associated
with the first element of the list with the begin function of L. Likewise, we can
implement the removeMin function of P as L.pop front(). Assuming that L is
implemented as a doubly linked list, operations min and removeMin in P take O(1)
time, so are quite efficient.
This benefit comes at a cost, however, for now function insert of P requires that
we scan through the list L to find the appropriate position in which to insert the new
entry. Thus, implementing the insert function of P now takes O(n) time, where
n is the number of entries in P at the time the function is executed. In summary,
when using a sorted list to implement a priority queue, insertion runs in linear time
whereas finding and removing the minimum can be done in constant time.
Table 8.1 compares the running times of the functions of a priority queue real
ized by means of an unsorted and sorted list, respectively. There is an interesting
contrast between the two functions. An unsorted list allows for fast insertions but
slow queries and deletions, while a sorted list allows for fast queries and deletions,
but slow insertions.
Operation Unsorted List Sorted List
size, emptyO(1)O(1)
insertO(1)O(n)
min, removeMinO(n)O(1)
Table 8.1: Worst-case running times of the functions of a priority queue of size n,
realized by means of an unsorted or sorted list, respectively. We assume that the
list is implemented by a doubly linked list. The space requirement is O(n).
Page 357
8.2. Implementing a Priority Queue with a List333
8.2.1 A C++ Priority Queue Implementation using a List
In Code Fragments 8.7 through 8.10, we present a priority queue implementation
that stores the elements in a sorted list. The list is implemented using an STL list
object (see Section 6.3.2), but any implementation of the list ADT would suffice.
In Code Fragment 8.7, we present the class definition for our priority queue.
The public part of the class is essentially the same as the interface that was pre
sented earlier in Code Fragment 8.4. In order to keep the code as simple as possi
ble, we have omitted error checking. The class’s data members consists of a list,
which holds the priority queue’s contents, and an instance of the comparator object,
which we call isLess.
template <typename E, typename C>
class ListPriorityQueue {
public:
int size() const;// number of elements
bool empty() const;// is the queue empty?
void insert(const E& e);// insert element
const E& min() const;// minimum element
void removeMin();// remove minimum
private:
std::list<E> L;// priority queue contents
C isLess;// less-than comparator
};
Code Fragment 8.7: The class definition for a priority queue based on an STL list.
We have not bothered to give an explicit constructor for our class, relying in
stead on the default constructor. The default constructor for the STL list produces
an empty list, which is exactly what we want.
Next, in Code Fragment 8.8, we present the implementations of the simple
member functions size and empty. Recall that, when dealing with templated classes,
it is necessary to repeat the full template specifications when defining member func
tions outside the class. Each of these functions simply invokes the corresponding
function for the STL list.
template <typename E, typename C> // number of elements
int ListPriorityQueue<E,C>::size() const
{ return L.size(); }
template <typename E, typename C> // is the queue empty?
bool ListPriorityQueue<E,C>::empty() const
{ return L.empty(); }
Code Fragment 8.8: Implementations of the functions size and empty.
Page 358
334Chapter 8. Heaps and Priority Queues
Let us now consider how to insert an element e into our priority queue. We
define p to be an iterator for the list. Our approach is to walk through the list until
we first find an element whose key value is larger than e’s, and then we insert e just
prior to p. Recall that *p accesses the element referenced by p, and ++p advances
p to the next element of the list. We stop the search either when we reach the
end of the list or when we first encounter a larger element, that is, one satisfying
isLess(e,*p). On reaching such an entry, we insert e just prior to it, by invoking the
STL list function insert. The code is shown in Code Fragment 8.9.
template <typename E, typename C>// insert element
void ListPriorityQueue<E,C>::insert(const E& e) {
typename std::list<E>::iterator p;
p = L.begin();
while (p != L.end() && !isLess(e, *p)) ++p;// find larger element
L.insert(p, e);// insert e before p
}
Code Fragment 8.9: Implementation of the priority queue function insert.
Consider how the above function behaves when e has a key value larger than
any in the queue. In such a case, the while loop exits under the condition that p is
equal to L.end(). Recall that L.end() refers to an imaginary element that lies just
beyond the end of the list. Thus, by inserting before this element, we effectively
append e to the back of the list, as desired.
You might notice the use of the keyword “typename” in the declaration of the
iterator p. This is due to a subtle issue in C++ involving dependent names, which
arises when processing name bindings within templated objects in C++. We do not
delve into the intricacies of this issue. For now, it suffices to remember to simply
Cautioninclude the keyword typename when using a template parameter (in this case E)
to define another type.
Finally, let us consider the operations min and removeMin. Since the list is
sorted in ascending order by key values, in order to implement min, we simply
return a reference to the front of the list. To implement removeMin, we remove the
front element of the list. The implementations are given in Code Fragment 8.10.
template <typename E, typename C>// minimum element
const E& ListPriorityQueue<E,C>::min() const
{ return L.front(); }// minimum is at the front
template <typename E, typename C>// remove minimum
void ListPriorityQueue<E,C>::removeMin()
{ L.pop front(); }
Code Fragment 8.10: Implementations of the priority queue functions min and
removeMin.
Page 359
8.2. Implementing a Priority Queue with a List335
8.2.2 Selection-Sort and Insertion-Sort
Recall the PriorityQueueSort scheme introduced in Section 8.1.5. We are given an
unsorted list L containing n elements, which we sort using a priority queue P in two
phases. In the first phase, we insert all the elements, and in the second phase, we
repeatedly remove elements using the min and removeMin operations.
Selection-Sort
If we implement the priority queue P with an unsorted list, then the first phase of
PriorityQueueSort takes O(n) time, since we can insert each element in constant
time. In the second phase, the running time of each min and removeMin operation
is proportional to the number of elements currently in P. Thus, the bottleneck
computation in this implementation is the repeated “selection” of the minimum
element from an unsorted list in the second phase. For this reason, this algorithm
is better known as selection-sort. (See Figure 8.1.)
List LPriority Queue P
Input(7,4,8,2,5,3,9)()
Phase 1 (a)(4,8,2,5,3,9)(7)
(b)(8,2,5,3,9)(7,4)
.........
(g)()(7,4,8,2,5,3,9)
Phase 2 (a)(2)(7,4,8,5,3,9)
(b)(2,3)(7,4,8,5,9)
(c)(2,3,4)(7,8,5,9)
(d)(2,3,4,5)(7,8,9)
(e)(2,3,4,5,7)(8,9)
(f)(2,3,4,5,7,8)(9)
(g) (2,3,4,5,7,8,9)()
Figure 8.1: Execution of selection-sort on list L = (7,4,8,2,5,3,9).
As noted above, the bottleneck is the second phase, where we repeatedly re
move an element with smallest key from the priority queue P. The size of P starts
at n and decreases to 0 with each removeMin. Thus, the first removeMin operation
takes time O(n), the second one takes time O(n−1), and so on. Therefore, the total
time needed for the second phase is
O(n+ (n−1)+···+ 2 + 1) = O(∑n i=1 i).
By Proposition 4.3, we have ∑n i=1 i = n(n + 1)/2. Thus, phase two takes O(n2)
time, as does the entire selection-sort algorithm.
Page 360
336Chapter 8. Heaps and Priority Queues
Insertion-Sort
If we implement the priority queue P using a sorted list, then we improve the run
ning time of the second phase to O(n), because each operation min and removeMin
on P now takes O(1) time. Unfortunately, the first phase now becomes the bottle
neck for the running time, since, in the worst case, each insert operation takes time
proportional to the size of P. This sorting algorithm is therefore better known as
insertion-sort (see Figure 8.2), for the bottleneck in this sorting algorithm involves
the repeated “insertion” of a new element at the appropriate position in a sorted list.
List LPriority Queue P
Input(7,4,8,2,5,3,9)()
Phase 1 (a)(4,8,2,5,3,9)(7)
(b)(8,2,5,3,9)(4,7)
(c)(2,5,3,9)(4,7,8)
(d)(5,3,9)(2,4,7,8)
(e)(3,9)(2,4,5,7,8)
(f)(9)(2,3,4,5,7,8)
(g)()(2,3,4,5,7,8,9)
Phase 2 (a)(2)(3,4,5,7,8,9)
(b)(2,3)(4,5,7,8,9)
.........
(g) (2,3,4,5,7,8,9)()
Figure 8.2: Execution of insertion-sort on list L = (7,4,8,2,5,3,9). In Phase 1,
we repeatedly remove the first element of L and insert it into P, by scanning the
list implementing P until we find the correct place for this element. In Phase 2,
we repeatedly perform removeMin operations on P, each of which returns the first
element of the list implementing P, and we add the element at the end of L.
Analyzing the running time of Phase 1 of insertion-sort, we note that
O(1 + 2 +...+ (n−1)+ n) = O(∑n i=1 i).
Again, by recalling Proposition 4.3, the first phase runs in O(n2) time; hence, so
does the entire algorithm.
Alternately, we could change our definition of insertion-sort so that we insert
elements starting from the end of the priority-queue sequence in the first phase,
in which case performing insertion-sort on a list that is already sorted would run
in O(n) time. Indeed, the running time of insertion-sort is O(n + I) in this case,
where I is the number of inversions in the input list, that is, the number of pairs of
elements that start out in the input list in the wrong relative order.
Page 361
8.3. Heaps337
8.3 Heaps
The two implementations of the PriorityQueueSort scheme presented in the previ
ous section suggest a possible way of improving the running time for priority-queue
sorting. One algorithm (selection-sort) achieves a fast running time for the first
phase, but has a slow second phase, whereas the other algorithm (insertion-sort)
has a slow first phase, but achieves a fast running time for the second phase. If we
could somehow balance the running times of the two phases, we might be able to
significantly speed up the overall running time for sorting. This approach is, in fact,
exactly what we can achieve using the priority-queue implementation discussed in
this section.
An efficient realization of a priority queue uses a data structure called a heap.
This data structure allows us to perform both insertions and removals in logarith
mic time, which is a significant improvement over the list-based implementations
discussed in Section 8.2. The fundamental way the heap achieves this improvement
is to abandon the idea of storing elements and keys in a list and take the approach
of storing elements and keys in a binary tree instead.
8.3.1 The Heap Data Structure
A heap (see Figure 8.3) is a binary tree T that stores a collection of elements with
their associated keys at its nodes and that satisfies two additional properties: a
relational property, defined in terms of the way keys are stored in T, and a structural
property, defined in terms of the nodes of T itself. We assume that a total order
relation on the keys is given, for example, by a comparator.
The relational property of T, defined in terms of the way keys are stored, is the
following:
Heap-Order Property: In a heap T, for every node v other than the root, the key
associated with v is greater than or equal to the key associated with v’s parent.
As a consequence of the heap-order property, the keys encountered on a path from
the root to an external node of T are in nondecreasing order. Also, a minimum key
is always stored at the root of T. This is the most important key and is informally
said to be “at the top of the heap,” hence, the name “heap” for the data structure.
By the way, the heap data structure defined here has nothing to do with the free
store memory heap (Section 14.1.1) used in the run-time environment supporting
programming languages like C++.
You might wonder why heaps are defined with the smallest key at the top,
rather than the largest. The distinction is arbitrary. (This is evidenced by the fact
that the STL priority queue does exactly the opposite.) Recall that a comparator
Page 362
338Chapter 8. Heaps and Priority Queues
Figure 8.3: Example of a heap storing 13 elements. Each element is a key-value
pair of the form (k,v). The heap is ordered based on the key value, k, of each
element.
implements the less-than operator between two keys. Suppose that we had instead
defined our comparator to indicate the opposite of the standard total order relation
between keys (so that, for example, isLess(x,y) would return true if x were greater
than y). Then the root of the resulting heap would store the largest key. This
versatility comes essentially for free from our use of the comparator pattern. By
Caution defining the minimum key in terms of the comparator, the “minimum” key with
a “reverse” comparator is in fact the largest. Thus, without loss of generality, we
assume that we are always interested in the minimum key, which is always at the
root of the heap.
For the sake of efficiency, which becomes clear later, we want the heap T to
have as small a height as possible. We enforce this desire by insisting that the heap
T satisfy an additional structural property, it must be complete. Before we define
this structural property, we need some definitions. We recall from Section 7.3.3
that level i of a binary tree T is the set of nodes of T that have depth i. Given nodes
v and w on the same level of T , we say that v is to the left of w if v is encountered
before w in an inorder traversal of T . That is, there is a node u of T such that v is
in the left subtree of u and w is in the right subtree of u. For example, in the binary
tree of Figure 8.3, the node storing entry (15,K) is to the left of the node storing
entry (7,Q). In a standard drawing of a binary tree, the “to the left of” relation is
visualized by the relative horizontal placement of the nodes.
Complete Binary Tree Property: A heap T with height h is a complete binary
tree, that is, levels 0,1,2,... ,h−1 of T have the maximum number of nodes
possible (namely, level i has 2i nodes, for 0 ≤ i ≤ h − 1) and the nodes at
level h fill this level from left to right.
Page 363
8.3. Heaps339
The Height of a Heap
Let h denote the height of T . Another way of defining the last node of T is that
it is the node on level h such that all the other nodes of level h are to the left of
it. Insisting that T be complete also has an important consequence as shown in
Proposition 8.5.
Proposition 8.5: A heap T storing n entries has height
h = ⌊log n⌋.
Justification: From the fact that T is complete, we know that there are 2i nodes
in level, i for 0 ≤ i ≤ h − 1, and level h has at least 1 node. Thus, the number of
nodes of T is at least
(1+2+4+···+2h−1)+1 = (2h − 1)+1
= 2h.
Level h has at most 2h nodes, and thus the number of nodes of T is at most
(1+2+4+···+2h−1)+2h = 2h+1 − 1.
Since the number of nodes is equal to the number n of entries, we obtain
2h ≤ n
and
n ≤ 2h+1 − 1.
Thus, by taking logarithms of both sides of these two inequalities, we see that
h ≤ log n
and
log(n+1)− 1 ≤ h.
Since h is an integer, the two inequalities above imply that
h = ⌊log n⌋.
Proposition 8.5 has an important consequence. It implies that if we can perform
update operations on a heap in time proportional to its height, then those operations
will run in logarithmic time. Therefore, let us turn to the problem of how to effi
ciently perform various priority queue functions using a heap.
Page 364
340Chapter 8. Heaps and Priority Queues
8.3.2 Complete Binary Trees and Their Representation
Let us discuss more about complete binary trees and how they are represented.
The Complete Binary Tree ADT
As an abstract data type, a complete binary tree T supports all the functions of the
binary tree ADT (Section 7.3.1), plus the following two functions:
add(e): Add to T and return a new external node v storing ele
ment e, such that the resulting tree is a complete binary
tree with last node v.
remove(): Remove the last node of T and return its element.
By using only these update operations, the resulting tree is guaranteed to be a com
plete binary. As shown in Figure 8.4, there are essentially two cases for the effect
of an add (and remove is similar).
• If the bottom level of T is not full, then add inserts a new node on the bottom
level of T, immediately after the rightmost node of this level (that is, the last
node); hence, T’s height remains the same.
• If the bottom level is full, then add inserts a new node as the left child of the
leftmost node of the bottom level of T; hence, T’s height increases by one.
w
(a)(b)
w
(c)(d)
Figure 8.4: Examples of operations add and remove on a complete binary tree,
where w denotes the node inserted by add or deleted by remove. The trees shown
in (b) and (d) are the results of performing add operations on the trees in (a) and (c),
respectively. Likewise, the trees shown in (a) and (c) are the results of performing
remove operations on the trees in (b) and (d), respectively.
Page 365
8.3. Heaps341
A Vector Representation of a Complete Binary Tree
The vector-based binary tree representation (recall Section 7.3.5) is especially suit
able for a complete binary tree T. We recall that in this implementation, the nodes
of T are stored in a vector A such that node v in T is the element of A with index
equal to the level number f (v) defined as follows:
• If v is the root of T, then f (v) = 1
• If v is the left child of node u, then f (v) = 2 f (u)
• If v is the right child of node u, then f (v) = 2 f (u)+ 1
With this implementation, the nodes of T have contiguous indices in the range [1,n]
and the last node of T is always at index n, where n is the number of nodes of T.
Figure 8.5 shows two examples illustrating this property of the last node.
w
w
(a)(b)
w065544332211
w076655443322118
(c)(d)
Figure 8.5: Two examples showing that the last node w of a heap with n nodes
has level number n: (a) heap T1 with more than one node on the bottom level;
(b) heap T2 with one node on the bottom level; (c) vector-based representation
of T1; (d) vector-based representation of T2.
The simplifications that come from representing a complete binary tree T with
a vector aid in the implementation of functions add and remove. Assuming that
no array expansion is necessary, functions add and remove can be performed in
O(1) time because they simply involve adding or removing the last element of the
vector. Moreover, the vector associated with T has n + 1 elements (the element at
index 0 is a placeholder). If we use an extendable array that grows and shrinks
for the implementation of the vector (for example, the STL vector class), the space
used by the vector-based representation of a complete binary tree with n nodes is
O(n) and operations add and remove take O(1) amortized time.
Page 366
342Chapter 8. Heaps and Priority Queues
A C++ Implementation of a Complete Binary Tree
We present the complete binary tree ADT as an informal interface, called Com
pleteTree, in Code Fragment 8.11. As with our other informal interfaces, this is not
a complete C++ class. It just gives the public portion of the class.
The interface defines a nested class, called Position, which represents a node of
the tree. We provide the necessary functions to access the root and last positions and
to navigate through the tree. The modifier functions add and remove are provided,
along with a function swap, which swaps the contents of two given nodes.
template <typename E>
class CompleteTree {// left-complete tree interface
public:// publicly accessible types
class Position;// node position type
int size() const;// number of elements
Position left(const Position& p);// get left child
Position right(const Position& p);// get right child
Position parent(const Position& p);// get parent
bool hasLeft(const Position& p) const; // does node have left child?
bool hasRight(const Position& p) const; // does node have right child?
bool isRoot(const Position& p) const;// is this the root?
Position root();// get root position
Position last();// get last node
void addLast(const E& e);// add a new last node
void removeLast();// remove the last node
void swap(const Position& p, const Position& q); // swap node contents
};
Code Fragment 8.11: Interface CompleteBinaryTree for a complete binary tree.
In order to implement this interface, we store the elements in an STL vector,
called V. We implement a tree position as an iterator to this vector. To convert from
the index representation of a node to this positional representation, we provide a
function pos. The reverse conversion is provided by function idx. This portion of
the class definition is given in Code Fragment 8.12.
private:// member data
std::vector<E> V;// tree contents
public:// publicly accessible types
typedef typename std::vector<E>::iterator Position; // a position in the tree
protected:// protected utility functions
Position pos(int i)// map an index to a position
{ return V.begin() + i; }
int idx(const Position& p) const// map a position to an index
{ return p − V.begin(); }
Code Fragment 8.12: Member data and private utilities for a complete tree class.
Page 367
8.3. Heaps343
Given the index of a node i, the function pos maps it to a position by adding
i to V.begin(). Here we are exploiting the fact that the STL vector supports a
random-access iterator (recall Section 6.2.5). In particular, given an integer i, the
expression V.begin() + i yields the position of the ith element of the vector, and,
given a position p, the expression p−V.begin() yields the index of position p.
We present a full implementation of a vector-based complete tree ADT in Code
Fragment 8.13. Because the class consists of a large number of small one-line
functions, we have chosen to violate our normal coding conventions by placing all
the function definitions inside the class definition.
template <typename E>
class VectorCompleteTree {
//. . . insert private member data and protected utilities here
public:
VectorCompleteTree() : V(1) {}// constructor
int size() const{ return V.size() − 1; }
Position left(const Position& p){ return pos(2*idx(p)); }
Position right(const Position& p){ return pos(2*idx(p) + 1); }
Position parent(const Position& p){ return pos(idx(p)/2); }
bool hasLeft(const Position& p) const { return 2*idx(p) <= size(); }
bool hasRight(const Position& p) const { return 2*idx(p) + 1 <= size(); }
bool isRoot(const Position& p) const { return idx(p) == 1; }
Position root(){ return pos(1); }
Position last(){ return pos(size()); }
void addLast(const E& e){ V.push back(e); }
void removeLast(){ V.pop back(); }
void swap(const Position& p, const Position& q)
{ E e = *q; *q = *p; *p = e; }
};
Code Fragment 8.13: A vector-based implementation of the complete tree ADT.
Recall from Section 7.3.5 that the root node is at index 1 of the vector. Since
STL vectors are indexed starting at 0, our constructor creates the initial vector with
one element. This element at index 0 is never used. As a consequence, the size of
the priority queue is one less than the size of the vector.
Recall from Section 7.3.5 that, given a node at index i, its left and right children
are located at indices 2i and 2i+1, respectively. Its parent is located at index ⌊i/2⌋.
Given a position p, the functions left, right, and parent first convert p to an index
using the utility idx, which is followed by the appropriate arithmetic operation on
this index, and finally they convert the index back to a position using the utility pos.
We determine whether a node has a child by evaluating the index of this child
and testing whether the node at that index exists in the vector. Operations add
and remove are implemented by adding or removing the last entry of the vector,
respectively.
Page 368
344Chapter 8. Heaps and Priority Queues
8.3.3 Implementing a Priority Queue with a Heap
We now discuss how to implement a priority queue using a heap. Our heap-based
representation for a priority queue P consists of the following (see Figure 8.6):
• heap: A complete binary tree T whose nodes store the elements of the queue
and whose keys satisfy the heap-order property. We assume the binary tree T
is implemented using a vector, as described in Section 8.3.2. For each node
v of T, we denote the associated key by k(v).
• comp: A comparator that defines the total order relation among the keys.
Figure 8.6: Illustration of the heap-based implementation of a priority queue.
With this data structure, functions size and empty take O(1) time, as usual. In
addition, function min can also be easily performed in O(1) time by accessing the
entry stored at the root of the heap (which is at index 1 in the vector).
Insertion
Let us consider how to perform insert on a priority queue implemented with a
heap T. To store a new element e in T, we add a new node z to T with operation add,
so that this new node becomes the last node of T, and then store e in this node.
After this action, the tree T is complete, but it may violate the heap-order prop
erty. Hence, unless node z is the root of T (that is, the priority queue was empty
before the insertion), we compare key k(z) with the key k(u) stored at the parent
u of z. If k(z) ≥ k(u), the heap-order property is satisfied and the algorithm ter
minates. If instead k(z) < k(u), then we need to restore the heap-order property,
which can be locally achieved by swapping the entries stored at z and u. (See Fig
ures 8.7(c) and (d).) This swap causes the new entry (k,e) to move up one level.
Again, the heap-order property may be violated, and we continue swapping, going
Page 369
8.3. Heaps345
(a)(b)
(c)(d)
(e)(f)
(g)(h)
Figure 8.7: Insertion of a new entry with key 2 into the heap of Figure8.6: (a) initial
heap; (b) after performing operation add; (c) and (d) swap to locally restore the
partial order property; (e) and (f) another swap; (g) and (h)final swap.
Page 370
346Chapter 8. Heaps and Priority Queues
up in T until no violation of the heap-order property occurs. (See Figures8.7(e)
and (h).)
The upward movement of the newly inserted entry by means of swaps is con
ventionally called up-heap bubbling. A swap either resolves the violation of the
heap-order property or propagates it one level up in the heap. In the worst case, up
heap bubbling causes the new entry to move all the way up to the root of heap T.
(See Figure 8.7.) Thus, in the worst case, the number of swaps performed in the
execution of function insert is equal to the height of T, that is, it is ⌊logn⌋ by
Proposition 8.5.
Removal
Let us now turn to function removeMin of the priority queue ADT. The algorithm
for performing function removeMin using heap T is illustrated in Figure 8.8.
We know that an element with the smallest key is stored at the root r of T (even
if there is more than one entry with the smallest key). However, unless r is the
only node of T, we cannot simply delete node r, because this action would disrupt
the binary tree structure. Instead, we access the last node w of T, copy its entry
to the root r, and then delete the last node by performing operation remove of the
complete binary tree ADT. (See Figure 8.8(a) and (b).)
Down-Heap Bubbling after a Removal
We are not necessarily done, however, for, even though T is now complete, T may
now violate the heap-order property. If T has only one node (the root), then the
heap-order property is trivially satisfied and the algorithm terminates. Otherwise,
we distinguish two cases, where r denotes the root of T:
• If r has no right child, let s be the left child of r
• Otherwise (r has both children), let s be a child of r with the smaller key
If k(r) ≤ k(s), the heap-order property is satisfied and the algorithm terminates.
If instead k(r) > k(s), then we need to restore the heap-order property, which can
be locally achieved by swapping the entries stored at r and s. (See Figure 8.8(c)
and (d).) (Note that we shouldn’t swap r with s’s sibling.) The swap we perform
restores the heap-order property for node r and its children, but it may violate this
property at s; hence, we may have to continue swapping down T until no violation
of the heap-order property occurs. (See Figure 8.8(e) and (h).)
This downward swapping process is called down-heap bubbling. A swap either
resolves the violation of the heap-order property or propagates it one level down in
the heap. In the worst case, an entry moves all the way down to the bottom level.
(See Figure 8.8.) Thus, the number of swaps performed in the execution of function
removeMin is, in the worst case, equal to the height of heap T, that is, it is ⌊logn⌋
by Proposition 8.5
Page 371
8.3. Heaps347
(a)(b)
(c)(d)
(e)(f)
(g)(h)
Figure 8.8: Removing the element with the smallest key from a heap: (a) and (b)
deletion of the last node, whose element is moved to the root; (c) and (d) swap to
locally restore the heap-order property; (e) and (f) another swap; (g) and (h) final
swap.
Page 372
348Chapter 8. Heaps and Priority Queues
Analysis
Table 8.2 shows the running time of the priority queue ADT functions for the heap
implementation of a priority queue, assuming that two keys can be compared in
O(1) time and that the heap T is implemented with either a vector or linked struc
ture.
Operation Time
size, empty O(1)
min O(1)
insert O(logn)
removeMin O(logn)
Table 8.2: Performance of a priority queue realized by means of a heap, which
is in turn implemented with a vector or linked structure. We denote with n the
number of entries in the priority queue at the time a method is executed. The
space requirement is O(n). The running time of operations insert and removeMin
is worst case for the array-list implementation of the heap and amortized for the
linked representation.
In short, each of the priority queue ADT functions can be performed in O(1)
time or in O(logn) time, where n is the number of elements at the time the function
is executed. This analysis is based on the following:
• The heap T has n nodes, each storing a reference to an entry
• Operations add and remove on T take either O(1) amortized time (vector
representation) or O(logn) worst-case time
• In the worst case, up-heap and down-heap bubbling perform a number of
swaps equal to the height of T
• The height of heap T is O(logn), since T is complete (Proposition 8.5)
Thus, if heap T is implemented with the linked structure for binary trees, the space
needed is O(n). If we use a vector-based implementation for T instead, then the
space is proportional to the size N of the array used for the vector representing T.
We conclude that the heap data structure is a very efficient realization of the
priority queue ADT, independent of whether the heap is implemented with a linked
structure or a vector. The heap-based implementation achieves fast running times
for both insertion and removal, unlike the list-based priority queue implementa
tions. Indeed, an important consequence of the efficiency of the heap-based imple
mentation is that it can speed up priority-queue sorting to be much faster than the
list-based insertion-sort and selection-sort algorithms.
Page 373
8.3. Heaps349
8.3.4 C++ Implementation
In this section, we present a heap-based priority queue implementation. The heap
is implemented using the vector-based complete tree implementation, which we
presented in Section 8.3.2.
In Code Fragment 8.7, we present the class definition. The public part of the
class is essentially the same as the interface, but, in order to keep the code simple,
we have ignored error checking. The class’s data members consists of the complete
tree, named T, and an instance of the comparator object, named isLess. We have
also provided a type definition for a node position in the tree, called Position.
template <typename E, typename C>
class HeapPriorityQueue {
public:
int size() const;// number of elements
bool empty() const;// is the queue empty?
void insert(const E& e);// insert element
const E& min();// minimum element
void removeMin();// remove minimum
private:
VectorCompleteTree<E> T;// priority queue contents
C isLess;// less-than comparator
// shortcut for tree position
typedef typename VectorCompleteTree<E>::Position Position;
};
Code Fragment 8.14: A heap-based implementation of a priority queue.
In Code Fragment 8.15, we present implementations of the simple member
functions size, empty, and min. The function min returns a reference to the root’s
element through the use of the “*” operator, which is provided by the Position class
of VectorCompleteTree.
template <typename E, typename C> // number of elements
int HeapPriorityQueue<E,C>::size() const
{ return T.size(); }
template <typename E, typename C> // is the queue empty?
bool HeapPriorityQueue<E,C>::empty() const
{ return size() == 0; }
template <typename E, typename C> // minimum element
const E& HeapPriorityQueue<E,C>::min()
{ return *(T.root()); }// return reference to root element
Code Fragment 8.15: The member functions size, empty, and min.
Page 374
350Chapter 8. Heaps and Priority Queues
Next, in Code Fragment 8.16, we present an implementation of the insert op
eration. As outlined in the previous section, this works by adding the new element
to the last position of the tree and then it performs up-heap bubbling by repeatedly
swapping this element with its parent until its parent has a smaller key value.
template <typename E, typename C> // insert element
void HeapPriorityQueue<E,C>::insert(const E& e) {
T.addLast(e);// add e to heap
Position v = T.last();// e’s position
while (!T.isRoot(v)) {// up-heap bubbling
Position u = T.parent(v);
if (!isLess(*v, *u)) break;// if v in order, we’re done
T.swap(v, u);// . . .else swap with parent
v = u;
}
}
Code Fragment 8.16: An implementation of the function insert.
Finally, let us consider the removeMin operation. If the tree has only one node,
then we simply remove it. Otherwise, we swap the root’s element with the last
element of the tree and remove the last element. We then apply down-heap bubbling
to the root. Letting u denote the current node, this involves determining u’s smaller
child, which is stored in v. If the child’s key is smaller than u’s, we swap u’s
contents with this child’s. The code is presented in Code Fragment 8.17.
template <typename E, typename C> // remove minimum
void HeapPriorityQueue<E,C>::removeMin() {
if (size() == 1)// only one node?
T.removeLast();// . . .remove it
else {
Position u = T.root();// root position
T.swap(u, T.last());// swap last with root
T.removeLast();// . . .and remove last
while (T.hasLeft(u)) {// down-heap bubbling
Position v = T.left(u);
if (T.hasRight(u) && isLess(*(T.right(u)), *v))
v = T.right(u);// v is u’s smaller child
if (isLess(*v, *u)) {// is u out of order?
T.swap(u, v);// . . .then swap
u = v;
}
else break;// else we’re done
}
}
}
Code Fragment 8.17: A heap-based implementation of a priority queue.
Page 375
8.3. Heaps351
8.3.5 Heap-Sort
As we have previously observed, realizing a priority queue with a heap has the
advantage that all the functions in the priority queue ADT run in logarithmic time or
better. Hence, this realization is suitable for applications where fast running times
are sought for all the priority queue functions. Therefore, let us again consider the
PriorityQueueSort sorting scheme from Section 8.1.5, which uses a priority queue
P to sort a list L.
During Phase 1, the i-th insert operation (1 ≤ i ≤ n) takes O(1 + logi) time,
since the heap has i entries after the operation is performed. Likewise, during
Phase 2, the j-th removeMin operation (1 ≤ j ≤ n) runs in time O(1+log(n− j+1),
since the heap has n− j + 1 entries at the time the operation is performed. Thus,
each phase takes O(nlogn) time, so the entire priority-queue sorting algorithm runs
in O(nlogn) time when we use a heap to implement the priority queue. This sorting
algorithm is better known as heap-sort, and its performance is summarized in the
following proposition.
Proposition 8.6: The heap-sort algorithm sorts a list L of n elements in O(nlogn)
time, assuming two elements of L can be compared in O(1) time.
Let us stress that the O(nlogn) running time of heap-sort is considerably better
than the O(n2) running time of selection-sort and insertion-sort (Section 8.2.2) and
is essentially the best possible for any sorting algorithm.
Implementing Heap-Sort In-Place
If the list L to be sorted is implemented by means of an array, we can speed up heap
sort and reduce its space requirement by a constant factor using a portion of the list
L itself to store the heap, thus avoiding the use of an external heap data structure.
This performance is accomplished by modifying the algorithm as follows:
1. We use a reverse comparator, which corresponds to a heap where the largest
element is at the top. At any time during the execution of the algorithm, we
use the left portion of L, up to a certain rank i−1, to store the elements in the
heap, and the right portion of L, from rank i to n−1 to store the elements in
the list. Thus, the first i elements of L (at ranks 0,...,i−1) provide the vector
representation of the heap (with modified level numbers starting at 0 instead
of 1), that is, the element at rank k is greater than or equal to its “children” at
ranks 2k + 1 and 2k + 2.
2. In the first phase of the algorithm, we start with an empty heap and move the
boundary between the heap and the list from left to right, one step at a time.
In step i (i = 1,...,n), we expand the heap by adding the element at rank i−1
and perform up-heap bubbling.
Page 376
352Chapter 8. Heaps and Priority Queues
3. In the second phase of the algorithm, we start with an empty list and move
the boundary between the heap and the list from right to left, one step at a
time. At step i (i = 1,...,n), we remove a maximum element from the heap
and store it at rank n−i.
The above variation of heap-sort is said to be in-place, since we use only a con
stant amount of space in addition to the list itself. Instead of transferring elements
out of the list and then back in, we simply rearrange them. We illustrate in-place
heap-sort in Figure 8.9. In general, we say that a sorting algorithm is in-place if it
uses only a constant amount of memory in addition to the memory needed for the
objects being sorted themselves. A sorting algorithm is considered space-efficient
if it can be implemented in-place.
7
33(a)
(b)
(c)
(d)
(e)4 3 2 1 7
7 4 2 1 34
3
1
3
1 2
2
1
13 21 4 7
2 1 43 7
14327(f)
(g)
(h)
(i)
7
4
1 3223 742 1
7 3 2 1 4
7 3 2 1 47
3 2
7
3
127 2 134
(j)43217
Figure 8.9: In-place heap-sort. Parts (a) through (e) show the addition of elements
to the heap; (f) through (j) show the removal of successive elements. The portions
of the array that are used for the heap structure are shown in blue.
Page 377
8.3. Heaps353
8.3.6 Bottom-Up Heap Construction ⋆
The analysis of the heap-sort algorithm shows that we can construct a heap stor
ing n elements in O(nlog n) time, by means of n successive insert operations, and
then use that heap to extract the elements in order. However, if all the elements to
be stored in the heap are given in advance, there is an alternative bottom-up con
struction function that runs in O(n) time. We describe this function in this section,
observing that it can be included as one of the constructors in a Heap class instead
of filling a heap using a series of n insert operations. For simplicity, we describe
this bottom-up heap construction assuming the number n of keys is an integer of the
type n = 2h − 1. That is, the heap is a complete binary tree with every level being
full, so the heap has height h = log(n+1). Viewed nonrecursively, bottom-up heap
construction consists of the following h = log(n + 1) steps:
1. In the first step (see Figure 8.10(a)), we construct (n+1)/2 elementary heaps
storing one entry each.
2. In the second step (see Figure 8.10(b)–(c)), we form (n + 1)/4 heaps, each
storing three entries, by joining pairs of elementary heaps and adding a new
entry. The new entry is placed at the root and may have to be swapped with
the entry stored at a child to preserve the heap-order property.
3. In the third step (see Figure 8.10(d)–(e)), we form (n + 1)/8 heaps, each
storing 7 entries, by joining pairs of 3-entry heaps (constructed in the pre
vious step) and adding a new entry. The new entry is placed initially at the
root, but may have to move down with a down-heap bubbling to preserve the
heap-order property.
...
i. In the generic ith step, 2 ≤ i ≤ h, we form (n+1)/2i heaps, each storing 2i−1
entries, by joining pairs of heaps storing (2i−1 −1) entries (constructed in the
previous step) and adding a new entry. The new entry is placed initially at
the root, but may have to move down with a down-heap bubbling to preserve
the heap-order property.
...
h + 1. In the last step (see Figure 8.10(f)–(g)), we form the final heap, storing all
the n entries, by joining two heaps storing (n− 1)/2 entries (constructed in
the previous step) and adding a new entry. The new entry is placed initially at
the root, but may have to move down with a down-heap bubbling to preserve
the heap-order property.
We illustrate bottom-up heap construction in Figure 8.10 for h = 3.
Page 378
354Chapter 8. Heaps and Priority Queues
(a)(b)
(c)(d)
(e)(f)
(g)
Figure 8.10: Bottom-up construction of a heap with 15 entries: (a) we begin by
constructing one-entry heaps on the bottom level; (b) and (c) we combine these
heaps into three-entry heaps; (d) and (e) seven-entry heaps; (f) and (g) we create
the final heap. The paths of the down-heap bubblings are highlighted in blue. For
simplicity, we only show the key within each node instead of the entire entry.
Page 379
8.3. Heaps355
Recursive Bottom-Up Heap Construction
We can also describe bottom-up heap construction as a recursive algorithm, as
shown in Code Fragment 8.18, which we call by passing a list storing the keys
for which we wish to build a heap.
Algorithm BottomUpHeap(L):
Input: An STL list L storing n = 2h+1 −1 entries
Output: A heap T storing the entries of L.
if L.empty() then
return an empty heap
e ← L.front()
L.pop front()
Split L into two lists, L1 and L2, each of size (n−1)/2
T1 ← BottomUpHeap(L1)
T2 ← BottomUpHeap(L2)
Create binary tree T with root r storing e, left subtree T1, and right subtree T2
Perform a down-heap bubbling from the root r of T, if necessary
return T
Code Fragment 8.18: Recursive bottom-up heap construction.
Although the algorithm has been expressed in terms of an STL list, the con
struction could have been performed equally well with a vector. In such a case, the
splitting of the vector is performed conceptually, by defining two ranges of indices,
one representing the front half L1 and the other representing the back half L2.
At first glance, it may seem that there is no substantial difference between this
algorithm and the incremental heap construction used in the heap-sort algorithm
of Section 8.3.5. One works by down-heap bubbling and the other uses up-heap
bubbling. It is somewhat surprising, therefore, that the bottom-up heap construction
is actually asymptotically faster than incrementally inserting n keys into an initially
empty heap. The following proposition shows this.
Proposition 8.7: Bottom-up construction of a heap with n entries takes O(n)
time, assuming two keys can be compared in O(1) time.
Justification: We analyze bottom-up heap construction using a “visual” ap
proach, which is illustrated in Figure 8.11.
Let T be the final heap, let v be a node of T, and let T(v) denote the subtree of
T rooted at v. In the worst case, the time for forming T(v) from the two recursively
formed subtrees rooted at v’s children is proportional to the height of T(v). The
worst case occurs when down-heap bubbling from v traverses a path from v all the
way to a bottommost node of T(v).
Page 380
356Chapter 8. Heaps and Priority Queues
Now consider the path p(v) of T from node v to its inorder successor external
node, that is, the path that starts at v, goes to the right child of v, and then goes down
leftward until it reaches an external node. We say that path p(v) is associated with
node v. Note that p(v) is not necessarily the path followed by down-heap bubbling
when forming T(v). Clearly, the size (number of nodes) of p(v) is equal to the
height of T(v) plus one. Hence, forming T(v) takes time proportional to the size of
p(v), in the worst case. Thus, the total running time of bottom-up heap construction
is proportional to the sum of the sizes of the paths associated with the nodes of T .
Observe that each node v of T belongs to at most two such paths: the path p(v)
associated with v itself and possibly also the path p(u) associated with the closest
ancestor u of v preceding v in an inorder traversal. (See Figure 8.11.) In particular,
the root r of T and the nodes on the leftmost root-to-leaf path each belong only to
one path, the one associated with the node itself. Therefore, the sum of the sizes
of the paths associated with the internal nodes of T is at most 2n−1. We conclude
that the bottom-up construction of heap T takes O(n) time.
Figure 8.11: Visual justification of the linear running time of bottom-up heap con
struction, where the paths associated with the internal nodes have been highlighted
with alternating colors. For example, the path associated with the root consists of
the nodes storing keys 4, 6, 7, and 11. Also, the path associated with the right child
of the root consists of the internal nodes storing keys 6, 20, and 23.
To summarize, Proposition 8.7 states that the running time for the first phase
of heap-sort can be reduced to be O(n). Unfortunately, the running time of the
second phase of heap-sort cannot be made asymptotically better than O(nlogn)
(that is, it will always be Ω(nlog n) in the worst case). We do not justify this lower
bound until Chapter 11, however. Instead, we conclude this chapter by discussing
a design pattern that allows us to extend the priority queue ADT to have additional
functionality.
Page 381
8.4. Adaptable Priority Queues357
8.4 Adaptable Priority Queues
The functions of the priority queue ADT given in Section 8.1.3 are sufficient for
most basic applications of priority queues such as sorting. However, there are situ
ations where additional functions would be useful as shown in the scenarios below
that refer to the standby airline passenger application.
• A standby passenger with a pessimistic attitude may become tired of waiting
and decide to leave ahead of the boarding time, requesting to be removed
from the waiting list. Thus, we would like to remove the entry associated
with this passenger from the priority queue. Operation removeMin is not
suitable for this purpose, since it only removes the entry with the lowest
priority. Instead, we want a new operation that removes an arbitrary entry.
• Another standby passenger finds her gold frequent-flyer card and shows it to
the agent. Thus, her priority has to be modified accordingly. To achieve this
change of priority, we would like to have a new operation that changes the
information associated with a given entry. This might affect the entry’s key
value (such as frequent-flyer status) or not (such as correcting a misspelled
name).
Functions of the Adaptable Priority Queue ADT
The above scenarios motivate the definition of a new ADT for priority queues,
which includes functions for modifying or removing specified entries. In order to
do this, we need some way of indicating which entry of the queue is to be affected
by the operation. Note that we cannot use the entry’s key value, because keys are
not distinct. Instead, we assume that the priority queue operation insert(e) is aug
mented so that, after inserting the element e, it returns a reference to the newly
created entry, called a position (recall Section 6.2.1). This position is permanently
attached to the entry, so that, even if the location of the entry changes within the
priority queue’s internal data structure (as is done when performing bubbling oper
ations in a heap), the position remains fixed to this entry. Thus, positions provide
us with a means to uniquely specify the entry to which each operation is applied.
We formally define an adaptable priority queue P to be a priority queue that, in
addition to the standard priority queue operations, supports the following enhance
ments.
insert(e): Insert the element e into P and return a position referring
to its entry.
remove(p): Remove the entry referenced by p from P.
replace(p,e): Replace with e the element associated with the entry ref
erenced by p and return the position of the altered entry.
Page 382
358Chapter 8. Heaps and Priority Queues
8.4.1 A List-Based Implementation
In this section, we present a simple implementation of an adaptable priority queue,
called AdaptPriorityQueue. Our implementation is a generalization of the sorted
list priority queue implementation given in Section 8.2.
In Code Fragment 8.7, we present the class definition, with the exception of the
class Position, which is presented later. The public part of the class is essentially
the same as the standard priority queue interface, which was presented in Code
Fragment 8.4, together with the new functions remove and replace. Note that the
function insert now returns a position.
template <typename E, typename C>
class AdaptPriorityQueue {// adaptable priority queue
protected:
typedef std::list<E> ElementList;// list of elements
public:
// . . .insert Position class definition here
public:
int size() const;// number of elements
bool empty() const;// is the queue empty?
const E& min() const;// minimum element
Position insert(const E& e);// insert element
void removeMin();// remove minimum
void remove(const Position& p);// remove at position p
Position replace(const Position& p, const E& e); // replace at position p
private:
ElementList L;// priority queue contents
C isLess;// less-than comparator
};
Code Fragment 8.19: The class definition for an adaptable priority queue.
We next define the class Position, which is nested within the public part of
class AdaptPriorityQueue. Its data member is an iterator to the STL list. This list
contains the contents of the priority queue. The main public member is a function
that returns a “const” reference the underlying element, which is implemented by
overloading the “*” operator. This is presented in Code Fragment 8.20.
class Position {// a position in the queue
private:
typename ElementList::iterator q;// a position in the list
public:
const E& operator*() { return *q; } // the element at this position
friend class AdaptPriorityQueue;// grant access
};
Code Fragment 8.20: The class representing a position in AdaptPriorityQueue.
Page 383
8.4. Adaptable Priority Queues359
The operation insert is presented in Code Fragment 8.21. It is essentially the
same as presented in the standard list priority queue (see Code Fragment 8.9). Since
it is declared outside the class, we need to provide the complete template specifi
cations for the function. We search for the first entry p whose key value exceeds
ours, and insert e just prior to this entry. We then create a position that refers to the
entry just prior to p and return it.
template <typename E, typename C>// insert element
typename AdaptPriorityQueue<E,C>::Position
AdaptPriorityQueue<E,C>::insert(const E& e) {
typename ElementList::iterator p = L.begin();
while (p != L.end() && !isLess(e, *p)) ++p;// find larger element
L.insert(p, e);// insert before p
Position pos; pos.q = −−p;
return pos;// inserted position
}
Code Fragment 8.21: The function insert for class AdaptPriorityQueue.
We omit the definitions of the member functions size, empty, min, and remove
Min, since they are the same as in the standard list-based priority queue implemen
tation (see Code Fragments 8.8 and 8.10). Next, in Code Fragment 8.22, we present
the implementations of the functions remove and replace. The function remove in
vokes the erase function of the STL list to remove the entry referred to by the given
position.
template <typename E, typename C>// remove at position p
void AdaptPriorityQueue<E,C>::remove(const Position& p)
{ L.erase(p.q); }
template <typename E, typename C>// replace at position p
typename AdaptPriorityQueue<E,C>::Position
AdaptPriorityQueue<E,C>::replace(const Position& p, const E& e) {
L.erase(p.q);// remove the old entry
return insert(e);// insert replacement
}
Code Fragment 8.22: The functions remove and replace for AdaptPriorityQueue.
We have chosen perhaps the simplest way to implement the function replace.
We remove the entry to be modified and simply insert the new element e into the
priority queue. In general, the key information may have changed, and therefore it
may need to be moved to a new location in the sorted list. Under the assumption
that key changes are rare, a more clever solution would involve searching forwards
or backwards to determine the proper position for the modified entry. While it may
not be very efficient, our approach has the virtue of simplicity.
Page 384
360Chapter 8. Heaps and Priority Queues
8.4.2 Location-Aware Entries
In our implementation of the adaptable priority queue, AdaptPriorityQueue, pre
sented in the previous section, we exploited a nice property of the list-based priority
queue implementation. In particular, once a new entry is added to the sorted list,
the element associated with this entry never changes. This means that the positions
returned by the insert and replace functions always refer to the same element.
Note, however, that this same approach would fail if we tried to apply it to
the heap-based priority queue of Section 8.3.3. The reason is that the heap-based
implementation moves the entries around the heap (for example, through up-heap
bubbling and down-heap bubbling). When an element e is inserted, we return a
reference to the entry p containing e. But if e were to be moved as a result of
subsequent operations applied to the priority queue, p does not change. As a result,
p might be pointing to a different element of the priority queue. An attempt to
apply remove(p) or replace(p,e′), would not be applied to e but instead to some
other element.
The solution to this problem involves decoupling positions and entries. In our
implementation of AdaptPriorityQueue, each position p is essentially a pointer to
a node of the underlying data structure (for this is how an STL iterator is imple
mented). If we move an entry, we need to also change the associated pointer. In
order to deal with moving entries, each time we insert a new element e in the prior
ity queue, in addition to creating a new entry in the data structure, we also allocate
memory for an object, called a locator. The locator’s job is to store the current
position p of element e in the data structure. Each entry of the priority queue needs
to know its associated locator l. Thus, rather than just storing the element itself in
the priority queue, we store a pair (e,&l), consisting of the element e and a pointer
to its locator. We call this a locator-aware entry. After inserting a new element in
the priority queue, we return the associated locator object, which points to this pair.
How does this solve the decoupling problem? First, observe that whenever
the user of the priority queue wants to locate the position p of a previously inserted
element, it suffices to access the locator that stores this position. Suppose, however,
that the entry moves to a different position p′ within the data structure. To handle
this, we first access the location-aware entry (e,&l) to access the locator l. We then
modify l so that it refers to the new position p′. The user may find the new position
by accessing the locator.
The price we pay for this extra generality is fairly small. For each entry, we
need to two store two additional pointers (the locator and the locator’s address).
Each time we move an object in the data structure, we need to modify a constant
number of pointers. Therefore, the running time increases by just a constant factor.
Page 385
8.5. Exercises361
8.5 Exercises
For help with exercises, please visit the web site, www.wiley.com/college/goodrich.
Reinforcement
R-8.1 What are the running times of each of the functions of the (standard) prior
ity queue ADT if we implement it by adapting the STL priority queue?
R-8.2 How long would it take to remove the ⌈logn⌉ smallest elements from a
heap that contains n entries using the removeMin() operation?
R-8.3 Show that, given only the less-than operator (<) and the boolean operators
and (&&), or (||), and not (!), it is possible to implement all of the other
comparators: >, <=, >=, ==, !=.
R-8.4 Explain how to implement a priority queue based on the composition
method (of storing key-element pairs) by adapting a priority queue based
on the comparator approach.
R-8.5 Suppose you label each node v of a binary tree T with a key equal to the
preorder rank of v. Under what circumstances is T a heap?
R-8.6 Show the output from the following sequence of priority queue ADT op
erations. The entries are key-element pairs, where sorting is based on the
key value: insert(5,a), insert(4,b), insert(7,i), insert(1,d), removeMin(),
insert(3, j), insert(6,c), removeMin(), removeMin(), insert(8,g), remove
Min(), insert(2,h), removeMin(), removeMin().
R-8.7 An airport is developing a computer simulation of air-traffic control that
handles events such as landings and takeoffs. Each event has a time-stamp
that denotes the time when the event occurs. The simulation program
needs to efficiently perform the following two fundamental operations:
• Insert an event with a given time-stamp (that is, add a future event)
• Extract the event with smallest time-stamp (that is, determine the
next event to process)
Which data structure should be used for the above operations? Why?
R-8.8 Although it is correct to use a “reverse” comparator with our priority
queue ADT so that we retrieve and remove an element with the maxi
mum key each time, it is confusing to have an element with the maximum
key returned by a function named “removeMin.” Write a short adapter
class that can take any priority queue P and an associated comparator C
and implement a priority queue that concentrates on the element with the
maximum key, using functions with names like removeMax.
(Hint: Define a new comparator C′ in terms of C.)
Page 386
362Chapter 8. Heaps and Priority Queues
R-8.9 Illustrate the performance of the selection-sort algorithm on the following
input sequence: (22,15,36,44,10,3,9,13,29,25).
R-8.10 Illustrate the performance of the insertion-sort algorithm on the input se
quence of the previous problem.
R-8.11 Give an example of a worst-case sequence with n elements for insertion
sort, and show that insertion-sort runs in Ω(n2) time on such a sequence.
R-8.12 At which nodes of a heap can an entry with the largest key be stored?
R-8.13 In defining the relation “to the left of” for two nodes of a binary tree (Sec
tion 8.3.1), can we use a preorder traversal instead of an inorder traversal?
How about a postorder traversal?
R-8.14 Illustrate the performance of the heap-sort algorithm on the following in
put sequence: (2,5,16,4,10,23,39,18,26,15).
R-8.15 Let T be a complete binary tree such that node v stores the key-entry pairs
( f (v),0), where f (v) is the level number of v. Is tree T a heap? Why or
why not?
R-8.16 Explain why the case where the right child of r is internal and the left child
is external was not considered in the description of down-heap bubbling.
R-8.17 Is there a heap T storing seven distinct elements such that a preorder
traversal of T yields the elements of T in sorted order? How about an
inorder traversal? How about a postorder traversal?
R-8.18 Consider the numbering of the nodes of a binary tree defined in Sec
tion 7.3.5, and show that the insertion position in a heap with n keys is
the node with number n + 1.
R-8.19 Let H be a heap storing 15 entries using the vector representation of a
complete binary tree. What is the sequence of indices of the vector that
are visited in a preorder traversal of H? What about an inorder traversal
of H? What about a postorder traversal of H?
R-8.20 Show that the sum ∑n i=1 logi, which appears in the analysis of heap-sort,
is Ω(nlog n).
R-8.21 Bill claims that a preorder traversal of a heap will list its keys in nonde
creasing order. Draw an example of a heap that proves him wrong.
R-8.22 Hillary claims that a postorder traversal of a heap will list its keys in non
increasing order. Draw an example of a heap that proves her wrong.
R-8.23 Show all the steps of the algorithm for removing key 16 from the heap of
Figure 8.3.
R-8.24 Draw an example of a heap whose keys are all the odd numbers from 1 to
59 (with no repeats), such that the insertion of an entry with key 32 would
cause up-heap bubbling to proceed all the way up to a child of the root
(replacing that child’s key with 32).
Page 387
8.5. Exercises363
R-8.25 Give a pseudo-code description of a nonrecursive in-place heap-sort algo
rithm.
R-8.26 A group of children want to play a game, called Unmonopoly, where in
each turn the player with the most money must give half of his/her money
to the player with the least amount of money. What data structure(s)
should be used to play this game efficiently? Why?
Creativity
C-8.1 An online computer system for trading stock needs to process orders of
the form “buy 100 shares at $x each” or “sell 100 shares at $y each.” A
buy order for $x can only be processed if there is an existing sell order
with price $y such that y ≤ x. Likewise, a sell order for $y can only be
processed if there is an existing buy order with price $x such that x ≥ y.
If a buy or sell order is entered but cannot be processed, it must wait
for a future order that allows it to be processed. Describe a scheme that
allows for buy and sell orders to be entered in O(logn) time, independent
of whether or not they can be immediately processed.
C-8.2 Extend a solution to the previous problem so that users are allowed to
update the prices for their buy or sell orders that have yet to be processed.
C-8.3 Write a comparator for integer objects that determines order based on the
number of 1s in each number’s binary expansion, so that i < j if the num
ber of 1s in the binary representation of i is less than the number of 1s in
the binary representation of j.
C-8.4 Show how to implement the stack ADT using only a priority queue and
one additional member variable.
C-8.5 Show how to implement the (standard) queue ADT using only a priority
queue and one additional member variable.
C-8.6 Describe, in detail, an implementation of a priority queue based on a
sorted array. Show that this implementation achieves O(1) time for op
erations min and removeMin and O(n) time for operation insert.
C-8.7 Describe an in-place version of the selection-sort algorithm that uses only
O(1) space for member variables in addition to an input array itself.
C-8.8 Assuming the input to the sorting problem is given in an array A, describe
how to implement the insertion-sort algorithm using only the array A and,
at most, six additional (base-type) variables.
C-8.9 Assuming the input to the sorting problem is given in an array A, describe
how to implement the heap-sort algorithm using only the array A and, at
most, six additional (base-type) variables.
Page 388
364Chapter 8. Heaps and Priority Queues
C-8.10 Describe a sequence of n insertions to a heap that requires Ω(nlog n) time
to process.
C-8.11 An alternative method for finding the last node during an insertion in a
heap T is to store, in the last node and each external node of T , a pointer
to the external node immediately to its right (wrapping to the first node
in the next lower level for the rightmost external node). Show how to
maintain such a pointer in O(1) time per operation of the priority queue
ADT, assuming T is implemented as a linked structure.
C-8.12 We can represent a path from the root to a given node of a binary tree by
means of a binary string, where 0 means “go to the left child” and 1 means
“go to the right child.” For example, the path from the root to the node
storing 8 in the heap of Figure 8.3 is represented by the binary string 101.
Design an O(logn)-time algorithm for finding the last node of a complete
binary tree with n nodes based on the above representation. Show how
this algorithm can be used in the implementation of a complete binary
tree by means of a linked structure that does not keep a reference to the
last node.
C-8.13 Suppose the binary tree T used to implement a heap can be accessed using
only the functions of the binary tree ADT. That is, we cannot assume T
is implemented as a vector. Given a pointer to the current last node, v,
describe an efficient algorithm for finding the insertion point (that is, the
new last node) using just the functions of the binary tree interface. Be
sure and handle all possible cases as illustrated in Figure 8.12. What is
the running time of this function?
(a)(b)
Figure 8.12: Updating the last node in a complete binary tree after operation add or
remove. Node w is the last node before operation add or after operation remove.
Node z is the last node after operation add or before operation remove.
C-8.14 Given a heap T and a key k, give an algorithm to compute all the entries in
T with a key less than or equal to k. For example, given the heap of Fig
ure 8.12(a) and query k = 7, the algorithm should report the entries with
keys 2, 4, 5, 6, and 7 (but not necessarily in this order). Your algorithm
should run in time proportional to the number of entries returned.
Page 389
8.5. Exercises365
C-8.15 Show that, for any n, there is a sequence of insertions in a heap that re
quires Ω(nlog n) time to process.
C-8.16 Provide a justification of the time bounds in Table 8.1.
C-8.17 Develop an algorithm that computes the kth smallest element of a set of n
distinct integers in O(n + k logn) time.
C-8.18 Suppose the internal nodes of two binary trees, T1 and T2 respectively,
hold items that satisfy the heap-order property. Describe a method for
combining these two trees into a tree T , whose internal nodes hold the
union of the items in T1 and T2 and also satisfy the heap-order property.
Your algorithms should run in time O(h1 + h2) where h1 and h2 are the
respective heights of T1 and T2.
C-8.19 Give an alternative analysis of bottom-up heap construction by showing
that, for any positive integer h, ∑h i=1(i/2i) is O(1).
C-8.20 Let T be a heap storing n keys. Give an efficient algorithm for reporting
all the keys in T that are smaller than or equal to a given query key x
(which is not necessarily in T ). For example, given the heap of Figure 8.3
and query key x = 7, the algorithm should report 4, 5, 6, 7. Note that the
keys do not need to be reported in sorted order. Ideally, your algorithm
should run in O(k) time, where k is the number of keys reported.
C-8.21 Give an alternate description of the in-place heap-sort algorithm that uses
a standard comparator instead of a reverse one.
C-8.22 Describe efficient algorithms for performing operations remove(e) on an
adaptable priority queue realized by means of an unsorted list with location
aware entries.
C-8.23 Let S be a set of n points in the plane with distinct integer x- and y
coordinates. Let T be a complete binary tree storing the points from S
at its external nodes, such that the points are ordered left-to-right by in
creasing x-coordinates. For each node v in T , let S(v) denote the subset of
S consisting of points stored in the subtree rooted at v. For the root r of
T , define top(r) to be the point in S = S(r) with maximum y-coordinate.
For every other node v, define top(r) to be the point in S with highest y
coordinate in S(v) that is not also the highest y-coordinate in S(u), where
u is the parent of v in T (if such a point exists). Such labeling turns T into
a priority search tree. Describe a linear-time algorithm for turning T into
a priority search tree.
Projects
P-8.1 Generalize the Heap data structure of Section 8.3 from a binary tree to
a k-ary tree, for an arbitrary k ≥ 2. Study the relative efficiencies of the
Page 390
366Chapter 8. Heaps and Priority Queues
resulting data structure for various values of k, by inserting and removing
a large number of randomly generated keys into each data structure.
P-8.2 Give a C++ implementation of a priority queue based on an unsorted list.
P-8.3 Develop a C++ implementation of a priority queue that is based on a heap
and supports the locator-based functions.
P-8.4 Implement the in-place heap-sort algorithm. Compare its running time
with that of the standard heap-sort that uses an external heap.
P-8.5 Implement a heap-based priority queue that supports the following addi
tional operation in linear time:
replaceComparator(c): Replace the current comparator with c.
After changing the comparator, the heap will need to be restructured.
(Hint: Utilize the bottom-up heap construction algorithm.)
P-8.6 Write a program that can process a sequence of stock buy and sell orders
as described in Exercise C-8.1.
P-8.7 One of the main applications of priority queues is in operating systems—
for scheduling jobs on a CPU. In this project you are to build a program
that schedules simulated CPU jobs. Your program should run in a loop,
each iteration of which corresponds to a time slice for the CPU. Each job
is assigned a priority, which is an integer between −20 (highest priority)
and 19 (lowest priority), inclusive. From among all jobs waiting to be
processed in a time slice, the CPU must work on the job with highest
priority. In this simulation, each job will also come with a length value,
which is an integer between 1 and 100, inclusive, indicating the number
of time slices that are needed to process this job. For simplicity, you may
assume jobs cannot be interrupted—once it is scheduled on the CPU, a job
runs for a number of time slices equal to its length. Your simulator must
output the name of the job running on the CPU in each time slice and must
process a sequence of commands, one per time slice, each of which is of
the form “add job name with length n and priority p” or “no new job this
slice.”
Chapter Notes
Knuth’s book on sorting and searching [57] describes the motivation and history for the
selection-sort, insertion-sort, and heap-sort algorithms. The heap-sort algorithm is due
to Williams [103], and the linear-time heap construction algorithm is due to Floyd [33].
Additional algorithms and analyses for heaps and heap-sort variations can be found in
papers by Bentley [12], Carlsson [20], Gonnet and Munro [38], McDiarmid and Reed [70],
and Schaffer and Sedgewick [88]. The design pattern of using location-aware entries (also
described in [39]), appears to be new.
Page 391
Chapter
9 Hash Tables, Maps, and Skip Lists
Contents
9.1 Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
9.1.1 The Map ADT . . . . . . . . . . . . . . . . . . . . 369
9.1.2 A C++ Map Interface . . . . . . . . . . . . . . . . . 371
9.1.3 The STL map Class . . . . . . . . . . . . . . . . . . 372
9.1.4 A Simple List-Based Map Implementation . . . . . . 374
9.2 Hash Tables . . . . . . . . . . . . . . . . . . . . . . . 375
9.2.1 Bucket Arrays . . . . . . . . . . . . . . . . . . . . . 375
9.2.2 Hash Functions . . . . . . . . . . . . . . . . . . . . 376
9.2.3 Hash Codes . . . . . . . . . . . . . . . . . . . . . . 376
9.2.4 Compression Functions . . . . . . . . . . . . . . . . 380
9.2.5 Collision-Handling Schemes . . . . . . . . . . . . . . 382
9.2.6 Load Factors and Rehashing . . . . . . . . . . . . . 386
9.2.7 A C++ Hash Table Implementation . . . . . . . . . . 387
9.3 Ordered Maps . . . . . . . . . . . . . . . . . . . . . . 394
9.3.1 Ordered Search Tables and Binary Search . . . . . . 395
9.3.2 Two Applications of Ordered Maps . . . . . . . . . . 399
9.4 Skip Lists . . . . . . . . . . . . . . . . . . . . . . . . . 402
9.4.1 Search and Update Operations in a Skip List . . . . 404
9.4.2 A Probabilistic Analysis of Skip Lists ⋆ . . . . . . . 408
9.5 Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . 411
9.5.1 The Dictionary ADT . . . . . . . . . . . . . . . . . 411
9.5.2 A C++ Dictionary Implementation . . . . . . . . . . 413
9.5.3 Implementations with Location-Aware Entries . . . . 415
9.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 417
Page 392
368Chapter 9. Hash Tables, Maps, and Skip Lists
9.1 Maps
key
value
Map
entry
Figure 9.1: A conceptual illustration of the map ADT. Keys (labels) are assigned to
values (folders) by a user. The resulting entries (labeled folders) are inserted into
the map (file cabinet). The keys can be used later to retrieve or remove values.
A map allows us to store elements so they can be located quickly using keys.
The motivation for such searches is that each element typically stores additional
useful information besides its search key, but the only way to get at that information
is to use the search key. Specifically, a map stores key-value pairs (k,v), which we
call entries, where k is the key and v is its corresponding value. In addition, the map
ADT requires that each key be unique, so the association of keys to values defines a
mapping. In order to achieve the highest level of generality, we allow both the keys
and the values stored in a map to be of any object type. (See Figure 9.1.) In a map
storing student records (such as the student’s name, address, and course grades),
the key might be the student’s ID number. In some applications, the key and the
value may be the same. For example, if we had a map storing prime numbers, we
could use each number itself as both a key and its value.
In either case, we use a key as a unique identifier that is assigned by an appli
cation or user to an associated value object. Thus, a map is most appropriate in
situations where each key is to be viewed as a kind of unique index address for its
value, that is, an object that serves as a kind of location for that value. For exam
ple, if we wish to store student records, we would probably want to use student ID
objects as keys (and disallow two students having the same student ID). In other
words, the key associated with an object can be viewed as an “address” for that
object. Indeed, maps are sometimes referred to as associative stores or associative
containers, because the key associated with an object determines its “location” in
the data structure.
Page 393
9.1. Maps369
Entries and the Composition Pattern
As mentioned above, a map stores key-value pairs, called entries. An entry is actu
ally an example of a more general object-oriented design pattern, the composition
pattern, which defines a single object that is composed of other objects. A pair is
the simplest composition, because it combines two objects into a single pair object.
To implement this concept, we define a class that stores two objects in its first
and second member variables, respectively, and provides functions to access and
update these variables. In Code Fragment 9.1, we present such an implementation
storing a single key-value pair. We define a class Entry, which is templated based
on the key and value types. In addition to a constructor, it provides member func
tions that return references to the key and value. It also provides functions that
allow us to set the key and value members.
template <typename K, typename V>
class Entry {// a (key, value) pair
public:// public functions
Entry(const K& k = K(), const V& v = V()) // constructor
: key(k), value(v) { }
const K& key() const { return key; }// get key
const V& value() const { return value; }// get value
void setKey(const K& k) { key = k; }// set key
void setValue(const V& v) { value = v; }// set value
private:// private data
K key;// key
V value;// value
};
Code Fragment 9.1: A C++ class for an entry storing a key-value pair.
9.1.1 The Map ADT
In this section, we describe a map ADT. Recall that a map is a collection of key
value entries, with each value associated with a distinct key. We assume that a map
provides a special pointer object, which permits us to reference entries of the map.
Such an object would normally be called a position. As we did in Chapter 6, in
order to be more consistent with the C++ Standard Template Library, we define a
somewhat more general object called an iterator, which can both reference entries
and navigate around the map. Given a map iterator p, the associated entry may be
accessed by dereferencing the iterator, namely as *p. The individual key and value
can be accessed using p->key() and p->value(), respectively.
In order to advance an iterator from its current position to the next, we overload
the increment operator. Thus, ++p advances the iterator p to the next entry of the
Page 394
370Chapter 9. Hash Tables, Maps, and Skip Lists
map. We can enumerate all the entries of a map M by initializing p to M.begin()
and then repeatedly incrementing p as long as it is not equal to M.end().
In order to indicate that an object is not present in the map, we assume that
there exists a special sentinel iterator called end. By convention, this sentinel refers
to an imaginary element that lies just beyond the last element of the map.
The map ADT consists of the following:
size(): Return the number of entries in M.
empty(): Return true if M is empty and false otherwise.
find(k): If M contains an entry e = (k,v), with key equal to k, then
return an iterator p referring to this entry, and otherwise
return the special iterator end.
put(k,v): If M does not have an entry with key equal to k, then
add entry (k,v) to M, and otherwise, replace the value
field of this entry with v; return an iterator to the in
serted/modified entry.
erase(k): Remove from M the entry with key equal to k; an error
condition occurs if M has no such entry.
erase(p): Remove from M the entry referenced by iterator p; an
error condition occurs if p points to the end sentinel.
begin(): Return an iterator to the first entry of M.
end(): Return an iterator to a position just beyond the end of M.
We have provided two means of removing entries, one given a key and the other
given an iterator. The key-based operation should be used only when it is known
that the key is present in the map. Otherwise, it is necessary to first check that the
key exists using the operation “p = M.find(k),” and if so, then apply the operation
M.erase(p). The iterator-based removal operation has the advantage that it does
not need to repeat the search for the key, and hence is more efficient.
The operation put, may either insert an entry or modify an existing entry. It is
designed explicitly in this way, since we require that the keys be unique. Later, in
Section 9.5, we consider a different data structure, which allows multiple instances
to have the same keys. Note that an iterator remains associated with an entry, even
if its value is changed.
Example 9.1: In the following, we show the effect of a series of operations on an
initially empty map storing entries with integer keys and single-character values.
In the column “Output,” we use the notation pi : [(k,v)] to mean that the operation
returns an iterator denoted by pi that refers to the entry (k,v). The entries of the
map are not listed in any particular order.
Page 395
9.1. Maps371
Operation Output Map
empty()true∅
put(5,A) p1 : [(5,A)] {(5,A)}
put(7,B) p2 : [(7,B)] {(5,A),(7,B)}
put(2,C) p3 : [(2,C)] {(5,A),(7,B),(2,C)}
put(2,E) p3 : [(2,E)] {(5,A),(7,B),(2,E)}
find(7)p2 : [(7,B)] {(5,A),(7,B),(2,E)}
find(4)end{(5,A),(7,B),(2,E)}
find(2)p3 : [(2,E)] {(5,A),(7,B),(2,E)}
size()3{(5,A),(7,B),(2,E)}
erase(5)–{(7,B),(2,E)}
erase(p3)–{(7,B)}
find(2)end{(7,B)}
9.1.2 A C++ Map Interface
Before discussing specific implementations of the map ADT, we first define a C++
interface for a map in Code Fragment 9.2. It is not a complete C++ class, just a
declaration of the public functions. The interface is templated by two types, the
key type K, and the value type V.
template <typename K, typename V>
class Map {// map interface
public:
class Entry;// a (key,value) pair
class Iterator;// an iterator (and position)
int size() const;// number of entries in the map
bool empty() const;// is the map empty?
Iterator find(const K& k) const;// find entry with key k
Iterator put(const K& k, const V& v);// insert/replace pair (k,v)
void erase(const K& k)// remove entry with key k
throw(NonexistentElement);
void erase(const Iterator& p);// erase entry at p
Iterator begin();// iterator to first entry
Iterator end();// iterator to end entry
};
Code Fragment 9.2: An informal C++ Map interface (not a complete class).
In addition to its member functions, the interface defines two types, Entry and
Iterator. These two classes provide the types for the entry and iterator objects,
respectively. Outside the class, these would be accessed with Map<K,V>::Entry
and Map<K,V>::Iterator, respectively.
Page 396
372Chapter 9. Hash Tables, Maps, and Skip Lists
We have not presented an interface for the iterator object, but its definition is
similar to the STL iterator. It supports the operator “*”, which returns a reference
to the associated entry. The unary increment and decrement operators “++” and
“– –” move an iterator forward and backwards, respectively. Also, two iterators can
be compared for equality using “==”.
A more sophisticated implementation would have also provided for a third type,
namely a “const” iterator. Such an iterator provides a function for reading entries
without modifying them. (Recall Section 6.1.4.) We omit this type in order to keep
our interface relatively simple.
The remainder of the interface follows from our earlier descriptions of the map
operations. An error condition occurs if the function erase(k) is called with a key
k that is not in the map. This is signaled by throwing an exception of type Nonex
istentElement. Its definition is similar to other exceptions that we have seen. (See
Code Fragment 5.2.)
9.1.3 The STL map Class
The C++ Standard Template Library (STL) provides an implementation of a map
simply called map. As with many of the other STL classes we have seen, the STL
map is an example of a container, and hence supports access by iterators.
In order to declare an object of type map, it is necessary to first include the
definition file called “map.” The map is part of the std namespace, and hence it is
necessary either to use “std::map” or to provide an appropriate “using” statement.
The STL map is templated with two arguments, the key type and the value type.
The declaration “map<K,V>” defines a map whose keys are of type K and whose
values are of type V. As with the other STL containers, an iterator type is provided
both for referencing individual entries and enumerating multiple entries. The map
iterator type is “map<K,E>::iterator.” The (k,v) entries are stored in a composite
object called pair. Given an iterator p, its associated key and value members can
be referenced using p->first and p->second, respectively. (These are equivalent to
p->key() and p->value() in our map ADT, but note that there are no parentheses
following first and second.)
As with other iterators we have seen, each map object M defines two special
iterators through the functions begin and end, where M.begin() yields an iterator
to the first element of the map, and M.end() yields an iterator to an imaginary
element just beyond the end of the map. A map iterator p is bidirectional, meaning
that we can move forwards and backwards through the map using the increment
and decrement operators, ++p and – –p, respectively.
The principal member functions of the STL map are given below. Let M be
declared to be an STL map, let k be a key object, and let v be a value object for the
class M. Let p be an iterator for M.
Page 397
9.1. Maps373
size(): Return the number of elements in the map.
empty(): Return true if the map is empty and false otherwise.
find(k): Find the entry with key k and return an iterator to it; if no
such key exists return end.
operator[k]: Produce a reference to the value of key k; if no such key
exists, create a new entry for key k.
insert(pair(k,v)): Insert pair (k,v), returning an iterator to its position.
erase(k): Remove the element with key k.
erase(p): Remove the element referenced by iterator p.
begin(): Return an iterator to the beginning of the map.
end(): Return an iterator just past the end of the map.
Our map ADT is quite similar to the above functions. The insert function is a
bit different. In our ADT, it is given two arguments. In the STL map, the argument
is a composite object of type pair, whose first and second elements are the key and
value, respectively.
The STL map provides a very convenient way to search, insert, and modify
entries by overloading the subscript operator (“[ ]”). Given a map M, the assignment
“M[k] = v” inserts the pair (k,v) if k is not already present, or modifies the value
if it is. Thus, the subscript assignment behaves essentially the same as our ADT
function put(k,v). Reading the value of M[k] is equivalent to performing find(k)
and accessing the value part of the resulting iterator. An example of the use of the
STL map is shown in Code Fragment 9.3.
map<string, int> myMap;// a (string,int) map
map<string, int>::iterator p;// an iterator to the map
myMap.insert(pair<string, int>("Rob", 28));// insert (“Rob”,28)
myMap["Joe"] = 38;// insert(“Joe”,38)
myMap["Joe"] = 50;// change to (“Joe”,50)
myMap["Sue"] = 75;// insert(“Sue”,75)
p = myMap.find("Joe");// *p = (“Joe”,50)
myMap.erase(p);// remove (“Joe”,50)
myMap.erase("Sue");// remove (“Sue”,75)
p = myMap.find("Joe");
if (p == myMap.end()) cout << "nonexistent\n"; // outputs: “nonexistent”
for (p = myMap.begin(); p != myMap.end(); ++p) { // print all entries
cout << "(" << p−>first << "," << p−>second << ")\n";
}
Code Fragment 9.3: Example of the usage of STL map.
As with the other STL containers we have seen, the STL does not check for
errors. It is up to the programmer to be sure that no illegal operations are performed.
Page 398
374Chapter 9. Hash Tables, Maps, and Skip Lists
9.1.4 A Simple List-Based Map Implementation
A simple way of implementing a map is to store its n entries in a list L, imple
mented as a doubly linked list. Performing the fundamental functions, find(k),
put(k,v), and erase(k), involves simple scans down L looking for an entry with
key k. Pseudo-code is presented in Code Fragments 9.4. We use the notation
[L.begin(),L.end()) to denote all the positions of list L, from L.begin() and up to,
but not including, L.end().
Algorithm find(k):
Input: A key k
Output: The position of the matching entry of L, or end if there is no key k in L
for each position p ∈ [L.begin(),L.end()) do
if p.key() = k then
return p
return end{there is no entry with key equal to k}
Algorithm put(k,v):
Input: A key-value pair (k,v)
Output: The position of the inserted/modified entry
for each position p ∈ [L.begin(),L.end()) do
if p.key() = k then
*p ← (k,v)
return p{return the position of the modified entry}
p ← L.insertBack((k,v))
n ← n + 1{increment variable storing number of entries}
return p{return the position of the inserted entry}
Algorithm erase(k):
Input: A key k
Output: None
for each position p ∈ [L.begin(),L.end()) do
if p.key() = k then
L.erase(p)
n ← n − 1{decrement variable storing number of entries}
Code Fragment 9.4: Algorithms for find, put, and erase for a map stored in a list L.
This list-based map implementation is simple, but it is only efficient for very
small maps. Every one of the fundamental functions takes O(n) time on a map
with n entries, because each function involves searching through the entire list in
the worst case. Thus, we would like something much faster.
Page 399
9.2. Hash Tables375
9.2 Hash Tables
The keys associated with values in a map are typically thought of as “addresses” for
those values. Examples of such applications include a compiler’s symbol table and
a registry of environment variables. Both of these structures consist of a collection
of symbolic names where each name serves as the “address” for properties about
a variable’s type and value. One of the most efficient ways to implement a map in
such circumstances is to use a hash table. Although, as we will see, the worst-case
running time of map operations in an n-entry hash table is O(n). A hash table can
usually perform these operations in O(1) expected time. In general, a hash table
consists of two major components, a bucket array and a hash function.
9.2.1 Bucket Arrays
A bucket array for a hash table is an array A of size N, where each cell of A is
thought of as a “bucket” (that is, a collection of key-value pairs) and the integer
N defines the capacity of the array. If the keys are integers well distributed in the
range [0,N − 1], this bucket array is all that is needed. An entry e with key k is
simply inserted into the bucket A[k]. (See Figure 9.2.)
Figure 9.2: A bucket array of size 11 for the entries (1,D), (3,C), (3,F), (3,Z), (6,A),
(6,C), and (7,Q).
If our keys are unique integers in the range [0,N − 1], then each bucket holds
at most one entry. Thus, searches, insertions, and removals in the bucket array take
O(1) time. This sounds like a great achievement, but it has two drawbacks. First,
the space used is proportional to N. Thus, if N is much larger than the number of
entries n actually present in the map, we have a waste of space. The second draw
back is that keys are required to be integers in the range [0,N −1], which is often not
the case. Because of these two drawbacks, we use the bucket array in conjunction
with a “good” mapping from the keys to the integers in the range [0,N −1].
Page 400
376Chapter 9. Hash Tables, Maps, and Skip Lists
9.2.2 Hash Functions
The second part of a hash table structure is a function, h, called a hash function,
that maps each key k in our map to an integer in the range [0,N − 1], where N is
the capacity of the bucket array for this table. Equipped with such a hash function,
h, we can apply the bucket array method to arbitrary keys. The main idea of this
approach is to use the hash function value, h(k), as an index into our bucket array,
A, instead of the key k (which is most likely inappropriate for use as a bucket array
index). That is, we store the entry (k,v) in the bucket A[h(k)].
Of course, if there are two or more keys with the same hash value, then two
different entries will be mapped to the same bucket in A. In this case, we say
that a collision has occurred. Clearly, if each bucket of A can store only a single
entry, then we cannot associate more than one entry with a single bucket, which
is a problem in the case of collisions. To be sure, there are ways of dealing with
collisions, which we discuss later, but the best strategy is to try to avoid them in
the first place. We say that a hash function is “good” if it maps the keys in our map
in such a way as to minimize collisions as much as possible. For practical reasons,
we also would like a hash function to be fast and easy to compute.
We view the evaluation of a hash function, h(k), as consisting of two actions—
mapping the key k to an integer, called the hash code, and mapping the hash code
to an integer within the range of indices ([0,N − 1]) of a bucket array, called the
compression function. (See Figure 9.3.)
Figure 9.3: The two parts of a hash function: hash code and compression function.
9.2.3 Hash Codes
The first action that a hash function performs is to take an arbitrary key k in our map
and assign it an integer value. The integer assigned to a key k is called the hash
code for k. This integer value need not be in the range [0,N −1], and may even be
Page 401
9.2. Hash Tables377
negative, but we want the set of hash codes assigned to our keys to avoid collisions
as much as possible. If the hash codes of our keys cause collisions, then there is no
hope for our compression function to avoid them. In addition, to be consistent with
all of our keys, the hash code we use for a key k should be the same as the hash
code for any key that is equal to k.
Hash Codes in C++
The hash codes described below are based on the assumption that the number of
bits of each type is known. This information is provided in the standard include file
<limits>. This include file defines a templated class numeric limits. Given a base
type T (such as char, int, or float), the number of bits in a variable of type T is
given by “numeric limits<T>.digits.” Let us consider several common data types
and some example functions for assigning hash codes to objects of these types.
Converting to an Integer
To begin, we note that, for any data type X that is represented using at most as
many bits as our integer hash codes, we can simply take an integer interpretation of
its bits as a hash code for X. Thus, for the C++ fundamental types char, short, and
int, we can achieve a good hash code simply by casting this type to int.
On many machines, the type long has a bit representation that is twice as long
as type int. One possible hash code for a long object is to simply cast it down to
an integer and then apply the integer hash code. The problem is that such a hash
code ignores half of the information present in the original value. If many of the
keys in our map only differ in these bits, they will collide using this simple hash
code. A better hash code, which takes all the original bits into consideration, sums
an integer representation of the high-order bits with an integer representation of the
low-order bits.
Indeed, the approach of summing components can be extended to any object x
whose binary representation can be viewed as a k-tuple (x0,x1,...,xk−1) of integers,
because we can then form a hash code for x as ∑k i=−01 xi. For example, given any
floating-point number, we can sum its mantissa and exponent as long integers, and
then apply a hash code for long integers to the result.
Polynomial Hash Codes
The summation hash code, described above, is not a good choice for character
strings or other variable-length objects that can be viewed as tuples of the form
(x0,x1,...,xk−1), where the order of the xi’s is significant. For example, consider
a hash code for a character string s that sums the ASCII values of the characters
Page 402
378Chapter 9. Hash Tables, Maps, and Skip Lists
in s. Unfortunately, this hash code produces lots of unwanted collisions for com
mon groups of strings. In particular, "temp01" and "temp10" collide using this
function, as do "stop", "tops", "pots", and "spot". A better hash code takes
into consideration the positions of the xi’s. An alternative hash code, which does
exactly this, chooses a nonzero constant, a ∕= 1, and uses
x0ak−1 +x1ak−2 +···+xk−2a+xk−1
as a hash code value. Mathematically speaking, this is simply a polynomial in a that
takes the components (x0,x1,...,xk−1) of an object x as its coefficients. This hash
code is therefore called a polynomial hash code. By Horner’s rule (see Exercise C-
4.16), this polynomial can be rewritten as
xk−1 +a(xk−2 +a(xk−3 +···+a(x2 +a(x1 +ax0))···)).
Intuitively, a polynomial hash code uses multiplication by the constant a as a
way of “making room” for each component in a tuple of values, while also preserv
ing a characterization of the previous components. Of course, on a typical com
puter, evaluating a polynomial is done using the finite bit representation for a hash
code; hence, the value periodically overflows the bits used for an integer. Since
we are more interested in a good spread of the object x with respect to other keys,
we simply ignore such overflows. Still, we should be mindful that such overflows
are occurring and choose the constant a so that it has some nonzero, low-order
bits, which serve to preserve some of the information content even if we are in an
overflow situation.
We have done some experimental studies that suggest that 33, 37, 39, and 41
are good choices for a when working with character strings that are English words.
In fact, in a list of over 50,000 English words formed as the union of the word lists
provided in two variants of Unix, we found that taking a to be 33, 37, 39, or 41
produced less than seven collisions in each case! Many implementations of string
hashing choose a polynomial hash function, using one of these constants for a, as
a default hash code. For the sake of speed, however, some implementations only
apply the polynomial hash function to a fraction of the characters in long strings.
Cyclic Shift Hash Codes
A variant of the polynomial hash code replaces multiplication by a with a cyclic
shift of a partial sum by a certain number of bits. Such a function, applied to
character strings in C++ could, for example, look like the following. We assume
a 32-bit integer word length, and we assume access to a function hashCode(x) for
integers. To achieve a 5-bit cyclic shift we form the “bitwise or” (see Section 1.2)
of a 5-bit left shift and a 27-bit right shift. As before, we use an unsigned integer
so that right shifts fill with zeros.
Page 403
9.2. Hash Tables379
int hashCode(const char* p, int len) {// hash a character array
unsigned int h = 0;
for (int i = 0; i < len; i++) {
h = (h << 5) | (h >> 27);// 5-bit cyclic shift
h += (unsigned int) p[i];// add in next character
}
return hashCode(int(h));
}
As with the traditional polynomial hash code, using the cyclic-shift hash code re
quires some fine-tuning. In this case, we must wisely choose the amount to shift by
for each new character.
Experimental Results
In Table 9.1, we show the results of some experiments run on a list of just over
25,000 English words, which compare the number of collisions for various shift
amounts.
CollisionsCollisions
Shift Total Max Shift Total Max
0 23739 869182
1 10517 2110 2773
2 2254611 4534
3 448312432
489213132
54214 1353
66215 10826
714216 87609
8 1052
Table 9.1: Comparison of collision behavior for the cyclic shift variant of the poly
nomial hash code as applied to a list of just over 25,000 English words. The “Total”
column records the total number of collisions and the “Max” column records the
maximum number of collisions for any one hash code. Note that, with a cyclic shift
of 0, this hash code reverts to the one that simply sums all the characters.
These and our previous experiments show that if we choose our constant a
or our shift value wisely, then either the polynomial hash code or its cyclic-shift
variant are suitable for any object that can be written as a tuple (x0,x1,...,xk−1),
Page 404
380Chapter 9. Hash Tables, Maps, and Skip Lists
where the order in tuples matters. In particular, note that using a shift of 5 or 6 is
particularly good for English words. Also, note how poorly a simple addition of
the values would be with no shifting (that is, for a shift of 0).
Hashing Floating-Point Quantities
On most machines, types int and float are both 32-bit quantities. Nonetheless, the
approach of casting a float variable to type int would not produce a good hash
function, since this would truncate the fractional part of the floating-point value.
For the purposes of hashing, we do not really care about the number’s value. It
is sufficient to treat the number as a sequence of bits. Assuming that a char is
stored as an 8-bit byte, we could interpret a 32-bit float as a four-element character
array, and a 64-bit double as an eight-element character array. C++ provides an
operation called a reinterpret cast, to cast between such unrelated types. This cast
treats quantities as a sequence of bits and makes no attempt to intelligently convert
the meaning of one quantity to another.
For example, we could design a hash function for a float by first reinterpreting
it as an array of characters and then applying the character-array hashCode function
defined above. We use the operator sizeof, which returns the number of bytes in a
type.
int hashCode(const float& x) {// hash a float
int len = sizeof(x);
const char* p = reinterpret cast<const char*>(&x);
return hashCode(p, len);
}
Reinterpret casts are generally not portable operations, since the result depends
on the particular machine’s encoding of types as a pattern of bits. In our case,
portability is not an issue since we are interested only in interpreting the floating
point value as a sequence of bits. The only property that we require is that float
variables with equal values must have the same bit sequence.
9.2.4 Compression Functions
The hash code for a key k is typically not suitable for immediate use with a bucket
array, because the range of possible hash codes for our keys typically exceeds the
range of legal indices of our bucket array A. That is, incorrectly using a hash code
as an index into our bucket array may result in an error condition, either because the
index is negative or it exceeds the capacity of A. Thus, once we have determined an
integer hash code for a key object k, there is still the issue of mapping that integer
Page 405
9.2. Hash Tables381
into the range [0,N − 1]. This compression step is the second action that a hash
function performs.
The Division Method
One simple compression function to use is
h(k) = |k| mod N,
which is called the division method. Additionally, if we take N to be a prime
number, then this hash function helps “spread out” the distribution of hashed values.
Indeed, if N is not prime, there is a higher likelihood that patterns in the distribution
of keys will be repeated in the distribution of hash codes, thereby causing collisions.
For example, if we hash the keys {200,205,210,215,220,... ,600} to a bucket
array of size 100 using the division method, then each hash code collides with
three others. But if this same set of keys is similarly hashed to a bucket array of
size 101, then there are no collisions. If a hash function is chosen well, it should
ensure that the probability of two different keys getting hashed to the same bucket
is 1/N. Choosing N to be a prime number is not always enough, however, because
if there is a repeated pattern of key values of the form iN + j for several different
i’s, then there are still collisions.
The MAD Method
A more sophisticated compression function, which helps eliminate repeated pat
terns in a set of integer keys is the multiply add and divide (or “MAD”) method. In
using this method, we define the compression function as
h(k) = |ak + b| mod N,
where N is a prime number, and a and b are nonnegative integers randomly chosen
at the time the compression function is determined, so that a mod N ∕= 0. This
compression function is chosen in order to eliminate repeated patterns in the set of
hash codes and to get us closer to having a “good” hash function, that is, one having
the probability that any two different keys collide is 1/N. This good behavior would
be the same as if these keys were “thrown” into A uniformly at random.
With a compression function such as this, that spreads n integers fairly evenly
in the range [0,N − 1], and a mapping of the keys in our map to integers, we have
an effective hash function. Together, such a hash function and a bucket array define
the key ingredients of the hash table implementation of the map ADT.
But before we can give the details of how to perform such operations as find,
insert, and erase, we must first resolve the issue of how we to handle collisions.
Page 406
382Chapter 9. Hash Tables, Maps, and Skip Lists
9.2.5 Collision-Handling Schemes
The main idea of a hash table is to take a bucket array, A, and a hash function,
h, and use them to implement a map by storing each entry (k,v) in the “bucket”
A[h(k)]. This simple idea is challenged, however, when we have two distinct keys,
k1 and k2, such that h(k1) = h(k2). The existence of such collisions prevents us
from simply inserting a new entry (k,v) directly in the bucket A[h(k)]. Collisions
also complicate our procedure for performing the find(k), put(k,v), and erase(k)
operations.
Separate Chaining
A simple and efficient way for dealing with collisions is to have each bucket A[i]
store a small map, Mi, implemented using a list, as described in Section 9.1.4,
holding entries (k,v) such that h(k) = i. That is, each separate Mi chains together
the entries that hash to index i in a linked list. This collision-resolution rule is
known as separate chaining. Assuming that we initialize each bucket A[i] to be an
empty list-based map, we can easily use the separate-chaining rule to perform the
fundamental map operations as shown in Code Fragment 9.5.
Algorithm find(k):
Output: The position of the matching entry of the map, or end if there is no key
k in the map
return A[h(k)].find(k) {delegate the find(k) to the list-based map at A[h(k)]}
Algorithm put(k,v):
p ← A[h(k)].put(k,v) {delegate the put to the list-based map at A[h(k)]}
n ← n+ 1
return p
Algorithm erase(k):
Output: None
A[h(k)].erase(k) {delegate the erase to the list-based map at A[h(k)]}
n ← n− 1
Code Fragment 9.5: The fundamental functions of the map ADT, implemented with
a hash table that uses separate chaining to resolve collisions among its n entries.
For each fundamental map operation involving a key k, the separate-chaining
approach delegates the handling of this operation to the miniature list-based map
stored at A[h(k)]. So, put(k,v) scans this list looking for an entry with key equal
to k; if it finds one, it replaces its value with v, otherwise, it puts (k,v) at the end
of this list. Likewise, find(k) searches through this list until it reaches the end or
Page 407
9.2. Hash Tables383
finds an entry with key equal to k. And erase(k) performs a similar search but
additionally removes an entry after it is found. We can “get away” with this simple
list-based approach because the spreading properties of the hash function help keep
each bucket’s list small. Indeed, a good hash function tries to minimize collisions
as much as possible, which implies that most of our buckets are either empty or
store just a single entry. In Figure 9.4, we give an illustration of a hash table with
separate chaining.
Figure 9.4: A hash table of size 13, storing 10 entries with integer keys, with colli
sions resolved by separate chaining. The compression function is h(k) = k mod 13.
For simplicity, we do not show the values associated with the keys.
Assuming we use a good hash function to index the n entries of our map in a
bucket array of capacity N, we expect each bucket to be of size n/N. This value,
called the load factor of the hash table (and denoted with λ), should be bounded
by a small constant, preferably below 1. Given a good hash function, the expected
running time of operations find, put, and erase in a map implemented with a hash
table that uses this function is O(⌈n/N⌉). Thus, we can implement these operations
to run in O(1) expected time provided n is O(N).
Open Addressing
The separate-chaining rule has many nice properties, such as allowing for simple
implementations of map operations, but it nevertheless has one slight disadvan
tage. It requires the use of an auxiliary data structure—a list—to hold entries with
colliding keys. We can handle collisions in other ways besides using the separate
Page 408
384Chapter 9. Hash Tables, Maps, and Skip Lists
chaining rule, however. In particular, if space is at a premium (for example, if we
are writing a program for a small handheld device), then we can use the alternative
approach of always storing each entry directly in a bucket, at most one entry per
bucket. This approach saves space because no auxiliary structures are employed,
but it requires a bit more complexity to deal with collisions. There are several vari
ants of this approach, collectively referred to as open-addressing schemes, which
we discuss next. Open addressing requires that the load factor is always at most 1
and that entries are stored directly in the cells of the bucket array itself.
Linear Probing and its Variants
A simple open-addressing method for collision handling is linear probing. In this
method, if we try to insert an entry (k,v) into a bucket A[i] that is already occupied
(where i = h(k)), then we try next at A[(i + 1) mod N]. If A[(i + 1) mod N] is also
occupied, then we try A[(i + 2) mod N], and so on, until we find an empty bucket
that can accept the new entry. Once this bucket is located, we simply insert the en
try there. Of course, this collision-resolution strategy requires that we change the
implementation of the get(k,v) operation. In particular, to perform such a search,
followed by either a replacement or insertion, we must examine consecutive buck
ets, starting from A[h(k)], until we either find an entry with key equal to k or we
find an empty bucket. (See Figure 9.5.) The name “linear probing” comes from the
fact that accessing a cell of the bucket array can be viewed as a “probe.”
Figure 9.5: An insertion into a hash table using linear probing to resolve collisions.
Here we use the compression function h(k) = k mod 11.
To implement erase(k), we might, at first, think we need to do a considerable
amount of shifting of entries to make it look as though the entry with key k was
never inserted, which would be very complicated. A typical way to get around this
difficulty is to replace a deleted entry with a special “available” marker object. With
this special marker possibly occupying buckets in our hash table, we modify our
search algorithm for erase(k) or find(k) so that the search for a key k skips over cells
Page 409
9.2. Hash Tables385
containing the available marker and continue probing until reaching the desired
entry or an empty bucket (or returning back to where we started). Additionally, our
algorithm for put(k,v) should remember an available cell encountered during the
search for k, since this is a valid place to put a new entry (k,v). Thus, linear probing
saves space, but it complicates removals.
Even with the use of the available marker object, linear probing suffers from an
additional disadvantage. It tends to cluster the entries of the map into contiguous
runs, which may even overlap (particularly if more than half of the cells in the hash
table are occupied). Such contiguous runs of occupied hash cells causes searches
to slow down considerably.
Quadratic Probing
Another open-addressing strategy, known as quadratic probing, involves iteratively
trying the buckets A[(i + f ( j)) mod N], for j = 0,1,2,..., where f ( j) = j2, until
finding an empty bucket. As with linear probing, the quadratic-probing strategy
complicates the removal operation, but it does avoid the kinds of clustering patterns
that occur with linear probing. Nevertheless, it creates its own kind of clustering,
called secondary clustering, where the set of filled array cells “bounces” around
the array in a fixed pattern. If N is not chosen as a prime, then the quadratic-probing
strategy may not find an empty bucket in A even if one exists. In fact, even if N is
prime, this strategy may not find an empty slot if the bucket array is at least half
full. We explore the cause of this type of clustering in an exercise (Exercise C-9.9).
Double Hashing
Another open-addressing strategy that does not cause clustering of the kind pro
duced by linear probing or by quadratic probing is the double-hashing strategy. In
this approach, we choose a secondary hash function, h′, and if h maps some key k
to a bucket A[i], with i = h(k), that is already occupied, then we iteratively try the
buckets A[(i + f ( j)) mod N] next, for j = 1,2,3,..., where f ( j) = j · h′(k). In this
scheme, the secondary hash function is not allowed to evaluate to zero; a common
choice is h′(k) = q− (k mod q), for some prime number q < N. Also, N should be
a prime. Moreover, we should choose a secondary hash function that attempts to
minimize clustering as much as possible.
These open-addressing schemes save some space over the separate-chaining
method, but they are not necessarily faster. In experimental and theoretical anal
yses, the chaining method is either competitive or faster than the other methods,
depending on the load factor of the bucket array. So, if memory space is not a ma
jor issue, the collision-handling method of choice seems to be separate chaining.
Page 410
386Chapter 9. Hash Tables, Maps, and Skip Lists
9.2.6 Load Factors and Rehashing
In all of the hash-table schemes described above, the load factor, λ = n/N, should
be kept below 1. Experiments and average-case analyses suggest that we should
maintain λ < 0.5 for the open-addressing schemes and we should maintain λ < 0.9
for separate chaining.
As we explore in Exercise C-9.9, some open-addressing schemes can start to
fail when λ ≥ 0.5. Although the details of the average-case analysis of hashing
are beyond the scope of this book, its probabilistic basis is quite intuitive. If our
hash function is good, then we expect the hash function values to be uniformly
distributed in the range [0,N − 1]. Thus, to store n items in our map, the expected
number of keys in a bucket would be ⌈n/N⌉ at most, which is O(1) if n is O(N).
With open addressing, as the load factor λ grows beyond 0.5 and starts ap
proaching 1, clusters of items in the bucket array start to grow as well. These
clusters cause the probing strategies to “bounce around” the bucket array for a con
siderable amount of time before they can finish. At the limit, when λ is close to
1, all map operations have linear expected running times, since, in this case, we
expect to encounter a linear number of occupied buckets before finding one of the
few remaining empty cells.
Rehashing into a New Table
Keeping the load factor below a certain threshold is vital for open-addressing schemes
and is also of concern to the separate-chaining method. If the load factor of a hash
table goes significantly above a specified threshold, then it is common to require
that the table be resized (to regain the specified load factor) and all the objects in
serted into this new resized table. Indeed, if we let our hash table become full, some
implementations may crash. When rehashing to a new table, a good requirement
is having the new array’s size be at least double the previous size. Once we have
allocated this new bucket array, we must define a new hash function to go with it
(possibly computing new parameters, as in the MAD method). Given this new hash
function, we then reinsert every item from the old array into the new array using
this new hash function. This process is known as rehashing.
Even with periodic rehashing, a hash table is an efficient means of implement
ing an unordered map. Indeed, if we always double the size of the table with each
rehashing operation, then we can amortize the cost of rehashing all the elements
in the table against the time used to insert them in the first place. The analysis
of this rehashing process is similar to that used to analyze vector growth. (See
Section 6.1.3.) Each rehashing generally scatters the elements throughout the new
bucket array. Thus, a hash table is a practical and effective implementation for an
unordered map.
Page 411
9.2. Hash Tables387
9.2.7 A C++ Hash Table Implementation
In Code Fragments 9.6 through 9.13, we present a C++ implementation of the map
ADT, called HashMap, which is based on hashing with separate chaining. The
class is templated with the key type K, the value type V , and the hash comparator
type H. The hash comparator defines a function, hash(k), which maps a key into an
integer index. As with less-than comparators (see Section 8.1.2), a hash comparator
class does this by overriding the “()” operator.
We present the general class structure in Code Fragment 9.6. The definition
begins with the public types required by the map interface, the entry type Entry,
and the iterator type Iterator. This is followed by the declarations of the public
member functions. We then give the private member data, which consists of the
number of entries n, the hash comparator function hash, and the bucket array B.
We have omitted two sections, which are filled in later. The first is a declaration
of some utility types and functions and the second is the declaration of the map’s
iterator class.
template <typename K, typename V, typename H>
class HashMap {
public:// public types
typedef Entry<const K,V> Entry;// a (key,value) pair
class Iterator;// a iterator/position
public:// public functions
HashMap(int capacity = 100);// constructor
int size() const;// number of entries
bool empty() const;// is the map empty?
Iterator find(const K& k);// find entry with key k
Iterator put(const K& k, const V& v);// insert/replace (k,v)
void erase(const K& k);// remove entry with key k
void erase(const Iterator& p);// erase entry at p
Iterator begin();// iterator to first entry
Iterator end();// iterator to end entry
protected:// protected types
typedef std::list<Entry> Bucket;// a bucket of entries
typedef std::vector<Bucket> BktArray;// a bucket array
// . . .insert HashMap utilities here
private:
int n;// number of entries
H hash;// the hash comparator
BktArray B;// bucket array
public:// public types
// . . .insert Iterator class declaration here
};
Code Fragment 9.6: The class HashMap, which implements the map ADT.
Page 412
388Chapter 9. Hash Tables, Maps, and Skip Lists
We have defined the key part of Entry to be “const K,” rather than “K.” This
prevents a user from inadvertently modifying a key. The class makes use of two
major data types. The first is an STL list of entries, called a Bucket, each storing a
single bucket. The other is an STL vector of buckets, called BktArray.
Before describing the main elements of the class, we introduce a few local (pro
tected) utilities in Code Fragment 9.7. We declare three helper functions, finder,
inserter, and eraser, which, respectively, handle the low-level details of finding, in
serting, and removing entries. For convenience, we define two iterator types, one
called BItor for iterating over the buckets of the bucket array, and one called EItor,
for iterating over the entries of a bucket. We also give two utility functions, nextBkt
and endOfBkt, which are used to iterate through the entries of a single bucket.
Iterator finder(const K& k);// find utility
Iterator inserter(const Iterator& p, const Entry& e); // insert utility
void eraser(const Iterator& p);// remove utility
typedef typename BktArray::iterator BItor;// bucket iterator
typedef typename Bucket::iterator EItor;// entry iterator
static void nextEntry(Iterator& p)// bucket’s next entry
{ ++p.ent; }
static bool endOfBkt(const Iterator& p)// end of bucket?
{ return p.ent == p.bkt−>end(); }
Code Fragment 9.7: Declarations of utilities to be inserted into HashMap.
We present the class Iterator in Code Fragment 9.8. An iterator needs to store
enough information about the position of an entry to allow it to navigate. The mem
bers ent, bkt, and ba store, respectively, an iterator to the current entry, the bucket
containing this entry, and the bucket array containing the bucket. The first two are
of types EItor and BItor, respectively, and the third is a pointer. Our implementa
tion is minimal. In addition to a constructor, we provide operators for dereferencing
(“*”), testing equality (“==”), and advancing through the map (“++”).
class Iterator {// an iterator (& position)
private:
EItor ent;// which entry
BItor bkt;// which bucket
const BktArray* ba;// which bucket array
public:
Iterator(const BktArray& a, const BItor& b, const EItor& q = EItor())
: ent(q), bkt(b), ba(&a) { }
Entry& operator*() const;// get entry
bool operator==(const Iterator& p) const;// are iterators equal?
Iterator& operator++();// advance to next entry
friend class HashMap;// give HashMap access
};
Code Fragment 9.8: Declaration of the Iterator class for HashMap.
Page 413
9.2. Hash Tables389
Iterator Dereferencing and Condensed Function Definitions
Let us now present the definitions of the class member functions for our map’s
Iterator class. In Code Fragment 9.9, we present an implementation of the deref
erencing operator. The function body itself is very simple and involves return
ing a reference to the corresponding entry. However, the rules of C++ syntax de
mand an extraordinary number of template qualifiers. First, we need to qualify the
function itself as being a member of HashMap’s iterator class, which we do with
the qualifier HashMap<K,V,H>::Iterator. Second, we need to qualify the func
tion’s return type as being HashMap’s entry class, which we do with the qualifier
HashMap<K,V,H>::Entry. On top of this, we must recall from Section 8.2.1 that,
since we are using a template parameter to define a type, we need to include the
keyword typename.
template <typename K, typename V, typename H> // get entry
typename HashMap<K,V,H>::Entry&
HashMap<K,V,H>::Iterator::operator*() const
{ return *ent; }
Code Fragment 9.9: The Iterator dereferencing operator (complete form).
In order to make our function definitions more readable, we adopt a notational
convention in some of our future code fragments of specifying the scoping qualifier
for the code fragment in italic blue font. We omit this qualifier from the code frag
ment, and we also omit the template statement and the typename specifications.
Adding these back is a simple mechanical exercise. Although this is not valid C++
Caution syntax, it conveys the important content in a much more succinct manner. An ex
ample of the same dereferencing operator is shown in Code Fragment 9.10.
/* HashMap〈K,V,H〉 :: */// get entry
Entry& Iterator::operator*() const
{ return *ent; }
Code Fragment 9.10: The same dereferencing operator of Code Fragment 9.9 in
condensed form.
Definitions of the Other Iterator Member Functions
Let us next consider the Iterator operator “operator == (p),” which tests whether
this iterator is equal to iterator p. We first check that they belong to the same bucket
array and the same bucket within this array. If not, the iterators certainly differ.
Otherwise, we check whether they both refer to the end of the bucket array. (Since
we have established that the buckets are equal, it suffices to test just one of them.)
If so, they are both equal to HashMap::end(). If not, we check whether they both
Page 414
390Chapter 9. Hash Tables, Maps, and Skip Lists
refer to the same entry of the bucket. This is implemented in Code Fragment 9.11.
/* HashMap〈K,V,H〉 :: */// are iterators equal?
bool Iterator::operator==(const Iterator& p) const {
if (ba != p.ba | | bkt != p.bkt) return false;// ba or bkt differ?
else if (bkt == ba−>end()) return true;// both at the end?
else return (ent == p.ent);// else use entry to decide
}
Code Fragment 9.11: The Iterator operators for equality testing and increment.
Next, let us consider the Iterator increment operator, shown in Code Frag
ment 9.12. The objective is to advance the iterator to the next valid entry. Typ
ically, this involves advancing to the next entry within the current bucket. But, if
we fall off the end of this bucket, we must advance to the first element of the next
nonempty bucket. To do this, we first advance to the next bucket entry by applying
the STL increment operator on the entry iterator ent. We then use the utility func
tion endOfBkt to determine whether we have arrived at the end of this bucket. If
so, we search for the next nonempty bucket. To do this, we repeatedly increment
bkt and check whether we have fallen off the end of the bucket array. If so, this is
the end of the map and we are done. Otherwise, we check whether the bucket is
empty. When we first find a nonempty bucket, we move ent to the first entry of this
bucket.
/* HashMap〈K,V,H〉 :: */// advance to next entry
Iterator& Iterator::operator++() {
++ent;// next entry in bucket
if (endOfBkt(*this)) {// at end of bucket?
++bkt;// go to next bucket
while (bkt != ba−>end() && bkt−>empty()) // find nonempty bucket
++bkt;
if (bkt == ba−>end()) return *this;// end of bucket array?
ent = bkt−>begin();// first nonempty entry
}
return *this;// return self
}
Code Fragment 9.12: The Iterator operators for equality testing and increment.
Definitions of the HashMap Member Functions
Before discussing the main functions of class HashMap, let us present the functions
begin and end. These are given in Code Fragment 9.13. The function end is the
simpler of the two. It involves generating an iterator whose bucket component is the
end of the bucket array. We do not bother to specify a value for the entry part of the
Page 415
9.2. Hash Tables391
iterator. The reason is that our iterator equality test (shown in Code Fragment 9.11)
does not bother to compare the entry iterator values if the bucket iterators are at the
end of the bucket array.
/* HashMap〈K,V,H〉 :: */// iterator to end
Iterator end()
{ return Iterator(B, B.end()); }
/* HashMap〈K,V,H〉 :: */// iterator to front
Iterator begin() {
if (empty()) return end();// emtpty - return end
BItor bkt = B.begin();// else search for an entry
while (bkt−>empty()) ++bkt;// find nonempty bucket
return Iterator(B, bkt, bkt−>begin());// return first of bucket
}
Code Fragment 9.13: The functions of HashMap returning iterators to the beginning
and end of the map.
The function begin, shown in the bottom part of Code Fragment 9.13, is more
complex, since we need to search for a nonempty bucket. We first check whether
the map is empty. If so, we simply return the map’s end. Otherwise, starting at the
beginning of the bucket array, we search for a nonempty bucket. (We know we will
succeed in finding one.) Once we find it, we return an iterator that points to the first
entry of this bucket.
Now that we have presented the iterator-related functions, we are ready to
present the functions for class HashMap. We begin with the constructor and sim
ple container functions. The constructor is given the bucket array’s capacity and
creates a vector of this size. The member n tracks the number of entries. These are
given in Code Fragment 9.14.
/* HashMap〈K,V,H〉 :: */// constructor
HashMap(int capacity) : n(0), B(capacity) { }
/* HashMap〈K,V,H〉 :: */// number of entries
int size() const { return n; }
/* HashMap〈K,V,H〉 :: */// is the map empty?
bool empty() const { return size() == 0; }
Code Fragment 9.14: The constructor and standard functions for HashMap.
Next, we present the functions related to finding keys in the top part of Code
Fragment 9.15. Most of the work is done by the utility function finder. It first
applies the hash function associated with the given hash comparator to the key k.
It converts this to an index into the bucket array by taking the hash value modulo
Page 416
392Chapter 9. Hash Tables, Maps, and Skip Lists
the array size. To obtain an iterator to the desired bucket, we add this index to
the beginning iterator of the bucket array. (We are using the fact mentioned in
Section 6.1.4 that STL vectors provide a random access iterator, so addition is
allowed.) Let bkt be an iterator to this bucket. We create an iterator p, which is
initialized to the beginning of this bucket. We then perform a search for an entry
whose key matches k or until we fall off the end of the list. In either case, we return
the final value of the iterator as the search result.
/* HashMap〈K,V,H〉 :: */// find utility
Iterator finder(const K& k) {
int i = hash(k) % B.size();// get hash index i
BItor bkt = B.begin() + i;// the ith bucket
Iterator p(B, bkt, bkt−>begin());// start of ith bucket
while (!endOfBkt(p) && (*p).key() != k)// search for k
nextEntry(p);
return p;// return final position
}
/* HashMap〈K,V,H〉 :: */// find key
Iterator find(const K& k) {
Iterator p = finder(k);// look for k
if (endOfBkt(p))// didn’t find it?
return end();// return end iterator
else
return p;// return its position
}
Code Fragment 9.15: The functions of HashMap related to finding keys.
The public member function find is shown in the bottom part of Code Frag
ment 9.15. It invokes the finder utility. If the entry component is at the end of the
bucket, we know that the key was not found, so we return the special iterator end()
to the end of the map. (In this way, all unsuccessful searches produce the same
result.) This is shown in Code Fragment 9.15.
The insertion utility, inserter, is shown in the top part of Code Fragment 9.16.
This utility is given the desired position at which to insert the new entry. It invokes
the STL list insert function to perform the insertion. It also increments the count of
the number of entries in the map and returns an iterator to the inserted position.
The public insert function, put, first applies finder to determine whether any
entry with this key exists in the map. We first determine whether it was not found
by testing whether the iterator as fallen off the end of the bucket. If so, we insert
it at the end of this bucket. Otherwise, we modify the existing value of this entry.
Later, in Section 9.5.2, we present an alternative approach, which inserts a new
entry, even when a duplicate key is discovered.
Page 417
9.2. Hash Tables393
/* HashMap〈K,V,H〉 :: */// insert utility
Iterator inserter(const Iterator& p, const Entry& e) {
EItor ins = p.bkt−>insert(p.ent, e);// insert before p
n++;// one more entry
return Iterator(B, p.bkt, ins);// return this position
}
/* HashMap〈K,V,H〉 :: */// insert/replace (v,k)
Iterator put(const K& k, const V& v) {
Iterator p = finder(k);// search for k
if (endOfBkt(p)) {// k not found?
return inserter(p, Entry(k, v));// insert at end of bucket
}
else {// found it?
p.ent−>setValue(v);// replace value with v
return p;// return this position
}
}
Code Fragment 9.16: The functions of HashMap for inserting and replacing entries.
The removal functions are also quite straightforward and are given in Code
Fragment 9.17. The main utility is the function eraser, which removes an entry at
a given position by invoking the STL list erase function. It also decrements the
number of entries. The iterator-based removal function simply invokes eraser. The
key-based removal function first applies the finder utility to look up the key. If it is
not found, that is, if the returned position is the end of the bucket, an exception is
thrown. Otherwise, the eraser utility is invoked to remove the entry.
/* HashMap〈K,V,H〉 :: */// remove utility
void eraser(const Iterator& p) {
p.bkt−>erase(p.ent);// remove entry from bucket
n−−;// one fewer entry
}
/* HashMap〈K,V,H〉 :: */// remove entry at p
void erase(const Iterator& p)
{ eraser(p); }
/* HashMap〈K,V,H〉 :: */// remove entry with key k
void erase(const K& k) {
Iterator p = finder(k);// find k
if (endOfBkt(p))// not found?
throw NonexistentElement("Erase of nonexistent"); // . . .error
eraser(p);// remove it
}
Code Fragment 9.17: The functions of HashMap involved with removing entries.
Page 418
394Chapter 9. Hash Tables, Maps, and Skip Lists
9.3 Ordered Maps
In some applications, simply looking up values based on associated keys is not
enough. We often also want to keep the entries in a map sorted according to some
total order and be able to look up keys and values based on this ordering. That is,
in an ordered map, we want to perform the usual map operations, but also maintain
an order relation for the keys in our map and use this order in some of the map
functions. We can use a comparator to provide the order relation among keys,
allowing us to define an ordered map relative to this comparator, which can be
provided to the ordered map as an argument to its constructor.
When the entries of a map are stored in order, we can provide efficient im
plementations for additional functions in the map ADT. As with the standard map
ADT, in order to indicate that an object is not present, the class provides a spe
cial sentinel iterator called end. The ordered map includes all the functions of the
standard map ADT plus the following:
firstEntry(k): Return an iterator to the entry with smallest key value; if
the map is empty, it returns end.
lastEntry(k): Return an iterator to the entry with largest key value; if
the map is empty, it returns end.
ceilingEntry(k): Return an iterator to the entry with the least key value
greater than or equal to k; if there is no such entry, it
returns end.
floorEntry(k): Return an iterator to the entry with the greatest key value
less than or equal to k; if there is no such entry, it returns
end.
lowerEntry(k): Return an iterator to the entry with the greatest key value
less than k; if there is no such entry, it returns end.
higherEntry(k): Return an iterator to the entry with the least key value
greater than k; if there is no such entry, it returns end.
Implementing an Ordered Map
The ordered nature of the operations given above for the ordered map ADT makes
the use of an unordered list or a hash table inappropriate, because neither of these
data structures maintains any ordering information for the keys in the map. Indeed,
hash tables achieve their best search speeds when their keys are distributed almost
at random. Thus, we should consider an alternative implementation when dealing
with ordered maps. We discuss one such implementation next, and we discuss other
implementations in Section 9.4 and Chapter 10.
Page 419
9.3. Ordered Maps395
9.3.1 Ordered Search Tables and Binary Search
If the keys in a map come from a total order, we can store the map’s entries in a
vector L in increasing order of the keys. (See Figure 9.6.) We specify that L is
a vector, rather than a node list, because the ordering of the keys in the vector L
allows for faster searching than would be possible had L been, say, implemented
with a linked list. Admittedly, a hash table has good expected running time for
searching. But its worst-case time for searching is no better than a linked list, and
in some applications, such as in real-time processing, we need to guarantee a worst
case searching bound. The fast algorithm for searching in an ordered vector, which
we discuss in this subsection, has a good worst-case guarantee on its running time.
So it might be preferred over a hash table in certain applications. We refer to this
ordered vector implementation of a map as an ordered search table.
Figure 9.6: Realization of a map by means of an ordered search table. We show
only the keys for this map in order to highlight their ordering.
The space requirement of an ordered search table is O(n), which is similar to
the list-based map implementation (Section 9.1.4), assuming we grow and shrink
the array supporting the vector L to keep the size of this array proportional to the
number of entries in L. Unlike an unordered list, however, performing updates in
a search table takes a considerable amount of time. In particular, performing the
insert(k,v) operation in a search table requires O(n) time in the worst case, since we
need to shift up all the entries in the vector with key greater than k to make room for
the new entry (k,v). A similar observation applies to the operation erase(k), since
it takes O(n) time in the worst case to shift all the entries in the vector with key
greater than k to close the “hole” left by the removed entry (or entries). The search
table implementation is therefore inferior to the linked list implementation in terms
of the worst-case running times of the map update operations. Nevertheless, we
can perform the find function much faster in a search table.
Binary Search
A significant advantage of using an ordered vector L to implement a map with n
entries is that accessing an element of L by its index takes O(1) time. We recall,
from Section 6.1, that the index of an element in a vector is the number of elements
preceding it. Thus, the first element in L has index 0, and the last element has
index n − 1. In this subsection, we give a classic algorithm, binary search, to
locate an entry in an ordered search table. We show how this method can be used
Page 420
396Chapter 9. Hash Tables, Maps, and Skip Lists
to quickly perform the find function of the map ADT, but a similar method can be
used for each of the ordered-map functions, ceilingEntry, floorEntry, lowerEntry,
and higherEntry.
The elements stored in L are the entries of a map, and since L is ordered, the en
try at index i has a key no smaller than the keys of the entries at indices 0,...,i−1,
and no larger than the keys of the entries at indices i+1,...,n−1. This observation
allows us to quickly “home in” on a search key k using a variant of the children’s
game “high-low.” We call an entry of our map a candidate if, at the current stage
of the search, we cannot rule out that this entry has key equal to k. The algorithm
maintains two parameters, low and high, such that all the candidate entries have
index at least low and at most high in L. Initially, low = 0 and high = n− 1. We
then compare k to the key of the median candidate e, that is, the entry e with index
mid = ⌊(low+high)/2⌋.
We consider three cases:
• If k = e.key(), then we have found the entry we were looking for, and the
search terminates successfully returning e
• If k < e.key(), then we recur on the first half of the vector, that is, on the
range of indices from low to mid −1
• If k > e.key(), we recur on the range of indices from mid +1 to high
This search method is called binary search, and is given in pseudo-code in Code
Fragment 9.18. Operation find(k) on an n-entry map implemented with an ordered
vector L consists of calling BinarySearch(L,k,0,n−1).
Algorithm BinarySearch(L,k,low,high):
Input: An ordered vector L storing n entries and integers low and high
Output: An entry of L with key equal to k and index between low and high, if
such an entry exists, and otherwise the special sentinel end
if low > high then
return end
else
mid ← ⌊(low+high)/2⌋
e ← L.at(mid)
if k = e.key() then
return e
else if k < e.key() then
return BinarySearch(L,k,low,mid −1)
else
return BinarySearch(L,k,mid +1,high)
Code Fragment 9.18: Binary search in an ordered vector.
Page 421
9.3. Ordered Maps397
We illustrate the binary search algorithm in Figure 9.7.
Figure 9.7: Example of a binary search to perform operation find(22), in a map
with integer keys, implemented with an ordered vector. For simplicity, we show
the keys, not the whole entries.
Considering the running time of binary search, we observe that a constant num
ber of primitive operations are executed at each recursive call of function Binary
Search. Hence, the running time is proportional to the number of recursive calls
performed. A crucial fact is that with each recursive call the number of candidate
entries still to be searched in the vector L is given by the value
high −low+ 1.
Moreover, the number of remaining candidates is reduced by at least one half with
each recursive call. Specifically, from the definition of mid, the number of remain
ing candidates is either
(mid −1)−low+ 1 = low +2high−low ≤ high−2low+ 1
orhigh−(mid + 1) + 1 = high−low +2high ≤ high −2low+ 1.
Initially, the number of candidate entries is n; after the first call to BinarySearch, it
is at most n/2; after the second call, it is at most n/4; and so on. In general, after the
ith call to BinarySearch, the number of candidate entries remaining is at most n/2i.
In the worst case (unsuccessful search), the recursive calls stop when there are no
more candidate entries. Hence, the maximum number of recursive calls performed,
is the smallest integer m, such that
n/2m < 1.
Page 422
398Chapter 9. Hash Tables, Maps, and Skip Lists
In other words (recalling that we omit a logarithm’s base when it is 2), m > logn.
Thus, we have
m = ⌊logn⌋+1,
which implies that binary search runs in O(logn) time.
Thus, we can use an ordered search table to perform fast searches in an ordered
map, but using such a table for lots of map updates would take a considerable
amount of time. For this reason, the primary applications for search tables are in
situations where we expect few updates but many searches. Such a situation could
arise, for example, in an ordered list of English words we use to order entries in an
encyclopedia or help file.
Comparing Map Implementations
Note that we can use an ordered search table to implement the map ADT even if
we don’t want to use the additional functions of the ordered map ADT. Table 9.2
compares the running times of the functions of a (standard) map realized by either
an unordered list, a hash table, or an ordered search table. Note that an unordered
list allows for fast insertions but slow searches and removals, whereas a search ta
ble allows for fast searches but slow insertions and removals. Incidentally, although
we don’t explicitly discuss it, we note that a sorted list implemented with a doubly
linked list would be slow in performing almost all the map operations. (See Exer
cise R-9.5.) Nevertheless, the list-like data structure we discuss in the next section
can perform the functions of the ordered map ADT quite efficiently.
Method ListHash TableSearch Table
size, empty O(1)O(1)O(1)
find O(n) O(1) exp., O(n) worst-case O(logn)
insertO(1)O(1)O(n)
erase O(n) O(1) exp., O(n) worst-caseO(n)
Table 9.2: Comparison of the running times of the functions of a map realized
by means of an unordered list, a hash table, or an ordered search table. We let n
denote the number of entries in the map and we let N denote the capacity of the
bucket array in the hash table implementation. The space requirement of all the
implementations is O(n), assuming that the arrays supporting the hash-table and
search-table implementations are maintained such that their capacity is proportional
to the number of entries in the map.
Page 423
9.3. Ordered Maps399
9.3.2 Two Applications of Ordered Maps
As we have mentioned in the preceding sections, unordered and ordered maps have
many applications. In this section, we explore some specific applications of ordered
maps.
Flight Databases
There are several web sites on the Internet that allow users to perform queries on
flight databases to find flights between various cities, typically with the intent of
buying a ticket. To make a query, a user specifies origin and destination cities, a
departure date, and a departure time. To support such queries, we can model the
flight database as a map, where keys are Flight objects that contain fields corre
sponding to these four parameters. That is, a key is a tuple
k = (origin, destination, date, time).
Additional information about a flight, such as the flight number, the number of seats
still available in first (F) and coach (Y) class, the flight duration, and the fare, can
be stored in the value object.
Finding a requested flight is not simply a matter of finding a key in the map
matching the requested query, however. The main difficulty is that, although a user
typically wants to exactly match the origin and destination cities, as well as the
departure date, he or she will probably be content with any departure time that is
close to his or her requested departure time. We can handle such a query, of course,
by ordering our keys lexicographically. Thus, given a user query key k, we could,
for instance, call ceilingEntry(k) to return the flight between the desired cities on
the desired date, with departure time at the desired time or after. A similar use
of floorEntry(k) would give us the flight with departure time at the desired time
or before. Given these entries, we could then use the higherEntry or lowerEntry
functions to find flights with the next close-by departure times that are respectively
higher or lower than the desired time, k. Therefore, an efficient implementation for
an ordered map would be a good way to satisfy such queries. For example, calling
ceilingEntry(k) on a query key k = (ORD, PVD, 05May, 09:30), followed by the
respective calls to higherEntry, might result in the following sequence of entries:
( (ORD, PVD, 05May, 09:53),(AA 1840, F5, Y15, 02:05, $251) )
( (ORD, PVD, 05May, 13:29),(AA 600, F2, Y0, 02:16, $713) )
( (ORD, PVD, 05May, 17:39),(AA 416, F3, Y9, 02:09, $365) )
( (ORD, PVD, 05May, 19:50),(AA 1828, F9, Y25, 02:13, $186) )
Page 424
400Chapter 9. Hash Tables, Maps, and Skip Lists
Maxima Sets
Life is full of trade-offs. We often have to trade off a desired performance measure
against a corresponding cost. Suppose, for the sake of an example, we are interested
in maintaining a database rating automobiles by their maximum speeds and their
cost. We would like to allow someone with a certain amount to spend to query our
database to find the fastest car they can possibly afford.
We can model such a trade-off problem as this by using a key-value pair to
model the two parameters that we are trading off, which in this case would be the
pair (cost,speed) for each car. Notice that some cars are strictly better than other
cars using this measure. For example, a car with cost-speed pair (20,000, 100) is
strictly better than a car with cost-speed pair (30,000, 90). At the same time, there
are some cars that are not strictly dominated by another car. For example, a car
with cost-speed pair (20,000, 100) may be better or worse than a car with cost
speed pair (30,000, 120), depending on how much money we have to spend. (See
Figure 9.8.)
Figure 9.8: The cost-performance trade-off with key-value pairs represented by
points in the plane. Notice that point p is strictly better than points c, d, and e, but
may be better or worse than points a, b, f , g, and h, depending on the price we are
willing to pay. Thus, if we were to add p to our set, we could remove the points c,
d, and e, but not the others.
Formally, we say a price-performance pair (a,b) dominates a pair (c,d) if a < c
and b > d. A pair (a,b) is called a maximum pair if it is not dominated by any other
pairs. We are interested in maintaining the set of maxima of a collection C of price
performance pairs. That is, we would like to add new pairs to this collection (for
example, when a new car is introduced), and we would like to query this collection
for a given dollar amount, d, to find the fastest car that costs no more than d dollars.
Page 425
9.3. Ordered Maps401
Maintaining a Maxima Set with an Ordered Map
We can store the set of maxima pairs in an ordered map, M, ordered by cost, so that
the cost is the key field and performance (speed) is the value field. We can then
implement operations add(c, p), which adds a new cost-performance pair (c, p),
and best(c), which returns the best pair with cost at most c, as shown in Code
Fragments 9.19 and 9.20.
Algorithm best(c):
Input: A cost c
Output: The cost-performance pair in M with largest cost less than or equal to
c, or the special sentinel end, if there is no such pair
return M.floorEntry(c)
Code Fragment 9.19: The best() function, used in a class maintaining a set of max
ima implemented with an ordered map M.
Algorithm add(c, p):
Input: A cost-performance pair (c, p)
Output: None (but M will have (c, p) added to the set of cost-performance
pairs)
e ← M.floorEntry(c){the greatest pair with cost at most c}
if e ∕= end then
if e.value() > p then
return{(c, p) is dominated, so don’t insert it in M}
e ← M.ceilingEntry(c){next pair with cost at least c}
{Remove all the pairs that are dominated by (c, p)}
while e ∕= end and e.value() < p do
M.erase(e.key()){this pair is dominated by (c, p)}
e ← M.higherEntry(e.key()){the next pair after e}
M.put(c, p){Add the pair (c, p), which is not dominated}
Code Fragment 9.20: The add(c, p) function used in a class for maintaining a set of
maxima implemented with an ordered map M.
Unfortunately, if we implement M using any of the data structures described
above, it results in a poor running time for the above algorithm. If, on the other
hand, we implement M using a skip list, which we describe next, then we can
perform best(c) queries in O(logn) expected time and add(c, p) updates in O((1+
r)logn) expected time, where r is the number of points removed.
Page 426
402Chapter 9. Hash Tables, Maps, and Skip Lists
9.4 Skip Lists
An interesting data structure for efficiently realizing the ordered map ADT is the
skip list. This data structure makes random choices in arranging the entries in such
a way that search and update times are O(logn) on average, where n is the number
of entries in the dictionary. Interestingly, the notion of average time complexity
used here does not depend on the probability distribution of the keys in the input.
Instead, it depends on the use of a random-number generator in the implementation
of the insertions to help decide where to place the new entry. The running time is
averaged over all possible outcomes of the random numbers used when inserting
entries.
Because they are used extensively in computer games, cryptography, and com
puter simulations, functions that generate numbers that can be viewed as random
numbers are built into most modern computers. Some functions, called pseudo
random number generators, generate random-like numbers, starting with an initial
seed. Other functions use hardware devices to extract “true” random numbers from
nature. In any case, we assume that our computer has access to numbers that are
sufficiently random for our analysis.
The main advantage of using randomization in data structure and algorithm
design is that the structures and functions that result are usually simple and efficient.
We can devise a simple randomized data structure, called the skip list, which has the
same logarithmic time bounds for searching as is achieved by the binary searching
algorithm. Nevertheless, the bounds are expected for the skip list, while they are
worst-case bounds for binary searching in a lookup table. On the other hand, skip
lists are much faster than lookup tables for map updates.
A skip list S for a map M consists of a series of lists {S0,S1,...,Sh}. Each list
Si stores a subset of the entries of M sorted by increasing keys plus entries with
two special keys, denoted −∞ and +∞, where −∞ is smaller than every possible
key that can be inserted in M and +∞ is larger than every possible key that can be
inserted in M. In addition, the lists in S satisfy the following:
• List S0 contains every entry of the map M (plus the special entries with keys
−∞ and +∞)
• For i = 1,...,h− 1, list Si contains (in addition to −∞ and +∞) a randomly
generated subset of the entries in list Si−1
• List Sh contains only −∞ and +∞.
An example of a skip list is shown in Figure 9.9. It is customary to visualize a skip
list S with list S0 at the bottom and lists S1,...,Sh above it. Also, we refer to h as
the height of skip list S.
Page 427
9.4. Skip Lists403
Figure 9.9: Example of a skip list storing 10 entries. For simplicity, we show only
the keys of the entries.
Intuitively, the lists are set up so that Si+1 contains more or less every other
entry in Si. As can be seen in the details of the insertion method, the entries in Si+1
are chosen at random from the entries in Si by picking each entry from Si to also
be in Si+1 with probability 1/2. That is, in essence, we “flip a coin” for each entry
in Si and place that entry in Si+1 if the coin comes up “heads.” Thus, we expect S1
to have about n/2 entries, S2 to have about n/4 entries, and, in general, Si to have
about n/2i entries. In other words, we expect the height h of S to be about logn.
The halving of the number of entries from one list to the next is not enforced as an
explicit property of skip lists, however. Instead, randomization is used.
Using the position abstraction used for lists and trees, we view a skip list as a
two-dimensional collection of positions arranged horizontally into levels and ver
tically into towers. Each level is a list Si and each tower contains positions storing
the same entry across consecutive lists. The positions in a skip list can be traversed
using the following operations:
after(p): Return the position following p on the same level.
before(p): Return the position preceding p on the same level.
below(p): Return the position below p in the same tower.
above(p): Return the position above p in the same tower.
We conventionally assume that the above operations return a null position if the
position requested does not exist. Without going into the details, we note that we
can easily implement a skip list by means of a linked structure such that the above
traversal functions each take O(1) time, given a skip-list position p. Such a linked
structure is essentially a collection of h doubly linked lists aligned at towers, which
are also doubly linked lists.
Page 428
404Chapter 9. Hash Tables, Maps, and Skip Lists
9.4.1 Search and Update Operations in a Skip List
The skip list structure allows for simple map search and update algorithms. In fact,
all of the skip list search and update algorithms are based on an elegant SkipSearch
function that takes a key k and finds the position p of the entry e in list S0 such that
e has the largest key (which is possibly −∞) less than or equal to k.
Searching in a Skip List
Suppose we are given a search key k. We begin the SkipSearch function by setting
a position variable p to the top-most, left position in the skip list S, called the start
position of S. That is, the start position is the position of Sh storing the special
entry with key −∞. We then perform the following steps (see Figure 9.10), where
key(p) denotes the key of the entry at position p:
1. If S.below(p) is null, then the search terminates—we are at the bottom and
have located the largest entry in S with key less than or equal to the search
key k. Otherwise, we drop down to the next lower level in the present tower
by setting p ← S.below(p).
2. Starting at position p, we move p forward until it is at the right-most position
on the present level such that key(p) ≤ k. We call this the scan forward step.
Note that such a position always exists, since each level contains the keys
+∞ and −∞. In fact, after we perform the scan forward for this level, p
may remain where it started. In any case, we then repeat the previous step.
Figure 9.10: Example of a search in a skip list. The positions visited when searching
for key 50 are highlighted in blue.
We give a pseudo-code description of the skip-list search algorithm, SkipSearch,
in Code Fragment 9.21. Given this function, it is now easy to implement the op
eration find(k)—we simply perform p ← SkipSearch(k) and test whether or not
key(p) = k. If these two keys are equal, we return p; otherwise, we return null.
Page 429
9.4. Skip Lists405
Algorithm SkipSearch(k):
Input: A search key k
Output: Position p in the bottom list S0 such that the entry at p has the largest
key less than or equal to k
p ← s
while below(p) ∕= null do
p ← below(p){drop down}
while k ≥ key(after(p)) do
p ← after(p){scan forward}
return p.
Code Fragment 9.21: Search in a skip list S. Variable s holds the start position of S.
As it turns out, the expected running time of algorithm SkipSearch on a skip
list with n entries is O(logn). We postpone the justification of this fact, however,
until after we discuss the implementation of the update functions for skip lists.
Insertion in a Skip List
The insertion algorithm for skip lists uses randomization to decide the height of the
tower for the new entry. We begin the insertion of a new entry (k,v) by performing
a SkipSearch(k) operation. This gives us the position p of the bottom-level entry
with the largest key less than or equal to k (note that p may hold the special entry
with key −∞). We then insert (k,v) immediately after position p. After inserting
the new entry at the bottom level, we “flip” a coin. If the flip comes up tails, then
we stop here. Else (the flip comes up heads), we backtrack to the previous (next
higher) level and insert (k,v) in this level at the appropriate position. We again
flip a coin; if it comes up heads, we go to the next higher level and repeat. Thus,
we continue to insert the new entry (k,v) in lists until we finally get a flip that
comes up tails. We link together all the references to the new entry (k,v) created
in this process to create the tower for the new entry. A coin flip can be simulated
with C++’s built-in, pseudo-random number generator by testing whether a random
integer is even or odd.
We give the insertion algorithm for a skip list S in Code Fragment 9.22 and we
illustrate it in Figure 9.11. The algorithm uses function insertAfterAbove(p,q,(k,v))
that inserts a position storing the entry (k,v) after position p (on the same level as
p) and above position q, returning the position r of the new entry (and setting in
ternal references so that after, before, above, and below functions work correctly
for p, q, and r). The expected running time of the insertion algorithm on a skip list
with n entries is O(logn), which we show in Section 9.4.2.
Page 430
406Chapter 9. Hash Tables, Maps, and Skip Lists
Algorithm SkipInsert(k,v):
Input: Key k and value v
Output: Topmost position of the entry inserted in the skip list
p ← SkipSearch(k)
q ← null
e ← (k,v)
i ← −1
repeat
i ← i + 1
if i ≥ h then
h ← h + 1{add a new level to the skip list}
t ← after(s)
s ← insertAfterAbove(null,s,(−∞,null))
insertAfterAbove(s,t,(+∞,null))
while above(p) = null do
p ← before(p){scan backward}
p ← above(p){jump up to higher level}
q ← insertAfterAbove(p,q,e) {add a position to the tower of the new entry}
until coinFlip() = tails
n ← n + 1
return q
Code Fragment 9.22: Insertion in a skip list. Method coinFlip() returns “heads” or
“tails,” each with probability 1/2. Variables n, h, and s hold the number of entries,
the height, and the start node of the skip list.
Figure 9.11: Insertion of an entry with key 42 into the skip list of Figure 9.9. We
assume that the random “coin flips” for the new entry came up heads three times
in a row, followed by tails. The positions visited are highlighted in blue. The
positions inserted to hold the new entry are drawn with thick lines, and the positions
preceding them are flagged.
Page 431
9.4. Skip Lists407
Removal in a Skip List
Like the search and insertion algorithms, the removal algorithm for a skip list is
quite simple. In fact, it is even easier than the insertion algorithm. That is, to
perform an erase(k) operation, we begin by executing function SkipSearch(k). If
the position p stores an entry with key different from k, we return null. Otherwise,
we remove p and all the positions above p, which are easily accessed by using
above operations to climb up the tower of this entry in S starting at position p. The
removal algorithm is illustrated in Figure 9.12 and a detailed description of it is
left as an exercise (Exercise R-9.17). As we show in the next subsection, operation
erase in a skip list with n entries has O(log n) expected running time.
Before we give this analysis, however, there are some minor improvements to
the skip list data structure we would like to discuss. First, we don’t actually need
to store references to entries at the levels of the skip list above the bottom level,
because all that is needed at these levels are references to keys. Second, we don’t
actually need the above function. In fact, we don’t need the before function either.
We can perform entry insertion and removal in strictly a top-down, scan-forward
fashion, thus saving space for “up” and “prev” references. We explore the details
of this optimization in Exercise C-9.10. Neither of these optimizations improve
the asymptotic performance of skip lists by more than a constant factor, but these
improvements can, nevertheless, be meaningful in practice. In fact, experimental
evidence suggests that optimized skip lists are faster in practice than AVL trees and
other balanced search trees, which are discussed in Chapter 10.
The expected running time of the removal algorithm is O(logn), which we
show in Section 9.4.2.
Figure 9.12: Removal of the entry with key 25 from the skip list of Figure 9.11.
The positions visited after the search for the position of S0 holding the entry are
highlighted in blue. The positions removed are drawn with dashed lines.
Page 432
408Chapter 9. Hash Tables, Maps, and Skip Lists
Maintaining the Top-most Level
A skip list S must maintain a reference to the start position (the top-most, left
position in S) as a member variable, and must have a policy for any insertion that
wishes to continue inserting a new entry past the top level of S. There are two
possible courses of action we can take, both of which have their merits.
One possibility is to restrict the top level, h, to be kept at some fixed value that
is a function of n, the number of entries currently in the map (from the analysis
we see that h = max{10,2⌈log n⌉} is a reasonable choice, and picking h = 3⌈log n⌉
is even safer). Implementing this choice means that we must modify the insertion
algorithm to stop inserting a new position once we reach the top-most level (unless
⌈log n⌉ < ⌈log(n+1)⌉, in which case we can now go at least one more level, since
the bound on the height is increasing).
The other possibility is to let an insertion continue inserting a new position as
long as heads keeps getting returned from the random number generator. This is
the approach taken in Algorithm SkipInsert of Code Fragment 9.22. As we show
in the analysis of skip lists, the probability that an insertion will go to a level that is
more than O(logn) is very low, so this design choice should also work.
Either choice still results in the expected O(logn) time to perform search, in
sertion, and removal, however, which we show in the next section.
9.4.2 A Probabilistic Analysis of Skip Lists ⋆
As we have shown above, skip lists provide a simple implementation of an ordered
map. In terms of worst-case performance, however, skip lists are not a superior
data structure. In fact, if we don’t officially prevent an insertion from continuing
significantly past the current highest level, then the insertion algorithm can go into
what is almost an infinite loop (it is not actually an infinite loop, however, since
the probability of having a fair coin repeatedly come up heads forever is 0). More
over, we cannot infinitely add positions to a list without eventually running out of
memory. In any case, if we terminate position insertion at the highest level h, then
the worst-case running time for performing the find, insert, and erase operations in
a skip list S with n entries and height h is O(n+ h). This worst-case performance
occurs when the tower of every entry reaches level h−1, where h is the height of S.
However, this event has very low probability. Judging from this worst case, we
might conclude that the skip list structure is strictly inferior to the other map im
plementations discussed earlier in this chapter. But this would not be a fair analysis
because this worst-case behavior is a gross overestimate.
Page 433
9.4. Skip Lists409
Bounding the Height of a Skip List
Because the insertion step involves randomization, a more accurate analysis of skip
lists involves a bit of probability. At first, this might seem like a major undertaking,
since a complete and thorough probabilistic analysis could require deep mathemat
ics (and, indeed, there are several such deep analyses that have appeared in the
research literature related to data structures). Fortunately, such an analysis is not
necessary to understand the expected asymptotic behavior of skip lists. The infor
mal and intuitive probabilistic analysis we give below uses only basic concepts of
probability theory.
Let us begin by determining the expected value of the height h of a skip list S
with n entries (assuming that we do not terminate insertions early). The probability
that a given entry has a tower of height i ≥ 1 is equal to the probability of getting i
consecutive heads when flipping a coin, that is, this probability is 1/2i. Hence, the
probability Pi that level i has at least one position is at most
Pi ≤n i2,
because the probability that any one of n different events occurs is at most the sum
of the probabilities that each occurs.
The probability that the height h of S is larger than i is equal to the probability
that level i has at least one position, that is, it is no more than Pi. This means that h
is larger than, say, 3log n with probability at most
P3logn ≤n
23log n
=n 3n=1 2n.
For example, if n = 1000, this probability is a one-in-a-million long shot. More
generally, given a constant c > 1, h is larger than clog n with probability at most
1/nc−1. That is, the probability that h is smaller than clog n is at least 1−1/nc−1.
Thus, with high probability, the height h of S is O(logn).
Analyzing Search Time in a Skip List
Next, consider the running time of a search in skip list S, and recall that such a
search involves two nested while loops. The inner loop performs a scan forward on
a level of S as long as the next key is no greater than the search key k, and the outer
loop drops down to the next level and repeats the scan forward iteration. Since the
height h of S is O(logn) with high probability, the number of drop-down steps is
O(logn) with high probability.
Page 434
410Chapter 9. Hash Tables, Maps, and Skip Lists
So we have yet to bound the number of scan-forward steps we make. Let ni be
the number of keys examined while scanning forward at level i. Observe that, after
the key at the starting position, each additional key examined in a scan-forward at
level i cannot also belong to level i+ 1. If any of these keys were on the previous
level, we would have encountered them in the previous scan-forward step. Thus,
the probability that any key is counted in ni is 1/2. Therefore, the expected value of
ni is exactly equal to the expected number of times we must flip a fair coin before
it comes up heads. This expected value is 2.
Hence, the expected amount of time spent scanning forward at any level i
is O(1). Since S has O(log n) levels with high probability, a search in S takes ex
pected time O(logn). By a similar analysis, we can show that the expected running
time of an insertion or a removal is O(logn).
Space Usage in a Skip List
Finally, let us turn to the space requirement of a skip list S with n entries. As we
observed above, the expected number of positions at level i is n/2i, which means
that the expected total number of positions in S is
h∑i=0n i2= nh∑i=01 2
i2
i.
Using Proposition 4.5 on geometric summations, we have
h∑i=01 2
i2
i = 1 2h+1 −1
12−1= 2·1− 2h1+1  < 2 for all h ≥ 0.
Hence, the expected space requirement of S is O(n).
Table 9.3 summarizes the performance of an ordered map realized by a skip
list.
Operation Time
size, empty O(1)
firstEntry, lastEntry O(1)
find, insert, erase O(log n) (expected)
ceilingEntry, floorEntry, lowerEntry, higherEntry O(log n) (expected)
Table 9.3: Performance of an ordered map implemented with a skip list. We use
n to denote the number of entries in the dictionary at the time the operation is
performed. The expected space requirement is O(n).
Page 435
9.5. Dictionaries411
9.5 Dictionaries
Like a map, a dictionary stores key-value pairs (k,v), which we call entries, where
k is the key and v is the value. Similarly, a dictionary allows for keys and values
to be of any object type. But, whereas a map insists that entries have unique keys,
a dictionary allows for multiple entries to have the same key, much like an English
dictionary, which allows for multiple definitions for the same word.
The ability to store multiple entries with the same key has several applications.
For example, we might want to store records for computer science authors indexed
by their first and last names. Since there are a few cases of different authors with
the same first and last name, there will naturally be some instances where we have
to deal with different entries having equal keys. Likewise, a multi-user computer
game involving players visiting various rooms in a large castle might need a map
ping from rooms to players. It is natural in this application to allow users to be
in the same room simultaneously, however, to engage in battles. Thus, this game
would naturally be another application where it would be useful to allow for multi
ple entries with equal keys.
9.5.1 The Dictionary ADT
The dictionary ADT is quite similar to the map ADT, which was presented in Sec
tion 9.1. The principal differences involve the issue of multiple values sharing a
common key. As with the map ADT, we assume that there is an object, called It
erator, that provides a way to reference entries of the dictionary. There is a special
sentinel value, end, which is used to indicate a nonexistent entry. The iterator may
be incremented from entry to entry, making it possible to enumerate entries from
the collection.
As an ADT, a (unordered) dictionary D supports the following functions:
size(): Return the number of entries in D.
empty(): Return true if D is empty and false otherwise.
find(k): If D contains an entry with key equal to k, then return an
iterator p referring any such entry, else return the special
iterator end.
findAll(k): Return a pair of iterators (b,e), such that all the entries
with key value k lie in the range from b up to, but not
including, e.
insert(k,v): Insert an entry with key k and value v into D, returning
an iterator referring to the newly created entry.
Page 436
412Chapter 9. Hash Tables, Maps, and Skip Lists
erase(k): Remove from D an arbitrary entry with key equal to k;
an error condition occurs if D has no such entry.
erase(p): Remove from D the entry referenced by iterator p; an
error condition occurs if p points to the end sentinel.
begin(): Return an iterator to the first entry of D.
end(): Return an iterator to a position just beyond the end of D.
Note that operation find(k) returns an arbitrary entry, whose key is equal to k, and
erase(k) removes an arbitrary entry with key value k. In order to remove a specific
entry among those having the same key, it would be necessary to remember the
iterator value p returned by insert(k,v), and then use the operation erase(p).
Example 9.2: In the following, we show a series of operations on an initially
empty dictionary storing entries with integer keys and character values. In the
column “Output,” we use the notation pi : [(k,v)] to mean that the operation returns
an iterator denoted by pi that refers to the entry (k,v).
Although the entries are not necessarily stored in any particular order, in order
to implement the operation findAll, we assume that items with the same keys are
stored contiguously. (Alternatively, the operation findAll would need to return a
smarter form of iterator that returns keys of equal value.)
Operation Output Dictionary
empty()true∅
insert(5,A) p1 : [(5,A)] {(5,A)}
insert(7,B) p2 : [(7,B) {(5,A),(7,B)}
insert(2,C) p3 : [(2,C) {(5,A),(7,B),(2,C)}
insert(8,D) p4 : [(8,D) {(5,A),(7,B),(2,C),(8,D)}
insert(2,E) p5 : [(2,E) {(5,A),(7,B),(2,C),(2,E),(8,D)}
find(7)p2 : [(7,B) {(5,A),(7,B),(2,C),(2,E),(8,D)}
find(4)end{(5,A),(7,B),(2,C),(2,E),(8,D)}
find(2)p3 : [(2,C) {(5,A),(7,B),(2,C),(2,E),(8,D)}
findAll(2)(p3, p4) {(5,A),(7,B),(2,C),(2,E),(8,D)}
size()5{(5,A),(7,B),(2,C),(2,E),(8,D)}
erase(5)–{(7,B),(2,C),(2,E),(8,D)}
erase(p3)–{(7,B),(2,E),(8,D)}
find(2)p5 : [(2,E)] {(7,B),(2,E),(8,D)}
The operation findAll(2) returns the iterator pair (p3, p4), referring to the en
tries (2,C) and (8,D). Assuming that the entries are stored in the order listed
above, iterating from p3 up to, but not including, p4, would enumerate the entries
{(2,C),(2,E)}.
Page 437
9.5. Dictionaries413
9.5.2 A C++ Dictionary Implementation
In this Section, we describe a C++ implementation of the dictionary ADT. Our
implementation, called HashDict, is a subclass of the HashMap class, from Sec
tion 9.2.7. The map ADT already includes most of the functions of the dictionary
ADT. Our HashDict class implements the new function insert, which inserts a key
value pair, and the function findAll, which generates an iterator range for all the
values equal to a given key. All the other functions are inherited from HashMap.
In order to support the return type of findAll, we define a nested class called
Range. It is presented in Code Fragment 9.23. This simple class stores a pair of
objects of type Iterator, a constructor, and two member functions for accessing each
of them. This definition will be nested inside the public portion of the HashMap
class definition.
class Range {// an iterator range
private:
Iterator begin;// front of range
Iterator end;// end of range
public:
Range(const Iterator& b, const Iterator& e)// constructor
: begin(b), end(e) { }
Iterator& begin() { return begin; }// get beginning
Iterator& end() { return end; }// get end
};
Code Fragment 9.23: Definition of the Range class to be added to HashMap.
The HashDict class definition is presented in Code Fragment 9.24. As indicated
in the first line of the declaration, this is a subclass of HashMap. The class begins
with type definitions for the Iterator and Entry types from the base class. This is
followed by the code for class Range from Code Fragment 9.23, and the public
function declarations.
template <typename K, typename V, typename H>
class HashDict : public HashMap<K,V,H> {
public:// public functions
typedef typename HashMap<K,V,H>::Iterator Iterator;
typedef typename HashMap<K,V,H>::Entry Entry;
// . . .insert Range class declaration here
public:// public functions
HashDict(int capacity = 100);// constructor
Range findAll(const K& k);// find all entries with k
Iterator insert(const K& k, const V& v);// insert pair (k,v)
};
Code Fragment 9.24: The class HashDict, which implements the dictionary ADT.
Page 438
414Chapter 9. Hash Tables, Maps, and Skip Lists
Observe that, when referring to the parent class, HashMap, we need to specify
its template parameters. To avoid the need for continually repeating these parame
ters, we have provided type definitions for the iterator and entry classes. Because
most of the dictionary ADT functions are already provided by HashMap, we need
only provide a constructor and the missing dictionary ADT functions.
The constructor definition is presented in Code Fragment 9.25. It simply in
vokes the constructor for the base class. Note that we employ the condensed func
tion notation that we introduced in Section 9.2.7.
/* HashDict〈K,V,H〉 :: */// constructor
HashDict(int capacity) : HashMap<K,V,H>(capacity) { }
Code Fragment 9.25: The class HashDict constructor.
In Code Fragment 9.26, we present an implementation of the function insert. It
first locates the key by invoking the finder utility (see Code Fragment 9.15). Recall
that this utility returns an iterator to an entry containing this key, if found, and
otherwise it returns an iterator to the end of the bucket. In either case, we insert the
new entry immediately prior to this location by invoking the inserter utility. (See
Code Fragment 9.16.) An iterator referencing the resulting location is returned.
/* HashDict〈K,V,H〉 :: */// insert pair (k,v)
Iterator insert(const K& k, const V& v) {
Iterator p = finder(k);// find key
Iterator q = inserter(p, Entry(k, v));// insert it here
return q;// return its position
}
Code Fragment 9.26: An implementation of the dictionary function insert.
We exploit a property of how insert works. Whenever a new entry (k,v) is
inserted, if the structure already contains another entry (k,v′) with the same key, the
finder utility function returns an iterator to the first such occurrence. The inserter
utility then inserts the new entry just prior to this. It follows that all the entries
having the same key are stored in a sequence of contiguous positions, all within
the same bucket. (In fact, they appear in the reverse of their insertion order.) This
means that, in order to produce an iterator range (b,e) for the call findAll(k), it
suffices to set b to the first entry of this sequence and set e to the entry immediately
following the last one.
Our implementation of findAll is given in Code Fragment 9.27. We first invoke
the finder function to locate the key. If the finder returns a position at the end of
some bucket, we know that the key is not present, and we return the empty iterator
(end,end). Otherwise, recall from Code Fragment 9.15 that finder returns the first
entry with the given key value. We store this in the entry iterator b. We then traverse
Page 439
9.5. Dictionaries415
the bucket until either coming to the bucket’s end or encountering an entry with a
key of different value. Let p be this iterator value. We return the iterator range
(b, p).
/* HashDict〈K,V,H〉 :: */// find all entries with k
Range findAll(const K& k) {
Iterator b = finder(k);// look up k
Iterator p = b;
while (!endOfBkt(p) && (*p).key() == (*b).key()) { // find next unequal key
++p;
}
return Range(b, p);// return range of positions
}
Code Fragment 9.27: An implementation of the dictionary function findAll.
9.5.3 Implementations with Location-Aware Entries
As with the map ADT, there are several possible ways we can implement the dic
tionary ADT, including an unordered list, a hash table, an ordered search table, or
a skip list. As we did for adaptable priority queues (Section 8.4.2), we can also
use location-aware entries to speed up the running time for some operations in a
dictionary. In removing a location-aware entry e, for instance, we could simply
go directly to the place in our data structure where we are storing e and remove
it. We could implement a location-aware entry, for example, by augmenting our
entry class with a private location variable and protected functions location() and
setLocation(p), which return and set this variable respectively. We would then re
quire that the location variable for an entry e would always refer to e’s position or
index in the data structure. We would, of course, have to update this variable any
time we moved an entry, as follows.
• Unordered list: In an unordered list, L, implementing a dictionary, we can
maintain the location variable of each entry e to point to e’s position in the
underlying linked list for L. This choice allows us to perform erase(e) as
L.erase(e.location()), which would run in O(1) time.
• Hash table with separate chaining: Consider a hash table, with bucket array
A and hash function h, that uses separate chaining for handling collisions.
We use the location variable of each entry e to point to e’s position in the list
L implementing the list A[h(k)]. This choice allows us to perform an erase(e)
as L.erase(e.location()), which would run in constant expected time.
• Ordered search table: In an ordered table, T , implementing a dictionary, we
should maintain the location variable of each entry e to be e’s index in T .
This choice would allow us to perform erase(e) as T.erase(e.location()).
Page 440
416Chapter 9. Hash Tables, Maps, and Skip Lists
(Recall that location() now returns an integer.) This approach would run fast
if entry e was stored near the end of T .
• Skip list: In a skip list, S, implementing a dictionary, we should maintain the
location variable of each entry e to point to e’s position in the bottom level
of S. This choice would allow us to skip the search step in our algorithm for
performing erase(e) in a skip list.
We summarize the performance of entry removal in a dictionary with location
aware entries in Table 9.4.
List Hash Table Search TableSkip List
O(1) O(1) (expected)O(n)O(logn) (expected)
Table 9.4: Performance of the erase function in dictionaries implemented with
location-aware entries. We use n to denote the number of entries in the dictionary.
Page 447
Chapter
10Search Trees
Contents
10.1 Binary Search Trees . . . . . . . . . . . . . . . . . . . 424
10.1.1 Searching . . . . . . . . . . . . . . . . . . . . . . . 426
10.1.2 Update Operations . . . . . . . . . . . . . . . . . . 428
10.1.3 C++ Implementation of a Binary Search Tree . . . . 432
10.2 AVL Trees . . . . . . . . . . . . . . . . . . . . . . . . 438
10.2.1 Update Operations . . . . . . . . . . . . . . . . . . 440
10.2.2 C++ Implementation of an AVL Tree . . . . . . . . . 446
10.3 Splay Trees . . . . . . . . . . . . . . . . . . . . . . . . 450
10.3.1 Splaying . . . . . . . . . . . . . . . . . . . . . . . . 450
10.3.2 When to Splay . . . . . . . . . . . . . . . . . . . . . 454
10.3.3 Amortized Analysis of Splaying ⋆ . . . . . . . . . . 456
10.4 (2,4) Trees . . . . . . . . . . . . . . . . . . . . . . . . 461
10.4.1 Multi-Way Search Trees . . . . . . . . . . . . . . . . 461
10.4.2 Update Operations for (2,4) Trees . . . . . . . . . . 467
10.5 Red-Black Trees . . . . . . . . . . . . . . . . . . . . . 473
10.5.1 Update Operations . . . . . . . . . . . . . . . . . . 475
10.5.2 C++ Implementation of a Red-Black Tree . . . . . . 488
10.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . 492
Page 448
424Chapter 10. Search Trees
10.1 Binary Search Trees
All of the structures that we discuss in this chapter are search trees, that is, tree data
structures that can be used to implement ordered maps and ordered dictionaries.
Recall from Chapter 9 that a map is a collection of key-value entries, with each
value associated with a distinct key. A dictionary differs in that multiple values
may share the same key value. Our presentation focuses mostly on maps, but we
consider both data structures in this chapter.
We assume that maps and dictionaries provide a special pointer object, called an
iterator, which permits us to reference and enumerate the entries of the structure. In
order to indicate that an object is not present, there exists a special sentinel iterator
called end. By convention, this sentinel refers to an imaginary element that lies just
beyond the last element of the structure.
Let M be a map. In addition to the standard container operations (size, empty,
begin, and end) the map ADT (Section 9.1) includes the following:
find(k): If M contains an entry e = (k,v), with key equal to k, then
return an iterator p referring to this entry, and otherwise
return the special iterator end.
put(k,v): If M does not have an entry with key equal to k, then
add entry (k,v) to M, and otherwise, replace the value
field of this entry with v; return an iterator to the in
serted/modified entry.
erase(k): Remove from M the entry with key equal to k; an error
condition occurs if M has no such entry.
erase(p): Remove from M the entry referenced by iterator p; an
error condition occurs if p points to the end sentinel.
begin(): Return an iterator to the first entry of M.
end(): Return an iterator to a position just beyond the end of M.
The dictionary ADT (Section 9.5) provides the additional operations insert(k,v),
which inserts the entry (k,v), and findAll(k), which returns an iterator range (b,e)
of all entries whose key value is k.
Given an iterator p, the associated entry may be accessed using *p. The indi
vidual key and value can be accessed using p->key() and p->value(), respectively.
We assume that the key elements are drawn from a total order, which is defined
by overloading the C++ relational less-than operator (“<”). Given an iterator p to
some entry, it may be advanced to the next entry in this order using the increment
operator (“++p”).
The ordered map and dictionary ADTs also include some additional functions
for finding predecessor and successor entries with respect to a given key, but their
Page 449
10.1. Binary Search Trees425
performance is similar to that of find. So, we focus on find as the primary search
operation in this chapter.
Binary Search Trees and Ordered Maps
Binary trees are an excellent data structure for storing the entries of a map, as
suming we have an order relation defined on the keys. As mentioned previously
(Section 7.3.6), a binary search tree is a binary tree T such that each internal node
v of T stores an entry (k,x) such that:
• Keys stored at nodes in the left subtree of v are less than or equal to k
• Keys stored at nodes in the right subtree of v are greater than or equal to k.
An example of a search tree storing integer keys is shown in Figure 10.1.
44
17
32
28
295465
82
76
809788
Figure 10.1: A binary search tree T representing a map with integer keys.
As we show below, the keys stored at the nodes of T provide a way of per
forming a search by making comparisons at a series of internal nodes. The search
can stop at the current node v or continue at v’s left or right child. Thus, we take
the view here that binary search trees are nonempty proper binary trees. That is,
we store entries only at the internal nodes of a binary search tree, and the external
nodes serve as “placeholders.” This approach simplifies several of our search and
update algorithms. Incidentally, we could allow for improper binary search trees,
which have better space usage, but at the expense of more complicated search and
update functions.
Independent of whether we view binary search trees as proper or not, the im
portant property of a binary search tree is the realization of an ordered map (or
dictionary). That is, a binary search tree should hierarchically represent an order
ing of its keys, using relationships between parent and children. Specifically, an
inorder traversal (Section 7.3.6) of the nodes of a binary search tree T should visit
the keys in nondecreasing order. Incrementing an iterator through a map visits the
entries in this same order.
Page 450
426Chapter 10. Search Trees
10.1.1 Searching
To perform operation find(k) in a map M that is represented with a binary search
tree T , we view the tree T as a decision tree (recall Figure 7.10). In this case, the
question asked at each internal node v of T is whether the search key k is less than,
equal to, or greater than the key stored at node v, denoted with key(v). If the answer
is “smaller,” then the search continues in the left subtree. If the answer is “equal,”
then the search terminates successfully. If the answer is “greater,” then the search
continues in the right subtree. Finally, if we reach an external node, then the search
terminates unsuccessfully. (See Figure 10.2.)
Figure 10.2: (a) A binary search tree T representing a map with integer keys; (b)
nodes of T visited when executing operations find(76) (successful) and find(25)
(unsuccessful) on M. For simplicity, we show only the keys of the entries.
We describe this approach in detail in Code Fragment 10.1. Given a search key
k and a node v of T , this function, TreeSearch, returns a node (position) w of the
subtree T(v) of T rooted at v, such that one of the following occurs:
• w is an internal node and w’s entry has key equal to k
• w is an external node representing k’s proper place in an inorder traversal of
T(v), but k is not a key contained in T(v)
Thus, function find(k) can be performed by calling TreeSearch(k,T.root()). Let w
be the node of T returned by this call. If w is an internal node, then we return w’s
entry; otherwise, we return null.
Algorithm TreeSearch(k,v):
if T.isExternal(v) then
return v
if k < key(v) then
return TreeSearch(k,T.left(v))
else if k > key(v) then
return TreeSearch(k,T.right(v))
return v{we know k = key(v)}
Code Fragment 10.1: Recursive search in a binary search tree.
Page 451
10.1. Binary Search Trees427
Analysis of Binary Tree Searching
The analysis of the worst-case running time of searching in a binary search tree T
is simple. Algorithm TreeSearch is recursive and executes a constant number of
primitive operations for each recursive call. Each recursive call of TreeSearch is
made on a child of the previous node. That is, TreeSearch is called on the nodes
of a path of T that starts at the root and goes down one level at a time. Thus, the
number of such nodes is bounded by h + 1, where h is the height of T. In other
words, since we spend O(1) time per node encountered in the search, function find
on map M runs in O(h) time, where h is the height of the binary search tree T used
to implement M. (See Figure 10.3.)
Figure 10.3: The running time of searching in a binary search tree. We use a stan
dard visualization shortcut of viewing a binary search tree as a big triangle and a
path from the root as a zig-zag line.
We can also show that a variation of the above algorithm performs operation
findAll(k) of the dictionary ADT in time O(h+s), where s is the number of entries
returned. However, this function is slightly more complicated, and the details are
left as an exercise (Exercise C-10.2).
Admittedly, the height h of T can be as large as the number of entries, n, but
we expect that it is usually much smaller. Indeed, we show how to maintain an
upper bound of O(logn) on the height of a search tree T in Section 10.2. Before we
describe such a scheme, however, let us describe implementations for map update
functions.
Page 452
428Chapter 10. Search Trees
10.1.2 Update Operations
Binary search trees allow implementations of the insert and erase operations using
algorithms that are fairly straightforward, but not trivial.
Insertion
Let us assume a proper binary tree T supports the following update operation:
insertAtExternal(v,e): Insert the element e at the external node v, and expand
v to be internal, having new (empty) external node chil
dren; an error occurs if v is an internal node.
Given this function, we perform insert(k,x) for a dictionary implemented with a
binary search tree T by calling TreeInsert(k,x,T.root()), which is given in Code
Fragment 10.2.
Algorithm TreeInsert(k,x,v):
Input: A search key k, an associated value, x, and a node v of T
Output: A new node w in the subtree T(v) that stores the entry (k,x)
w ← TreeSearch(k,v)
if T.isInternal(w) then
return TreeInsert(k,x,T.left(w)) {going to the right would be correct too}
T.insertAtExternal(w,(k,x)){this is an appropriate place to put (k,x)}
return w
Code Fragment 10.2: Recursive algorithm for insertion in a binary search tree.
This algorithm traces a path from T’s root to an external node, which is ex
panded into a new internal node accommodating the new entry. An example of
insertion into a binary search tree is shown in Figure 10.4.
Figure 10.4: Insertion of an entry with key 78 into the search tree of Figure 10.1:
(a) finding the position to insert; (b) the resulting tree.
Page 453
10.1. Binary Search Trees429
Removal
The implementation of the erase(k) operation on a map M implemented with a
binary search tree T is a bit more complex, since we do not wish to create any
“holes” in the tree T . We assume, in this case, that a proper binary tree supports
the following additional update operation:
removeAboveExternal(v): Remove an external node v and its parent, replacing v’s
parent with v’s sibling; an error occurs if v is not external.
Given this operation, we begin our implementation of operation erase(k) of
the map ADT by calling TreeSearch(k,T.root()) on T to find a node of T storing
an entry with key equal to k. If TreeSearch returns an external node, then there
is no entry with key k in map M, and an error condition is signaled. If, instead,
TreeSearch returns an internal node w, then w stores an entry we wish to remove,
and we distinguish two cases:
• If one of the children of node w is an external node, say node z, we simply re
move w and z from T by means of operation removeAboveExternal(z) on T .
This operation restructures T by replacing w with the sibling of z, removing
both w and z from T . (See Figure 10.5.)
• If both children of node w are internal nodes, we cannot simply remove the
node w from T , since this would create a “hole” in T . Instead, we proceed as
follows (see Figure 10.6):
◦ We find the first internal node y that follows w in an inorder traversal
of T . Node y is the left-most internal node in the right subtree of w,
and is found by going first to the right child of w and then down T
from there, following the left children. Also, the left child x of y is the
external node that immediately follows node w in the inorder traversal
of T .
◦ We move the entry of y into w. This action has the effect of removing
the former entry stored at w.
◦ We remove nodes x and y from T by calling removeAboveExternal(x)
on T . This action replaces y with x’s sibling, and removes both x and y
from T .
As with searching and insertion, this removal algorithm traverses a path from
the root to an external node, possibly moving an entry between two nodes of this
path, and then performs a removeAboveExternal operation at that external node.
The position-based variant of removal is the same, except that we can skip the
initial step of invoking TreeSearch(k,T.root()) to locate the node containing the
key.
Page 454
430Chapter 10. Search Trees
Figure 10.5: Removal from the binary search tree of Figure 10.4b, where the entry
to remove (with key 32) is stored at a node (w) with an external child: (a) before
the removal; (b) after the removal.
Figure 10.6: Removal from the binary search tree of Figure 10.4b, where the entry
to remove (with key 65) is stored at a node (w) whose children are both internal:
(a) before the removal; (b) after the removal.
Page 455
10.1. Binary Search Trees431
Performance of a Binary Search Tree
The analysis of the search, insertion, and removal algorithms are similar. We spend
O(1) time at each node visited, and, in the worst case, the number of nodes visited
is proportional to the height h of T. Thus, in a map M implemented with a binary
search tree T, the find, insert, and erase functions run in O(h) time, where h is the
height of T. Thus, a binary search tree T is an efficient implementation of a map
with n entries only if the height of T is small. In the best case, T has height h =
⌈log(n+1)⌉, which yields logarithmic-time performance for all the map operations.
In the worst case, however, T has height n, in which case it would look and feel like
an ordered list implementation of a map. Such a worst-case configuration arises,
for example, if we insert a series of entries with keys in increasing or decreasing
order. (See Figure 10.7.)
10
20
30
40
Figure 10.7: Example of a binary search tree with linear height, obtained by insert
ing entries with keys in increasing order.
The performance of a map implemented with a binary search tree is summa
rized in the following proposition and in Table 10.1.
Proposition 10.1: A binary search tree T with height h for n key-value entries
uses O(n) space and executes the map ADT operations with the following running
times. Operations size and empty each take O(1) time. Operations find, insert, and
erase each take O(h) time.
Operation Time
size, empty O(1)
find, insert, erase O(h)
Table 10.1: Running times of the main functions of a map realized by a binary
search tree. We denote the current height of the tree with h. The space usage
is O(n), where n is the number of entries stored in the map.
Note that the running time of search and update operations in a binary search
tree varies dramatically depending on the tree’s height. We can nevertheless take
Page 456
432Chapter 10. Search Trees
comfort that, on average, a binary search tree with n keys generated from a ran
dom series of insertions and removals of keys has expected height O(logn). Such a
statement requires careful mathematical language to precisely define what we mean
by a random series of insertions and removals, and sophisticated probability theory
to prove; hence, its justification is beyond the scope of this book. Nevertheless,
keep in mind the poor worst-case performance and take care in using standard bi
nary search trees in applications where updates are not random. There are, after
all, applications where it is essential to have a map with fast worst-case search and
update times. The data structures presented in the next sections address this need.
10.1.3 C++ Implementation of a Binary Search Tree
In this section, we present a C++ implementation of the dictionary ADT based on
a binary search tree, which we call SearchTree. Recall that a dictionary differs
from a map in that it allows multiple copies of the same key to be inserted. For
simplicity, we have not implemented the findAll function.
To keep the number of template parameters small, rather than templating our
class on the key and value types, we have chosen instead to template our binary
search tree on just the entry type denoted E. To obtain access to the key and value
types, we assume that the entry class defines two public types defining them. Given
an entry object of type E, we may access these types E::Key and E::Value. Oth
erwise, our entry class is essentially the same as the entry class given in Code
Fragment 9.1. It is presented in Code Fragment 10.3.
template <typename K, typename V>
class Entry {// a (key, value) pair
public:// public types
typedef K Key;// key type
typedef V Value;// value type
public:// public functions
Entry(const K& k = K(), const V& v = V()) // constructor
: key(k), value(v) { }
const K& key() const { return key; }// get key (read only)
const V& value() const { return value; }// get value (read only)
void setKey(const K& k) { key = k; }// set key
void setValue(const V& v) { value = v; }// set value
private:// private data
K key;// key
V value;// value
};
Code Fragment 10.3: A C++ class for a key-value entry.
Page 457
10.1. Binary Search Trees433
In Code Fragment 10.4, we present the main parts of the class definition for
our binary search tree. We begin by defining the publicly accessible types for the
entry, key, value, and the class iterator. This is followed by a declaration of the
public member functions. We define two local types, BinaryTree and TPos, which
represent a binary search tree and position within this binary tree, respectively.
We also declare a number of local utility functions to help in finding, inserting,
and erasing entries. The member data consists of a binary tree and the number of
entries in the tree.
template <typename E>
class SearchTree {// a binary search tree
public:// public types
typedef typename E::Key K;// a key
typedef typename E::Value V;// a value
class Iterator;// an iterator/position
public:// public functions
SearchTree();// constructor
int size() const;// number of entries
bool empty() const;// is the tree empty?
Iterator find(const K& k);// find entry with key k
Iterator insert(const K& k, const V& x);// insert (k,x)
void erase(const K& k) throw(NonexistentElement); // remove key k entry
void erase(const Iterator& p);// remove entry at p
Iterator begin();// iterator to first entry
Iterator end();// iterator to end entry
protected:// local utilities
typedef BinaryTree<E> BinaryTree;// linked binary tree
typedef typename BinaryTree::Position TPos;// position in the tree
TPos root() const;// get virtual root
TPos finder(const K& k, const TPos& v);// find utility
TPos inserter(const K& k, const V& x);// insert utility
TPos eraser(TPos& v);// erase utility
TPos restructure(const TPos& v)// restructure
throw(BoundaryViolation);
private:// member data
BinaryTree T;// the binary tree
int n;// number of entries
public:
// . . .insert Iterator class declaration here
};
Code Fragment 10.4: Class SearchTree, which implements a binary search tree.
We have omitted the definition of the iterator class for our binary search tree.
This is presented in Code Fragment 10.5. An iterator consists of a single position
in the tree. We overload the dereferencing operator (“*”) to provide both read
Page 458
434Chapter 10. Search Trees
only and read-write access to the node referenced by the iterator. We also provide
an operator for checking the equality of two iterators. This is useful for checking
whether an iterator is equal to end.
class Iterator {// an iterator/position
private:
TPos v;// which entry
public:
Iterator(const TPos& vv) : v(vv) { }// constructor
const E& operator*() const { return *v; } // get entry (read only)
E& operator*() { return *v; }// get entry (read/write)
bool operator==(const Iterator& p) const // are iterators equal?
{ return v == p.v; }
Iterator& operator++();// inorder successor
friend class SearchTree;// give search tree access
};
Code Fragment 10.5: Declaration of the Iterator class, which is part of SearchTree.
Code Fragment 10.6 presents the definition of the iterator’s increment operator,
which advances the iterator from a given position of the tree to its inorder successor.
Only internal nodes are visited, since external nodes do not contain entries. If the
node v has a right child, the inorder successor is the leftmost internal node of its
right subtree. Otherwise, v must be the largest key in the left subtree of some node
w. To find w, we walk up the tree through successive ancestors. As long as we are
the right child of the current ancestor, we continue to move upwards. When this is
no longer true, the parent is the desired node w. Note that we employ the condensed
function notation, which we introduced in Section 9.2.7, where the messy scoping
qualifiers involving SearchTree have been omitted.
/* SearchTree〈E〉 :: */// inorder successor
Iterator& Iterator::operator++() {
TPos w = v.right();
if (w.isInternal()) {// have right subtree?
do { v = w; w = w.left(); }// move down left chain
while (w.isInternal());
} else {
w = v.parent();// get parent
while (v == w.right())// move up right chain
{ v = w; w = w.parent(); }
v = w;// and first link to left
}
return *this;
}
Code Fragment 10.6: The increment operator (“++”) for Iterator.
Page 459
10.1. Binary Search Trees435
The implementation of the increment operator appears to contain an obvious
bug. If the iterator points to the rightmost node of the entire tree, then the above
function would loop until arriving at the root, which has no parent. The rightmost
node of the tree has no successor, so the iterator should return the value end.
There is a simple and elegant way to achieve the desired behavior. We add a
special sentinel node to our tree, called the super root, which is created when the
initial tree is constructed. The root of the binary search tree, which we call the
virtual root, is made the left child of the super root. We define end to be an iterator
that returns the position of the super root. Observe that, if we attempt to increment
an iterator that points to the rightmost node of the tree, the function given in Code
Fragment 10.6 moves up the right chain until reaching the virtual root, and then
stops at its parent, the super root, since the virtual root is its left child. Therefore,
it returns an iterator pointing to the super root, which is equivalent to end. This is
exactly the behavior we desire.
To implement this strategy, we define the constructor to create the super root.
We also define a function root, which returns the virtual root’s position, that is, the
left child of the super root. These functions are given in Code Fragment 10.7.
/* SearchTree〈E〉 :: */// constructor
SearchTree() : T(), n(0)
{ T.addRoot(); T.expandExternal(T.root()); }// create the super root
/* SearchTree〈E〉 :: */// get virtual root
TPos root() const
{ return T.root().left(); }// left child of super root
Code Fragment 10.7: The constructor and the utility function root. The constructor
creates the super root, and root returns the virtual root of the binary search tree.
Next, in Code Fragment 10.8, we define the functions begin and end. The
function begin returns the first node according to an inorder traversal, which is the
leftmost internal node. The function end returns the position of the super root.
/* SearchTree〈E〉 :: */// iterator to first entry
Iterator begin() {
TPos v = root();// start at virtual root
while (v.isInternal()) v = v.left();// find leftmost node
return Iterator(v.parent());
}
/* SearchTree〈E〉 :: */// iterator to end entry
Iterator end()
{ return Iterator(T.root()); }// return the super root
Code Fragment 10.8: The begin and end functions of class SearchTree. The func
tion end returns a pointer to the super root.
Page 460
436Chapter 10. Search Trees
We are now ready to present implementations of the principal class functions,
for finding, inserting, and removing entries. We begin by presenting the func
tion find(k) in Code Fragment 10.9. It invokes the recursive utility function finder
starting at the root. This utility function is based on the algorithm given in Code
Fragment 10.1. The code has been structured so that only the less-than operator
needs to be defined on keys.
/* SearchTree〈E〉 :: */// find utility
TPos finder(const K& k, const TPos& v) {
if (v.isExternal()) return v;// key not found
if (k < v−>key()) return finder(k, v.left());// search left subtree
else if (v−>key() < k) return finder(k, v.right()); // search right subtree
else return v;// found it here
}
/* SearchTree〈E〉 :: */// find entry with key k
Iterator find(const K& k) {
TPos v = finder(k, root());// search from virtual root
if (v.isInternal()) return Iterator(v);// found it
else return end();// didn’t find it
}
Code Fragment 10.9: The functions of SearchTree related to finding keys.
The insertion functions are shown in Code Fragment 10.10. The inserter utility
does all the work. First, it searches for the key. If found, we continue to search until
reaching an external node. (Recall that we allow duplicate keys.) We then create
a node, copy the entry information into this node, and update the entry count. The
insert function simply invokes the inserter utility, and converts the resulting node
position into an iterator.
/* SearchTree〈E〉 :: */// insert utility
TPos inserter(const K& k, const V& x) {
TPos v = finder(k, root());// search from virtual root
while (v.isInternal())// key already exists?
v = finder(k, v.right());// look further
T.expandExternal(v);// add new internal node
v−>setKey(k); v−>setValue(x);// set entry
n++;// one more entry
return v;// return insert position
}
/* SearchTree〈E〉 :: */// insert (k,x)
Iterator insert(const K& k, const V& x)
{ TPos v = inserter(k, x); return Iterator(v); }
Code Fragment 10.10: The functions of SearchTree for inserting entries.
Page 461
10.1. Binary Search Trees437
Finally, we present the removal functions in Code Fragment 10.11. We imple
ment the approach presented in Section 10.1.2. If the node has an external child,
we set w to point to this child. Otherwise, we let w be the leftmost external node in
v’s right subtree. Let u be w’s parent. We copy u’s entry contents to v. In all cases,
we then remove the external node w and its parent through the use of the binary
tree functions removeAboveExternal.
/* SearchTree〈E〉 :: */// remove utility
TPos eraser(TPos& v) {
TPos w;
if (v.left().isExternal()) w = v.left();// remove from left
else if (v.right().isExternal()) w = v.right();// remove from right
else {// both internal?
w = v.right();// go to right subtree
do { w = w.left(); } while (w.isInternal());// get leftmost node
TPos u = w.parent();
v−>setKey(u−>key()); v−>setValue(u−>value()); // copy w’s parent to v
} n−−;// one less entry
return T.removeAboveExternal(w);// remove w and parent
}
/* SearchTree〈E〉 :: */// remove key k entry
void erase(const K& k) throw(NonexistentElement) {
TPos v = finder(k, root());// search from virtual root
if (v.isExternal())// not found?
throw NonexistentElement("Erase of nonexistent");
eraser(v);// remove it
}
/* SearchTree〈E〉 :: */// erase entry at p
void erase(const Iterator& p)
{ eraser(p.v); }
Code Fragment 10.11: The functions of SearchTree involved with removing entries.
When updating node entries (in inserter and eraser), we explicitly change only
the key and value (using setKey and setValue). You might wonder, what else is
there to change? Later in this chapter, we present data structures that are based
on modifying the Entry class. It is important that only the key and value data are
altered when copying nodes for these structures.
Our implementation has focused on the main elements of the binary search tree
implementation. There are a few more things that could have been included. It is
a straightforward exercise to implement the dictionary operation findAll. It would
also be worthwhile to implement the decrement operator (“– –”), which moves an
iterator to its inorder predecessor.
Page 462
438Chapter 10. Search Trees
10.2 AVL Trees
In the previous section, we discussed what should be an efficient map data struc
ture, but the worst-case performance it achieves for the various operations is linear
time, which is no better than the performance of list- and array-based map imple
mentations (such as the unordered lists and search tables discussed in Chapter 9).
In this section, we describe a simple way of correcting this problem in order to
achieve logarithmic time for all the fundamental map operations.
Definition of an AVL Tree
The simple correction is to add a rule to the binary search tree definition that main
tains a logarithmic height for the tree. The rule we consider in this section is the
following height-balance property, which characterizes the structure of a binary
search tree T in terms of the heights of its internal nodes (recall from Section 7.2.1
that the height of a node v in a tree is the length of the longest path from v to an
external node):
Height-Balance Property: For every internal node v of T, the heights of the chil
dren of v differ by at most 1.
Any binary search tree T that satisfies the height-balance property is said to be an
AVL tree, named after the initials of its inventors, Adel’son-Vel’skii and Landis.
An example of an AVL tree is shown in Figure 10.8.
Figure 10.8: An example of an AVL tree. The keys of the entries are shown inside
the nodes, and the heights of the nodes are shown next to the nodes.
An immediate consequence of the height-balance property is that a subtree of an
AVL tree is itself an AVL tree. The height-balance property has also the important
consequence of keeping the height small, as shown in the following proposition.
Page 463
10.2. AVL Trees439
Proposition 10.2: The height of an AVL tree storing n entries is O(logn).
Justification: Instead of trying to find an upper bound on the height of an AVL
tree directly, it turns out to be easier to work on the “inverse problem” of finding a
lower bound on the minimum number of internal nodes n(h) of an AVL tree with
height h. We show that n(h) grows at least exponentially. From this, it is an easy
step to derive that the height of an AVL tree storing n entries is O(logn).
To start with, notice that n(1) = 1 and n(2) = 2, because an AVL tree of height
1 must have at least one internal node and an AVL tree of height 2 must have at least
two internal nodes. Now, for h ≥ 3, an AVL tree with height h and the minimum
number of nodes is such that both its subtrees are AVL trees with the minimum
number of nodes: one with height h−1 and the other with height h−2. Taking the
root into account, we obtain the following formula that relates n(h) to n(h−1) and
n(h−2), for h ≥ 3:
n(h) = 1 + n(h−1)+ n(h−2).(10.1)
At this point, the reader familiar with the properties of Fibonacci progressions (Sec
tion 2.2.3 and Exercise C-4.17) already sees that n(h) is a function exponential in h.
For the rest of the readers, we will proceed with our reasoning.
Formula 10.1 implies that n(h) is a strictly increasing function of h. Thus, we
know that n(h− 1) > n(h− 2). Replacing n(h− 1) with n(h− 2) in Formula 10.1
and dropping the 1, we get, for h ≥ 3,
n(h) > 2·n(h−2).(10.2)
Formula 10.2 indicates that n(h) at least doubles each time h increases by 2, which
intuitively means that n(h) grows exponentially. To show this fact in a formal way,
we apply Formula 10.2 repeatedly, yielding the following series of inequalities:
n(h) > 2·n(h−2)
> 4·n(h−4)
> 8·n(h−6)
...
> 2i ·n(h−2i).(10.3)
That is, n(h) > 2i·n(h−2i), for any integer i, such that h−2i ≥ 1. Since we already
know the values of n(1) and n(2), we pick i so that h−2i is equal to either 1 or 2.
That is, we pick
i = h2−1.
Page 464
440Chapter 10. Search Trees
By substituting the above value of i in formula 10.3, we obtain, for h ≥ 3,
n(h) > 2⌈h2⌉−1 ·nh−2h 2+ 2
≥ 2⌈h 2⌉−1n(1)
≥ 2h2−1.(10.4)
By taking logarithms of both sides of formula 10.4, we obtain
logn(h) > h
22−1,
from which we get
h < 2logn(h) + 2,(10.5)
which implies that an AVL tree storing n entries has height at most 2logn+ 2.
By Proposition 10.2 and the analysis of binary search trees given in Section 10.1,
the operation find, in a map implemented with an AVL tree, runs in time O(logn),
where n is the number of entries in the map. Of course, we still have to show how
to maintain the height-balance property after an insertion or removal.
10.2.1 Update Operations
The insertion and removal operations for AVL trees are similar to those for binary
search trees, but with AVL trees we must perform additional computations.
Insertion
An insertion in an AVL tree T begins as in an insert operation described in Sec
tion 10.1.2 for a (simple) binary search tree. Recall that this operation always
inserts the new entry at a node w in T that was previously an external node, and
it makes w become an internal node with operation insertAtExternal. That is, it
adds two external node children to w. This action may violate the height-balance
property, however, for some nodes increase their heights by one. In particular, node
w, and possibly some of its ancestors, increase their heights by one. Therefore, let
us describe how to restructure T to restore its height balance.
Given a binary search tree T, we say that an internal node v of T is balanced
if the absolute value of the difference between the heights of the children of v is
at most 1, and we say that it is unbalanced otherwise. Thus, the height-balance
property characterizing AVL trees is equivalent to saying that every internal node
is balanced.
Suppose that T satisfies the height-balance property, and hence is an AVL tree,
prior to our inserting the new entry. As we have mentioned, after performing the
Page 465
10.2. AVL Trees441
operation insertAtExternal on T, the heights of some nodes of T, including w,
increase. All such nodes are on the path of T from w to the root of T, and these are
the only nodes of T that may have just become unbalanced. (See Figure 10.9(a).)
Of course, if this happens, then T is no longer an AVL tree; hence, we need a
mechanism to fix the “unbalance” that we have just caused.
Figure 10.9: An example insertion of an entry with key 54 in the AVL tree of
Figure 10.8: (a) after adding a new node for key 54, the nodes storing keys 78
and 44 become unbalanced; (b) a trinode restructuring restores the height-balance
property. We show the heights of nodes next to them, and we identify the nodes x,
y, and z participating in the trinode restructuring.
We restore the balance of the nodes in the binary search tree T by a simple
“search-and-repair” strategy. In particular, let z be the first node we encounter in go
ing up from w toward the root of T such that z is unbalanced. (See Figure 10.9(a).)
Also, let y denote the child of z with higher height (and note that node y must be
an ancestor of w). Finally, let x be the child of y with higher height (there cannot
be a tie and node x must be an ancestor of w). Also, node x is a grandchild of z
and could be equal to w. Since z became unbalanced because of an insertion in the
subtree rooted at its child y, the height of y is 2 greater than its sibling.
We now rebalance the subtree rooted at z by calling the trinode restructur
ing function, restructure(x), given in Code Fragment 10.12 and illustrated in Fig
ures 10.9 and 10.10. A trinode restructuring temporarily renames the nodes x, y,
and z as a, b, and c, so that a precedes b and b precedes c in an inorder traversal
of T. There are four possible ways of mapping x, y, and z to a, b, and c, as shown
in Figure 10.10, which are unified into one case by our relabeling. The trinode
restructuring then replaces z with the node called b, makes the children of this node
be a and c, and makes the children of a and c be the four previous children of x,
y, and z (other than x and y) while maintaining the inorder relationships of all the
nodes in T.
Page 466
442Chapter 10. Search Trees
Algorithm restructure(x):
Input: A node x of a binary search tree T that has both a parent y and a grand
parent z
Output: Tree T after a trinode restructuring (which corresponds to a single or
double rotation) involving nodes x, y, and z
1: Let (a,b,c) be a left-to-right (inorder) listing of the nodes x, y, and z, and let
(T0,T1,T2,T3) be a left-to-right (inorder) listing of the four subtrees of x, y, and
z not rooted at x, y, or z.
2: Replace the subtree rooted at z with a new subtree rooted at b.
3: Let a be the left child of b and let T0 and T1 be the left and right subtrees of a,
respectively.
4: Let c be the right child of b and let T2 and T3 be the left and right subtrees of c,
respectively.
Code Fragment 10.12: The trinode restructuring operation in a binary search tree.
The modification of a tree T caused by a trinode restructuring operation is often
called a rotation, because of the geometric way we can visualize the way it changes
T . If b = y, the trinode restructuring method is called a single rotation, for it can
be visualized as “rotating” y over z. (See Figure 10.10(a) and (b).) Otherwise,
if b = x, the trinode restructuring operation is called a double rotation, for it can
be visualized as first “rotating” x over y and then over z. (See Figure 10.10(c)
and (d), and Figure 10.9.) Some computer researchers treat these two kinds of
rotations as separate methods, each with two symmetric types. We have chosen,
however, to unify these four types of rotations into a single trinode restructuring
operation. No matter how we view it, though, the trinode restructuring method
modifies parent-child relationships of O(1) nodes in T , while preserving the inorder
traversal ordering of all the nodes in T .
In addition to its order-preserving property, a trinode restructuring changes the
heights of several nodes in T , so as to restore balance. Recall that we execute the
function restructure(x) because z, the grandparent of x, is unbalanced. Moreover,
this unbalance is due to one of the children of x now having too large a height
relative to the height of z’s other child. As a result of a rotation, we move up the
“tall” child of x while pushing down the “short” child of z. Thus, after performing
restructure(x), all the nodes in the subtree now rooted at the node we called b are
balanced. (See Figure 10.10.) Thus, we restore the height-balance property locally
at the nodes x, y, and z. In addition, since after performing the new entry insertion
the subtree rooted at b replaces the one formerly rooted at z, which was taller by one
unit, all the ancestors of z that were formerly unbalanced become balanced. (See
Figure 10.9.) (The justification of this fact is left as Exercise C-10.14.) Therefore,
this one restructuring also restores the height-balance property globally.
Page 467
10.2. AVL Trees443
(a)
(b)
(c)
(d)
Figure 10.10: Schematic illustration of a trinode restructuring operation (Code
Fragment 10.12): (a) and (b) a single rotation; (c) and (d) a double rotation.
Page 468
444Chapter 10. Search Trees
Removal
As was the case for the insert map operation, we begin the implementation of the
erase map operation on an AVL tree T by using the algorithm for performing this
operation on a regular binary search tree. The added difficulty in using this ap
proach with an AVL tree is that it may violate the height-balance property. In
particular, after removing an internal node with operation removeAboveExternal
and elevating one of its children into its place, there may be an unbalanced node in
T on the path from the parent w of the previously removed node to the root of T.
(See Figure 10.11(a).) In fact, there can be one such unbalanced node at most. (The
justification of this fact is left as Exercise C-10.13.)
Figure 10.11: Removal of the entry with key 32 from the AVL tree of Figure 10.8:
(a) after removing the node storing key 32, the root becomes unbalanced; (b) a
(single) rotation restores the height-balance property.
As with insertion, we use trinode restructuring to restore balance in the tree T.
In particular, let z be the first unbalanced node encountered going up from w toward
the root of T. Also, let y be the child of z with larger height (note that node y is the
child of z that is not an ancestor of w), and let x be the child of y defined as follows:
if one of the children of y is taller than the other, let x be the taller child of y; else
(both children of y have the same height), let x be the child of y on the same side as
y (that is, if y is a left child, let x be the left child of y, else let x be the right child
of y). In any case, we then perform a restructure(x) operation, which restores the
height-balance property locally, at the subtree that was formerly rooted at z and is
now rooted at the node we temporarily called b. (See Figure 10.11(b).)
Unfortunately, this trinode restructuring may reduce the height of the subtree
rooted at b by 1, which may cause an ancestor of b to become unbalanced. So,
after rebalancing z, we continue walking up T looking for unbalanced nodes. If we
find another, we perform a restructure operation to restore its balance, and continue
marching up T looking for more, all the way to the root. Still, since the height of T
is O(logn), where n is the number of entries, by Proposition 10.2, O(logn) trinode
restructurings are sufficient to restore the height-balance property.
Page 469
10.2. AVL Trees445
Performance of AVL Trees
We summarize the analysis of the performance of an AVL tree T as follows. Op
erations find, insert, and erase visit the nodes along a root-to-leaf path of T, plus,
possibly, their siblings, and spend O(1) time per node. Thus, since the height of T
is O(logn) by Proposition 10.2, each of the above operations takes O(logn) time.
In Table 10.2, we summarize the performance of a map implemented with an AVL
tree. We illustrate this performance in Figure 10.12.
Operation TimeTime
size, empty O(1)
find, insert, erase O(logn)
Table 10.2: Performance of an n-entry map realized by an AVL tree. The space
usage is O(n).
Figure 10.12: Illustrating the running time of searches and updates in an AVL tree.
The time performance is O(1) per level, broken into a down phase, which typi
cally involves searching, and an up phase, which typically involves updating height
values and performing local trinode restructurings (rotations).
Page 470
446Chapter 10. Search Trees
10.2.2 C++ Implementation of an AVL Tree
Let us now turn to the implementation details and analysis of using an AVL tree T
with n internal nodes to implement an ordered dictionary of n entries. The insertion
and removal algorithms for T require that we are able to perform trinode restruc
turings and determine the difference between the heights of two sibling nodes. Re
garding restructurings, we now need to make sure our underlying implementation
of a binary search tree includes the method restructure(x), which performs a trinode
restructuring operation (Code Fragment 10.12). (We do not provide an implemen
tation of this function, but it is a straightforward addition to the linked binary tree
class given in Section 7.3.4.) It is easy to see that a restructure operation can be
performed in O(1) time if T is implemented with a linked structure. We assume
that the SearchTree class includes this function.
Regarding height information, we have chosen to store the height of each inter
nal node, v, explicitly in each node. Alternatively, we could have stored the balance
factor of v at v, which is defined as the height of the left child of v minus the height
of the right child of v. Thus, the balance factor of v is always equal to −1, 0, or 1,
except during an insertion or removal, when it may become temporarily equal to
−2 or +2. During the execution of an insertion or removal, the heights and balance
factors of O(logn) nodes are affected and can be maintained in O(logn) time.
In order to store the height information, we derive a subclass, called AVLEntry,
from the standard entry class given earlier in Code Fragment 10.3. It is templated
with the base entry type, from which it inherits the key and value members. It
defines a member variable ht, which stores the height of the subtree rooted at the
associated node. It provides member functions for accessing and setting this value.
These functions are protected, so that a user cannot access them, but AVLTree can.
template <typename E>
class AVLEntry : public E {// an AVL entry
private:
int ht;// node height
protected:// local types
typedef typename E::Key K;// key type
typedef typename E::Value V;// value type
int height() const { return ht; }// get height
void setHeight(int h) { ht = h; }// set height
public:// public functions
AVLEntry(const K& k = K(), const V& v = V()) // constructor
: E(k,v), ht(0) { }
friend class AVLTree<E>;// allow AVLTree access
};
Code Fragment 10.13: An enhanced key-value entry for class AVLTree, containing
the height of the associated node.
Page 471
10.2. AVL Trees447
In Code Fragment 10.14, we present the class definition for AVLTree. This
class is derived from the class SearchTree, but using our enhanced AVLEntry in
order to maintain height information for the nodes of the tree. The class defines a
number of typedef shortcuts for referring to entities such as keys, values, and tree
positions. The class declares all the standard dictionary public member functions.
At the end, it also defines a number of protected utility functions, which are used
in maintaining the AVL tree balance properties.
template <typename E>// an AVL tree
class AVLTree : public SearchTree< AVLEntry<E> > {
public:// public types
typedef AVLEntry<E> AVLEntry;// an entry
typedef typename SearchTree<AVLEntry>::Iterator Iterator; // an iterator
protected:// local types
typedef typename AVLEntry::Key K;// a key
typedef typename AVLEntry::Value V;// a value
typedef SearchTree<AVLEntry> ST;// a search tree
typedef typename ST::TPos TPos;// a tree position
public:// public functions
AVLTree();// constructor
Iterator insert(const K& k, const V& x);// insert (k,x)
void erase(const K& k) throw(NonexistentElement); // remove key k entry
void erase(const Iterator& p);// remove entry at p
protected:// utility functions
int height(const TPos& v) const;// node height utility
void setHeight(TPos v);// set height utility
bool isBalanced(const TPos& v) const;// is v balanced?
TPos tallGrandchild(const TPos& v) const;// get tallest grandchild
void rebalance(const TPos& v);// rebalance utility
};
Code Fragment 10.14: Class AVLTree, an AVL tree implementation of a dictionary.
Next, in Code Fragment 10.15, we present the constructor and height utility
function. The constructor simply invokes the constructor for the binary search tree,
which creates a tree having no entries. The function height returns the height of
a node, by extracting the height information from the AVLEntry. We employ the
condensed function notation that we introduced in Section 9.2.7.
/* AVLTree〈E〉 :: */// constructor
AVLTree() : ST() { }
/* AVLTree〈E〉 :: */// node height utility
int height(const TPos& v) const
{ return (v.isExternal() ? 0 : v−>height()); }
Code Fragment 10.15: The constructor for class AVLTree and a utility for extracting
heights.
Page 472
448Chapter 10. Search Trees
In Code Fragment 10.16, we present a few utility functions needed for main
taining the tree’s balance. The function setHeight sets the height information for a
node as one more than the maximum of the heights of its two children. The func
tion isBalanced determines whether a node satisfies the AVL balance condition, by
checking that the height difference between its children is at most 1. Finally, the
function tallGrandchild determines the tallest grandchild of a node. Recall that this
procedure is needed by the removal operation to determine the node to which the
restructuring operation will be applied.
/* AVLTree〈E〉 :: */// set height utility
void setHeight(TPos v) {
int hl = height(v.left());
int hr = height(v.right());
v−>setHeight(1 + std::max(hl, hr));// max of left & right
}
/* AVLTree〈E〉 :: */// is v balanced?
bool isBalanced(const TPos& v) const {
int bal = height(v.left()) − height(v.right());
return ((−1 <= bal) && (bal <= 1));
}
/* AVLTree〈E〉 :: */// get tallest grandchild
TPos tallGrandchild(const TPos& z) const {
TPos zl = z.left();
TPos zr = z.right();
if (height(zl) >= height(zr))// left child taller
if (height(zl.left()) >= height(zl.right()))
return zl.left();
else
return zl.right();
else// right child taller
if (height(zr.right()) >= height(zr.left()))
return zr.right();
else
return zr.left();
}
Code Fragment 10.16: Some utility functions used for maintaining balance in the
AVL tree.
Next, we present the principal function for rebalancing the AVL tree after an
insertion or removal. The procedure starts at the node v affected by the operation.
It then walks up the tree to the root level. On visiting each node z, it updates
z’s height information (which may have changed due to the update operation) and
Page 473
10.2. AVL Trees449
checks whether z is balanced. If not, it finds z’s tallest grandchild, and applies the
restructuring operation to this node. Since heights may have changed as a result, it
updates the height information for z’s children and itself.
/* AVLTree〈E〉 :: */// rebalancing utility
void rebalance(const TPos& v) {
TPos z = v;
while (!(z == ST::root())) {// rebalance up to root
z = z.parent();
setHeight(z);// compute new height
if (!isBalanced(z)) {// restructuring needed
TPos x = tallGrandchild(z);
z = restructure(x);// trinode restructure
setHeight(z.left());// update heights
setHeight(z.right());
setHeight(z);
}
}
}
Code Fragment 10.17: Rebalancing the tree after an update operation.
Finally, in Code Fragment 10.18, we present the functions for inserting and
erasing keys. (We have omitted the iterator-based erase function, since it is very
simple.) Each invokes the associated utility function (inserter or eraser, respec
tively) from the base class SearchTree. Each then invokes rebalance to restore
balance to the tree.
/* AVLTree〈E〉 :: */// insert (k,x)
Iterator insert(const K& k, const V& x) {
TPos v = inserter(k, x);// insert in base tree
setHeight(v);// compute its height
rebalance(v);// rebalance if needed
return Iterator(v);
}
/* AVLTree〈E〉 :: */// remove key k entry
void erase(const K& k) throw(NonexistentElement) {
TPos v = finder(k, ST::root());// find in base tree
if (Iterator(v) == ST::end())// not found?
throw NonexistentElement("Erase of nonexistent");
TPos w = eraser(v);// remove it
rebalance(w);// rebalance if needed
}
Code Fragment 10.18: The insertion and erasure functions.
Page 474
450Chapter 10. Search Trees
10.3 Splay Trees
Another way we can implement the fundamental map operations is to use a bal
anced search tree data structure known as a splay tree. This structure is conceptu
ally quite different from the other balanced search trees we discuss in this chapter,
for a splay tree does not use any explicit rules to enforce its balance. Instead, it ap
plies a certain move-to-root operation, called splaying, after every access, in order
to keep the search tree balanced in an amortized sense. The splaying operation is
performed at the bottom-most node x reached during an insertion, deletion, or even
a search. The surprising thing about splaying is that it allows us to guarantee an
amortized running time for insertions, deletions, and searches, that is logarithmic.
The structure of a splay tree is simply a binary search tree T . In fact, there are no
additional height, balance, or color labels that we associate with the nodes of this
tree.
10.3.1 Splaying
Given an internal node x of a binary search tree T , we splay x by moving x to
the root of T through a sequence of restructurings. The particular restructurings
we perform are important, for it is not sufficient to move x to the root of T by
just any sequence of restructurings. The specific operation we perform to move
x up depends upon the relative positions of x, its parent y, and (if it exists) x’s
grandparent z. There are three cases that we consider.
zig-zig: The node x and its parent y are both left children or both right children.
(See Figure 10.13.) We replace z by x, making y a child of x and z a child of
y, while maintaining the inorder relationships of the nodes in T .
(a)(b)
Figure 10.13: Zig-zig: (a) before; (b) after. There is another symmetric configura
tion where x and y are left children.
Page 475
10.3. Splay Trees451
zig-zag: One of x and y is a left child and the other is a right child. (See Fig
ure 10.14.) In this case, we replace z by x and make x have y and z as its
children, while maintaining the inorder relationships of the nodes in T .
(a)(b)
Figure 10.14: Zig-zag: (a) before; (b) after. There is another symmetric configura
tion where x is a right child and y is a left child.
zig: x does not have a grandparent (or we are not considering x’s grandparent for
some reason). (See Figure 10.15.) In this case, we rotate x over y, making x’s
children be the node y and one of x’s former children w, in order to maintain
the relative inorder relationships of the nodes in T .
(a)(b)
Figure 10.15: Zig: (a) before; (b) after. There is another symmetric configuration
where x and w are left children.
We perform a zig-zig or a zig-zag when x has a grandparent, and we perform a
zig when x has a parent but not a grandparent. A splaying step consists of repeating
these restructurings at x until x becomes the root of T . Note that this is not the
same as a sequence of simple rotations that brings x to the root. An example of the
splaying of a node is shown in Figures 10.16 and 10.17.
Page 476
452Chapter 10. Search Trees
(a)
(b)
(c)
Figure 10.16: Example of splaying a node: (a) splaying the node storing 14 starts
with a zig-zag; (b) after the zig-zag; (c) the next step is a zig-zig. (Continues in
Figure 10.17.)
Page 477
10.3. Splay Trees453
(d)
(e)
(f)
Figure 10.17: Example of splaying a node: (d) after the zig-zig; (e) the next step is
again a zig-zig; (f) after the zig-zig (Continued from Figure 10.17.)
Page 478
454Chapter 10. Search Trees
10.3.2 When to Splay
The rules that dictate when splaying is performed are as follows:
• When searching for key k, if k is found at a node x, we splay x, else we splay
the parent of the external node at which the search terminates unsuccessfully.
For example, the splaying in Figures 10.16 and 10.17 would be performed
after searching successfully for key 14 or unsuccessfully for key 14.5.
• When inserting key k, we splay the newly created internal node where k
gets inserted. For example, the splaying in Figures 10.16 and 10.17 would
be performed if 14 were the newly inserted key. We show a sequence of
insertions in a splay tree in Figure 10.18.
(a)(b)(c)
(d)(e)(f)
(g)
Figure 10.18: A sequence of insertions in a splay tree: (a) initial tree; (b) after
inserting 2; (c) after splaying; (d) after inserting 3; (e) after splaying; (f) after
inserting 4; (g) after splaying.
Page 479
10.3. Splay Trees455
• When deleting a key k, we splay the parent of the node w that gets removed,
that is, w is either the node storing k or one of its descendents. (Recall the re
moval algorithm for binary search trees.) An example of splaying following
a deletion is shown in Figure 10.19.
(a)(b)
(c)(d)
(e)
Figure 10.19: Deletion from a splay tree: (a) the deletion of 8 from node r is per
formed by moving the key of the right-most internal nodr v to r, in the left subtree
of r, deleting v, and splaying the parent u of v; (b) splaying u starts with a zig-zig;
(c) after the zig-zig; (d) the next step is a zig; (e) after the zig.
Page 480
456Chapter 10. Search Trees
10.3.3 Amortized Analysis of Splaying ⋆
After a zig-zig or zig-zag, the depth of x decreases by two, and after a zig the depth
of x decreases by one. Thus, if x has depth d, splaying x consists of a sequence of
⌊d/2⌋ zig-zigs and/or zig-zags, plus one final zig if d is odd. Since a single zig-zig,
zig-zag, or zig effects a constant number of nodes, it can be done in O(1) time.
Thus, splaying a node x in a binary search tree T takes time O(d), where d is the
depth of x in T . In other words, the time for performing a splaying step for a node x
is asymptotically the same as the time needed just to reach that node in a top-down
search from the root of T .
Worst-Case Time
In the worst case, the overall running time of a search, insertion, or deletion in a
splay tree of height h is O(h), since the node we splay might be the deepest node in
the tree. Moreover, it is possible for h to be as large as n, as shown in Figure 10.18.
Thus, from a worst-case point of view, a splay tree is not an attractive data structure.
In spite of its poor worst-case performance, a splay tree performs well in an
amortized sense. That is, in a sequence of intermixed searches, insertions, and
deletions, each operation takes, on average, logarithmic time. We perform the
amortized analysis of splay trees using the accounting method.
Amortized Performance of Splay Trees
For our analysis, we note that the time for performing a search, insertion, or deletion
is proportional to the time for the associated splaying. So let us consider only
splaying time.
Let T be a splay tree with n keys, and let v be a node of T . We define the size
n(v) of v as the number of nodes in the subtree rooted at v. Note that this definition
implies that the size of an internal node is one more than the sum of the sizes of
its two children. We define the rank r(v) of a node v as the logarithm in base 2 of
the size of v, that is, r(v) = log(n(v)). Clearly, the root of T has the maximum size
(2n + 1) and the maximum rank, log(2n + 1), while each external node has size 1
and rank 0.
We use cyber-dollars to pay for the work we perform in splaying a node x in
T , and we assume that one cyber-dollar pays for a zig, while two cyber-dollars pay
for a zig-zig or a zig-zag. Hence, the cost of splaying a node at depth d is d cyber
dollars. We keep a virtual account storing cyber-dollars at each internal node of T .
Note that this account exists only for the purpose of our amortized analysis, and
does not need to be included in a data structure implementing the splay tree T .
Page 481
10.3. Splay Trees457
An Accounting Analysis of Splaying
When we perform a splaying, we pay a certain number of cyber-dollars (the exact
value of the payment will be determined at the end of our analysis). We distinguish
three cases:
• If the payment is equal to the splaying work, then we use it all to pay for the
splaying.
• If the payment is greater than the splaying work, we deposit the excess in the
accounts of several nodes.
• If the payment is less than the splaying work, we make withdrawals from the
accounts of several nodes to cover the deficiency.
We show below that a payment of O(logn) cyber-dollars per operation is sufficient
to keep the system working, that is, to ensure that each node keeps a nonnegative
account balance.
An Accounting Invariant for Splaying
We use a scheme in which transfers are made between the accounts of the nodes
to ensure that there will always be enough cyber-dollars to withdraw for paying for
splaying work when needed.
In order to use the accounting method to perform our analysis of splaying, we
maintain the following invariant:
Before and after a splaying, each node v of T has r(v) cyber-dollars
in its account.
Note that the invariant is “financially sound,” since it does not require us to make a
preliminary deposit to endow a tree with zero keys.
Let r(T) be the sum of the ranks of all the nodes of T. To preserve the invariant
after a splaying, we must make a payment equal to the splaying work plus the total
change in r(T). We refer to a single zig, zig-zig, or zig-zag operation in a splaying
as a splaying substep. Also, we denote the rank of a node v of T before and after a
splaying substep with r(v) and r′(v), respectively. The following proposition gives
an upper bound on the change of r(T) caused by a single splaying substep. We
repeatedly use this lemma in our analysis of a full splaying of a node to the root.
Page 482
458Chapter 10. Search Trees
Proposition 10.3: Let δ be the variation of r(T ) caused by a single splaying sub
step (a zig, zig-zig, or zig-zag) for a node x in T. We have the following:
• δ ≤ 3(r′(x) − r(x)) − 2 if the substep is a zig-zig or zig-zag
• δ ≤ 3(r′(x) − r(x)) if the substep is a zig
Justification: We use the fact (see Proposition A.1, Appendix A) that, if a > 0,
b > 0, and c > a + b,loga + logb ≤ 2logc − 2.(10.6)
Let us consider the change in r(T) caused by each type of splaying substep.
zig-zig: (Recall Figure 10.13.) Since the size of each node is one more than the
size of its two children, note that only the ranks of x, y, and z change in a
zig-zig operation, where y is the parent of x and z is the parent of y. Also,
r′(x) = r(z), r′(y) ≤ r′(x), and r(y) ≥ r(x) . Thus
δ = r′(x)+ r′(y)+ r′(z) − r(x) − r(y) − r(z)
≤ r′(y)+ r′(z) − r(x) − r(y)
≤ r′(x)+ r′(z) − 2r(x).(10.7)
Note that n(x)+ n′(z) ≤ n′(x). By 10.6, r(x) + r′(z) ≤ 2r′(x) − 2, that is,
r′(z) ≤ 2r′(x) − r(x) − 2.
This inequality and 10.7 imply
δ ≤ r′(x)+ (2r′(x) − r(x) − 2) − 2r(x)
≤ 3(r′(x) − r(x)) − 2.
zig-zag: (Recall Figure 10.14.) Again, by the definition of size and rank, only the
ranks of x, y, and z change, where y denotes the parent of x and z denotes the
parent of y. Also, r′(x) = r(z) and r(x) ≤ r(y). Thus
δ = r′(x)+ r′(y)+ r′(z) − r(x) − r(y) − r(z)
≤ r′(y)+ r′(z) − r(x) − r(y)
≤ r′(y)+ r′(z) − 2r(x).(10.8)
Note that n′(y) + n′(z) ≤ n′(x); hence, by 10.6, r′(y) + r′(z) ≤ 2r′(x) − 2.
Thus
δ ≤ 2r′(x) − 2 − 2r(x)
≤ 3(r′(x) − r(x)) − 2.
zig: (Recall Figure 10.15.) In this case, only the ranks of x and y change, where y
denotes the parent of x. Also, r′(y) ≤ r(y) and r′(x) ≥ r(x). Thus
δ = r′(y)+ r′(x) − r(y) − r(x)
≤ r′(x) − r(x)
≤ 3(r′(x) − r(x)).
Page 483
10.3. Splay Trees459
Proposition 10.4: Let T be a splay tree with root t, and let ∆ be the total variation
of r(T) caused by splaying a node x at depth d. We have
∆ ≤ 3(r(t)−r(x))−d + 2.
Justification: Splaying node x consists of p = ⌈d/2⌉ splaying substeps, each
of which is a zig-zig or a zig-zag, except possibly the last one, which is a zig if d
is odd. Let r0(x) = r(x) be the initial rank of x, and for i = 1,..., p, let ri(x) be
the rank of x after the ith substep and δi be the variation of r(T) caused by the ith
substep. By Lemma 10.3, the total variation ∆ of r(T) caused by splaying x is
∆ =p∑i=1δi
≤p∑i=1(3(ri(x)−ri−1(x))−2) + 2
= 3(rp(x)−r0(x))−2p+ 2
≤ 3(r(t)−r(x))−d + 2.
By Proposition 10.4, if we make a payment of 3(r(t)−r(x))+ 2 cyber-dollars
towards the splaying of node x, we have enough cyber-dollars to maintain the in
variant, keeping r(v) cyber-dollars at each node v in T, and pay for the entire splay
ing work, which costs d dollars. Since the size of the root t is 2n + 1, its rank
r(t) = log(2n + 1). In addition, we have r(x) < r(t). Thus, the payment to be
made for splaying is O(logn) cyber-dollars. To complete our analysis, we have to
compute the cost for maintaining the invariant when a node is inserted or deleted.
When inserting a new node v into a splay tree with n keys, the ranks of all the
ancestors of v are increased. Namely, let v0,vi,...,vd be the ancestors of v, where
v0 = v, vi is the parent of vi−1, and vd is the root. For i = 1,...,d, let n′(vi) and
n(vi) be the size of vi before and after the insertion, respectively, and let r′(vi) and
r(vi) be the rank of vi before and after the insertion, respectively. We have
n′(vi) = n(vi)+ 1.
Also, since n(vi) + 1 ≤ n(vi+1), for i = 0,1,...,d − 1, we have the following for
each i in this range
r′(vi) = log(n′(vi)) = log(n(vi) + 1) ≤ log(n(vi+1)) = r(vi+1).
Thus, the total variation of r(T) caused by the insertion is
d∑i=1 r′(vi)−r(vi) ≤ r′(vd) +d−1
∑i=1(r(vi+1)−r(vi))
= r′(vd)−r(v0)
≤ log(2n+ 1).
Therefore, a payment of O(logn) cyber-dollars is sufficient to maintain the invariant
when a new node is inserted.
Page 484
460Chapter 10. Search Trees
When deleting a node v from a splay tree with n keys, the ranks of all the
ancestors of v are decreased. Thus, the total variation of r(T ) caused by the deletion
is negative, and we do not need to make any payment to maintain the invariant
when a node is deleted. Therefore, we may summarize our amortized analysis in
the following proposition (which is sometimes called the “balance proposition” for
splay trees).
Proposition 10.5: Consider a sequence of m operations on a splay tree, each one
a search, insertion, or deletion, starting from a splay tree with zero keys. Also, let
ni be the number of keys in the tree after operation i, and n be the total number of
insertions. The total running time for performing the sequence of operations is
O m +m∑i=1logni!,
which is O(mlogn).
In other words, the amortized running time of performing a search, insertion, or
deletion in a splay tree is O(logn), where n is the size of the splay tree at the time.
Thus, a splay tree can achieve logarithmic-time amortized performance for imple
menting an ordered map ADT. This amortized performance matches the worst-case
performance of AVL trees, (2,4) trees, and red-black trees, but it does so using a
simple binary tree that does not need any extra balance information stored at each
of its nodes. In addition, splay trees have a number of other interesting properties
that are not shared by these other balanced search trees. We explore one such addi
tional property in the following proposition (which is sometimes called the “Static
Optimality” proposition for splay trees).
Proposition 10.6: Consider a sequence of m operations on a splay tree, each one
a search, insertion, or deletion, starting from a splay tree T with zero keys. Also, let
f (i) denote the number of times the entry i is accessed in the splay tree, that is, its
frequency, and let n denote the total number of entries. Assuming that each entry is
accessed at least once, then the total running time for performing the sequence of
operations is
O m +n∑i=1f (i)log(m/f (i))!.
We omit the proof of this proposition, but it is not as hard to justify as one might
imagine. The remarkable thing is that this proposition states that the amortized
running time of accessing an entry i is O(log(m/f (i))).
Page 485
10.4. (2,4) Trees461
10.4 (2,4) Trees
Some data structures we discuss in this chapter, including (2,4) trees, are multi
way search trees, that is, trees with internal nodes that have two or more children.
Thus, before we define (2,4) trees, let us discuss multi-way search trees.
10.4.1 Multi-Way Search Trees
Recall that multi-way trees are defined so that each internal node can have many
children. In this section, we discuss how multi-way trees can be used as search
trees. Recall that the entries that we store in a search tree are pairs of the form
(k,x), where k is the key and x is the value associated with the key. However, we
do not discuss how to perform updates in multi-way search trees now, since the
details for update methods depend on additional properties we want to maintain for
multi-way trees, which we discuss in Section 14.3.1.
Definition of a Multi-way Search Tree
Let v be a node of an ordered tree. We say that v is a d-node if v has d children.
We define a multi-way search tree to be an ordered tree T that has the following
properties, which are illustrated in Figure 10.1(a):
• Each internal node of T has at least two children. That is, each internal node
is a d-node such that d ≥ 2.
• Each internal d-node v of T with children v1,...,vd stores an ordered set of
d − 1 key-value entries (k1,x1),..., (kd−1,xd−1), where k1 ≤ ··· ≤ kd−1.
• Let us conventionally define k0 = −∞ and kd = +∞. For each entry (k,x)
stored at a node in the subtree of v rooted at vi, i = 1,...,d, we have that
ki−1 ≤ k ≤ ki.
That is, if we think of the set of keys stored at v as including the special fictitious
keys k0 = −∞ and kd = +∞, then a key k stored in the subtree of T rooted at a
child node vi must be “in between” two keys stored at v. This simple viewpoint
gives rise to the rule that a d-node stores d − 1 regular keys, and it also forms the
basis of the algorithm for searching in a multi-way search tree.
By the above definition, the external nodes of a multi-way search do not store
any entries and serve only as “placeholders,” as has been our convention with binary
search trees (Section 10.1); hence, a binary search tree can be viewed as a special
case of a multi-way search tree, where each internal node stores one entry and has
two children. In addition, while the external nodes could be null, we make the
simplifying assumption here that they are actual nodes that don’t store anything.
Page 486
462Chapter 10. Search Trees
(a)
(b)
(c)
Figure 10.20: (a) A multi-way search tree T; (b) search path in T for key 12 (un
successful search); (c) search path in T for key 24 (successful search).
Page 487
10.4. (2,4) Trees463
Whether internal nodes of a multi-way tree have two children or many, however,
there is an interesting relationship between the number of entries and the number
of external nodes.
Proposition 10.7: An n-entry multi-way search tree has n+1 external nodes.
We leave the justification of this proposition as an exercise (Exercise C-10.17).
Searching in a Multi-Way Tree
Given a multi-way search tree T , we note that searching for an entry with key k is
simple. We perform such a search by tracing a path in T starting at the root. (See
Figure 10.1(b) and (c).) When we are at a d-node v during this search, we compare
the key k with the keys k1,...,kd−1 stored at v. If k = ki for some i, the search is
successfully completed. Otherwise, we continue the search in the child vi of v such
that ki−1 < k < ki. (Recall that we conventionally define k0 = −∞ and kd = +∞.)
If we reach an external node, then we know that there is no entry with key k in T ,
and the search terminates unsuccessfully.
Data Structures for Representing Multi-way Search Trees
In Section 7.1.4, we discuss a linked data structure for representing a general tree.
This representation can also be used for a multi-way search tree. In fact, in using a
general tree to implement a multi-way search tree, the only additional information
that we need to store at each node is the set of entries (including keys) associated
with that node. That is, we need to store with v a reference to some collection that
stores the entries for v.
Recall that when we use a binary search tree to represent an ordered map M,
we simply store a reference to a single entry at each internal node. In using a multi
way search tree T to represent M, we must store a reference to the ordered set of
entries associated with v at each internal node v of T . This reasoning may at first
seem like a circular argument, since we need a representation of an ordered map to
represent an ordered map. We can avoid any circular arguments, however, by using
the bootstrapping technique, where we use a previous (less advanced) solution to
a problem to create a new (more advanced) solution. In this case, bootstrapping
consists of representing the ordered set associated with each internal node using
a map data structure that we have previously constructed (for example, a search
table based on a sorted array, as shown in Section 9.3.1). In particular, assuming
we already have a way of implementing ordered maps, we can realize a multi-way
search tree by taking a tree T and storing such a map at each node of T .
Page 488
464Chapter 10. Search Trees
The map we store at each node v is known as a secondary data structure, be
cause we are using it to support the bigger, primary data structure. We denote the
map stored at a node v of T as M(v). The entries we store in M(v) allow us to find
which child node to move to next during a search operation. Specifically, for each
node v of T , with children v1,...,vd and entries (k1,x1), ..., (kd−1,xd−1), we store,
in the map M(v), the entries
(k1,(x1,v1)),(k2,(x2,v2)),...,(kd−1,(xd−1,vd−1)),(+∞, (∅, vd)).
That is, an entry (ki,(xi,vi)) of map M(v) has key ki and value (xi,vi). Note that the
last entry stores the special key +∞.
With the realization of the multi-way search tree T above, processing a d-node
v while searching for an entry of T with key k can be done by performing a search
operation to find the entry (ki,(xi,vi)) in M(v) with smallest key greater than or
equal to k. We distinguish two cases:
• If k < ki, then we continue the search by processing child vi. (Note that if the
special key kd = +∞ is returned, then k is greater than all the keys stored at
node v, and we continue the search processing child vd.)
• Otherwise (k = ki), then the search terminates successfully.
Consider the space requirement for the above realization of a multi-way search
tree T storing n entries. By Proposition 10.7, using any of the common realizations
of an ordered map (Chapter 9) for the secondary structures of the nodes of T , the
overall space requirement for T is O(n).
Consider next the time spent answering a search in T . The time spent at a d
node v of T during a search depends on how we realize the secondary data structure
M(v). If M(v) is realized with a sorted array (that is, an ordered search table),
then we can process v in O(log d) time. If M(v) is realized using an unsorted list
instead, then processing v takes O(d) time. Let dmax denote the maximum number
of children of any node of T , and let h denote the height of T . The search time in a
multi-way search tree is either O(hdmax) or O(hlog dmax), depending on the specific
implementation of the secondary structures at the nodes of T (the map M(v)). If
dmax is a constant, the running time for performing a search is O(h), irrespective of
the implementation of the secondary structures.
Thus, the primary efficiency goal for a multi-way search tree is to keep the
height as small as possible, that is, we want h to be a logarithmic function of n, the
total number of entries stored in the map. A search tree with logarithmic height
such as this is called a balanced search tree.
Page 489
10.4. (2,4) Trees465
Definition of a (2,4) Tree
A multi-way search tree that keeps the secondary data structures stored at each node
small and also keeps the primary multi-way tree balanced is the (2,4) tree, which
is sometimes called 2-4 tree or 2-3-4 tree. This data structure achieves these goals
by maintaining two simple properties (see Figure 10.21):
Size Property: Every internal node has at most four children
Depth Property: All the external nodes have the same depth
Figure 10.21: A (2,4) tree.
Again, we assume that external nodes are empty and, for the sake of simplicity,
we describe our search and update methods assuming that external nodes are real
nodes, although this latter requirement is not strictly needed.
Enforcing the size property for (2,4) trees keeps the nodes in the multi-way
search tree simple. It also gives rise to the alternative name “2-3-4 tree,” since it
implies that each internal node in the tree has 2, 3, or 4 children. Another implica
tion of this rule is that we can represent the map M(v) stored at each internal node
v using an unordered list or an ordered array, and still achieve O(1)-time perfor
mance for all operations (since dmax = 4). The depth property, on the other hand,
enforces an important bound on the height of a (2,4) tree.
Page 490
466Chapter 10. Search Trees
Proposition 10.8: The height of a (2,4) tree storing n entries is O(log n).
Justification: Let h be the height of a (2,4) tree T storing n entries. We justify
the proposition by showing that the claims
1 2log(n + 1) ≤ h(10.9)
and
h ≤ log(n + 1)(10.10)
are true.
To justify these claims note first that, by the size property, we can have at most
4 nodes at depth 1, at most 42 nodes at depth 2, and so on. Thus, the number of
external nodes in T is at most 4h. Likewise, by the depth property and the definition
of a (2,4) tree, we must have at least 2 nodes at depth 1, at least 22 nodes at depth
2, and so on. Thus, the number of external nodes in T is at least 2h. In addition, by
Proposition 10.7, the number of external nodes in T is n + 1. Therefore, we obtain
2h ≤ n + 1
and
n + 1 ≤ 4h.
Taking the logarithm in base 2 of each of the above terms, we get that
h ≤ log(n + 1)
and
log(n + 1) ≤ 2h,
which justifies our claims (10.9 and 10.10).
Proposition 10.8 states that the size and depth properties are sufficient for keep
ing a multi-way tree balanced (Section 10.4.1). Moreover, this proposition implies
that performing a search in a (2,4) tree takes O(logn) time and that the specific
realization of the secondary structures at the nodes is not a crucial design choice,
since the maximum number of children dmax is a constant (4). We can, for exam
ple, use a simple ordered map implementation, such as an array-list search table,
for each secondary structure.
Page 491
10.4. (2,4) Trees467
10.4.2 Update Operations for (2,4) Trees
Maintaining the size and depth properties requires some effort after performing
insertions and removals in a (2,4) tree, however. We discuss these operations next.
Insertion
To insert a new entry (k,x), with key k, into a (2,4) tree T , we first perform a
search for k. Assuming that T has no entry with key k, this search terminates
unsuccessfully at an external node z. Let v be the parent of z. We insert the new
entry into node v and add a new child w (an external node) to v on the left of z. That
is, we add entry (k,x,w) to the map M(v).
Our insertion method preserves the depth property, since we add a new external
node at the same level as existing external nodes. Nevertheless, it may violate the
size property. Indeed, if a node v was previously a 4-node, then it may become a
5-node after the insertion, which causes the tree T to no longer be a (2,4) tree. This
type of violation of the size property is called an overflow at node v, and it must
be resolved in order to restore the properties of a (2,4) tree. Let v1,...,v5 be the
children of v, and let k1,...,k4 be the keys stored at v. To remedy the overflow at
node v, we perform a split operation on v as follows (see Figure 10.22):
• Replace v with two nodes v′ and v′′, where
◦ v′ is a 3-node with children v1,v2,v3 storing keys k1 and k2
◦ v′′ is a 2-node with children v4,v5 storing key k4
• If v was the root of T , create a new root node u; else, let u be the parent of v
• Insert key k3 into u and make v′ and v′′ children of u, so that if v was child i
of u, then v′ and v′′ become children i and i + 1 of u, respectively
We show a sequence of insertions in a (2,4) tree in Figure 10.23.
(a)(b)(c)
Figure 10.22: A node split: (a) overflow at a 5-node v; (b) the third key of v inserted
into the parent u of v; (c) node v replaced with a 3-node v′ and a 2-node v′′.
Page 492
468Chapter 10. Search Trees
(a)(b)(c)(d)
(e)(f)
(g)(h)
(i)(j)
(k)(l)
Figure 10.23: A sequence of insertions into a (2,4) tree: (a) initial tree with one
entry; (b) insertion of 6; (c) insertion of 12; (d) insertion of 15, which causes an
overflow; (e) split, which causes the creation of a new root node; (f) after the split;
(g) insertion of 3; (h) insertion of 5, which causes an overflow; (i) split; (j) after the
split; (k) insertion of 10; (l) insertion of 8.
Page 493
10.4. (2,4) Trees469
Analysis of Insertion in a (2,4) Tree
A split operation affects a constant number of nodes of the tree and O(1) entries
stored at such nodes. Thus, it can be implemented to run in O(1) time.
As a consequence of a split operation on node v, a new overflow may occur at
the parent u of v. If such an overflow occurs, it triggers a split at node u in turn. (See
Figure 10.24.) A split operation either eliminates the overflow or propagates it into
the parent of the current node. Hence, the number of split operations is bounded by
the height of the tree, which is O(logn) by Proposition 10.8. Therefore, the total
time to perform an insertion in a (2,4) tree is O(logn).
(a)(b)
(c)(d)
(e)(f)
Figure 10.24: An insertion in a (2,4) tree that causes a cascading split: (a) before
the insertion; (b) insertion of 17, causing an overflow; (c) a split; (d) after the split
a new overflow occurs; (e) another split, creating a new root node; (f) final tree.
Page 494
470Chapter 10. Search Trees
Removal
Let us now consider the removal of an entry with key k from a (2,4) tree T . We
begin such an operation by performing a search in T for an entry with key k. Re
moving such an entry from a (2,4) tree can always be reduced to the case where
the entry to be removed is stored at a node v whose children are external nodes.
Suppose, for instance, that the entry with key k that we wish to remove is stored in
the ith entry (ki,xi) at a node z that has only internal-node children. In this case,
we swap the entry (ki,xi) with an appropriate entry that is stored at a node v with
external-node children as follows (see Figure 10.25(d)):
1. We find the right-most internal node v in the subtree rooted at the ith child of
z, noting that the children of node v are all external nodes.
2. We swap the entry (ki,xi) at z with the last entry of v.
Once we ensure that the entry to remove is stored at a node v with only external
node children (because either it was already at v or we swapped it into v), we simply
remove the entry from v (that is, from the map M(v)) and remove the ith external
node of v.
Removing an entry (and a child) from a node v as described above preserves
the depth property, because we always remove an external node child from a node
v with only external-node children. However, in removing such an external node
we may violate the size property at v. Indeed, if v was previously a 2-node, then
it becomes a 1-node with no entries after the removal (Figure 10.25(d) and (e)),
which is not allowed in a (2,4) tree. This type of violation of the size property
is called an underflow at node v. To remedy an underflow, we check whether an
immediate sibling of v is a 3-node or a 4-node. If we find such a sibling w, then
we perform a transfer operation, in which we move a child of w to v, a key of w to
the parent u of v and w, and a key of u to v. (See Figure 10.25(b) and (c).) If v has
only one sibling, or if both immediate siblings of v are 2-nodes, then we perform a
fusion operation, in which we merge v with a sibling, creating a new node v′, and
move a key from the parent u of v to v′. (See Figure 10.26(e) and (f).)
A fusion operation at node v may cause a new underflow to occur at the parent u
of v, which in turn triggers a transfer or fusion at u. (See Figure 10.26.) Hence, the
number of fusion operations is bounded by the height of the tree, which is O(logn)
by Proposition 10.8. If an underflow propagates all the way up to the root, then
the root is simply deleted. (See Figure 10.26(c) and (d).) We show a sequence of
removals from a (2,4) tree in Figures 10.25 and 10.26.
Page 495
10.4. (2,4) Trees471
(a)(b)
(c)(d)
(e)(f)
(g)(h)
Figure 10.25: A sequence of removals from a (2,4) tree: (a) removal of 4, causing
an underflow; (b) a transfer operation; (c) after the transfer operation; (d) removal
of 12, causing an underflow; (e) a fusion operation; (f) after the fusion operation;
(g) removal of 13; (h) after removing 13.
Page 496
472Chapter 10. Search Trees
(a)(b)
(c)(d)
Figure 10.26: A propagating sequence of fusions in a (2,4) tree: (a) removal of 14,
which causes an underflow; (b) fusion, which causes another underflow; (c) second
fusion operation, which causes the root to be removed; (d) final tree.
Performance of (2,4) Trees
Table 10.3 summarizes the running times of the main operations of a map realized
with a (2,4) tree. The time complexity analysis is based on the following:
• The height of a (2,4) tree storing n entries is O(logn), by Proposition 10.8
• A split, transfer, or fusion operation takes O(1) time
• A search, insertion, or removal of an entry visits O(logn) nodes.
Operation Time
size, empty O(1)
find, insert, erase O(logn)
Table 10.3: Performance of an n-entry map realized by a (2,4) tree. The space
usage is O(n).
Thus, (2,4) trees provide for fast map search and update operations. (2,4) trees
also have an interesting relationship to the data structure we discuss next.
Page 497
10.5. Red-Black Trees473
10.5 Red-Black Trees
Although AVL trees and (2,4) trees have a number of nice properties, there are
some map applications for which they are not well suited. For instance, AVL trees
may require many restructure operations (rotations) to be performed after a re
moval, and (2,4) trees may require many fusing or split operations to be performed
after either an insertion or removal. The data structure we discuss in this section,
the red-black tree, does not have these drawbacks, however, as it requires that only
O(1) structural changes be made after an update in order to stay balanced.
A red-black tree is a binary search tree (see Section 10.1) with nodes colored
red and black in a way that satisfies the following properties:
Root Property: The root is black.
External Property: Every external node is black.
Internal Property: The children of a red node are black.
Depth Property: All the external nodes have the same black depth, defined as the
number of black ancestors minus one. (Recall that a node is an ancestor of
itself.)
An example of a red-black tree is shown in Figure 10.27.
Figure 10.27: Red-black tree associated with the (2,4) tree of Figure 10.21. Each
external node of this red-black tree has 4 black ancestors (including itself); hence, it
has black depth 3. We use the color blue instead of red. Also, we use the convention
of giving an edge of the tree the same color as the child node.
As for previous types of search trees, we assume that entries are stored at the
internal nodes of a red-black tree, with the external nodes being empty placehold
ers. Also, we assume that the external nodes are actual nodes, but we note that, at
the expense of slightly more complicated methods, external nodes could be null.
Page 498
474Chapter 10. Search Trees
We can make the red-black tree definition more intuitive by noting an interest
ing correspondence between red-black trees and (2,4) trees as illustrated in Fig
ure 10.28. Namely, given a red-black tree, we can construct a corresponding (2,4)
tree by merging every red node v into its parent and storing the entry from v at its
parent. Conversely, we can transform any (2,4) tree into a corresponding red-black
tree by coloring each node black and performing the following transformation for
each internal node v:
• If v is a 2-node, then keep the (black) children of v as is
• If v is a 3-node, then create a new red node w, give v’s first two (black)
children to w, and make w and v’s third child be the two children of v
• If v is a 4-node, then create two new red nodes w and z, give v’s first two
(black) children to w, give v’s last two (black) children to z, and make w and
z be the two children of v
−→
(a)
−→
(b)
−→
(c)
Figure 10.28: Correspondence between a (2,4) tree and a red-black tree: (a) 2-node;
(b) 3-node; (c) 4-node.
The correspondence between (2,4) trees and red-black trees provides important
intuition that we use in our discussion of how to perform updates in red-black trees.
In fact, the update algorithms for red-black trees are mysteriously complex without
this intuition.
Page 499
10.5. Red-Black Trees475
Proposition 10.9: The height of a red-black tree storing n entries is O(logn).
Justification: Let T be a red-black tree storing n entries, and let h be the height
of T . We justify this proposition by establishing the following fact
log(n + 1) ≤ h ≤ 2log(n + 1).
Let d be the common black depth of all the external nodes of T . Let T ′ be the
(2,4) tree associated with T , and let h′ be the height of T ′. Because of the corre
spondence between red-black trees and (2,4) trees, we know that h′ = d. Hence,
by Proposition 10.8, d = h′ ≤ log(n + 1). By the internal node property, h ≤ 2d.
Thus, we obtain h ≤ 2log(n + 1). The other inequality, log(n + 1) ≤ h, follows
from Proposition 7.10 and the fact that T has n internal nodes.
We assume that a red-black tree is realized with a linked structure for binary
trees (Section 7.3.4), in which we store a map entry and a color indicator at each
node. Thus, the space requirement for storing n keys is O(n). The algorithm for
searching in a red-black tree T is the same as that for a standard binary search tree
(Section 10.1). Thus, searching in a red-black tree takes O(logn) time.
10.5.1 Update Operations
Performing the update operations in a red-black tree is similar to that of a binary
search tree, except that we must additionally restore the color properties.
Insertion
Now consider the insertion of an entry with key k into a red-black tree T , keeping
in mind the correspondence between T and its associated (2,4) tree T ′ and the
insertion algorithm for T ′. The algorithm initially proceeds as in a binary search
tree (Section 10.1.2). Namely, we search for k in T until we reach an external node
of T , and we replace this node with an internal node z, storing (k,x) and having two
external-node children. If z is the root of T , we color z black, else we color z red.
We also color the children of z black. This action corresponds to inserting (k,x) into
a node of the (2,4) tree T ′ with external children. In addition, this action preserves
the root, external, and depth properties of T , but it may violate the internal property.
Indeed, if z is not the root of T and the parent v of z is red, then we have a parent and
a child (namely, v and z) that are both red. Note that by the root property, v cannot
be the root of T , and by the internal property (which was previously satisfied), the
parent u of v must be black. Since z and its parent are red, but z’s grandparent u is
black, we call this violation of the internal property a double red at node z.
To remedy a double red, we consider two cases.
Page 500
476Chapter 10. Search Trees
Case 1: The Sibling w of v is Black. (See Figure 10.29.) In this case, the double
red denotes the fact that we have created in our red-black tree T a malformed
replacement for a corresponding 4-node of the (2,4) tree T′, which has as its
children the four black children of u, v, and z. Our malformed replacement
has one red node (v) that is the parent of another red node (z), while we want
it to have the two red nodes as siblings instead. To fix this problem, we
perform a trinode restructuring of T. The trinode restructuring is done by
the operation restructure(z), which consists of the following steps (see again
Figure 10.29; this operation is also discussed in Section 10.2):
• Take node z, its parent v, and grandparent u, and temporarily relabel
them as a, b, and c, in left-to-right order, so that a, b, and c will be
visited in this order by an inorder tree traversal.
• Replace the grandparent u with the node labeled b, and make nodes a
and c the children of b, keeping inorder relationships unchanged.
After performing the restructure(z) operation, we color b black and we color
a and c red. Thus, the restructuring eliminates the double red problem.
(a)
(b)
Figure 10.29: Restructuring a red-black tree to remedy a double red: (a) the four
configurations for u, v, and z before restructuring; (b) after restructuring.
Page 501
10.5. Red-Black Trees477
Case 2: The Sibling w of v is Red. (See Figure 10.30.) In this case, the double red
denotes an overflow in the corresponding (2,4) tree T. To fix the problem,
we perform the equivalent of a split operation. Namely, we do a recoloring:
we color v and w black and their parent u red (unless u is the root, in which
case, it is colored black). It is possible that, after such a recoloring, the
double red problem reappears, although higher up in the tree T, since u may
have a red parent. If the double red problem reappears at u, then we repeat
the consideration of the two cases at u. Thus, a recoloring either eliminates
the double red problem at node z, or propagates it to the grandparent u of z.
We continue going up T performing recolorings until we finally resolve the
double red problem (with either a final recoloring or a trinode restructuring).
Thus, the number of recolorings caused by an insertion is no more than half
the height of tree T, that is, no more than log(n+ 1) by Proposition 10.9.
(a)
(b)
Figure 10.30: Recoloring to remedy the double red problem: (a) before recoloring
and the corresponding 5-node in the associated (2,4) tree before the split; (b) after
the recoloring (and corresponding nodes in the associated (2,4) tree after the split).
Figures 10.31 and 10.32 show a sequence of insertion operations in a red-black
tree.
Page 502
478Chapter 10. Search Trees
(a)(b)(c)(d)
(e)(f)(g)(h)
(i)(j)
(k)(l)
Figure 10.31: A sequence of insertions in a red-black tree: (a) initial tree; (b) in
sertion of 7; (c) insertion of 12, which causes a double red; (d) after restructuring;
(e) insertion of 15, which causes a double red; (f) after recoloring (the root remains
black); (g) insertion of 3; (h) insertion of 5; (i) insertion of 14, which causes a
double red; (j) after restructuring; (k) insertion of 18, which causes a double red;
(l) after recoloring. (Continues in Figure 10.32.)
Page 503
10.5. Red-Black Trees479
(m)(n)
(o)(p)
(q)
Figure 10.32: A sequence of insertions in a red-black tree: (m) insertion of 16,
which causes a double red; (n) after restructuring; (o) insertion of 17, which causes
a double red; (p) after recoloring there is again a double red, to be handled by a
restructuring; (q) after restructuring. (Continued from Figure 10.31.)
Page 504
480Chapter 10. Search Trees
The cases for insertion imply an interesting property for red-black trees. Namely,
since the Case 1 action eliminates the double-red problem with a single trinode re
structuring and the Case 2 action performs no restructuring operations, at most one
restructuring is needed in a red-black tree insertion. By the above analysis and the
fact that a restructuring or recoloring takes O(1) time, we have the following.
Proposition 10.10: The insertion of a key-value entry in a red-black tree storing
n entries can be done in O(logn) time and requires O(logn) recolorings and one
trinode restructuring (a restructure operation).
Removal
Suppose now that we are asked to remove an entry with key k from a red-black
tree T. Removing such an entry initially proceeds like a binary search tree (Sec
tion 10.1.2). First, we search for a node u storing such an entry. If node u does
not have an external child, we find the internal node v following u in the inorder
traversal of T, move the entry at v to u, and perform the removal at v. Thus, we may
consider only the removal of an entry with key k stored at a node v with an external
child w. Also, as we did for insertions, we keep in mind the correspondence be
tween red-black tree T and its associated (2,4) tree T′ (and the removal algorithm
for T′).
To remove the entry with key k from a node v of T with an external child w we
proceed as follows. Let r be the sibling of w and x be the parent of v. We remove
nodes v and w, and make r a child of x. If v was red (hence r is black) or r is red
(hence v was black), we color r black and we are done. If, instead, r is black and v
was black, then, to preserve the depth property, we give r a fictitious double black
color. We now have a color violation, called the double black problem. A double
black in T denotes an underflow in the corresponding (2,4) tree T′. Recall that x
is the parent of the double black node r. To remedy the double-black problem at r,
we consider three cases.
Case 1: The Sibling y of r is Black and Has a Red Child z. (See Figure 10.33.)
Resolving this case corresponds to a transfer operation in the (2,4) tree T′.
We perform a trinode restructuring by means of operation restructure(z).
Recall that the operation restructure(z) takes the node z, its parent y, and
grandparent x, labels them temporarily left to right as a, b, and c, and replaces
x with the node labeled b, making it the parent of the other two. (See the
description of restructure in Section 10.2.) We color a and c black, give b
the former color of x, and color r black. This trinode restructuring eliminates
the double black problem. Hence, at most one restructuring is performed in
a removal operation in this case.
Page 505
10.5. Red-Black Trees481
(a)
(b)
(c)
Figure 10.33: Restructuring of a red-black tree to remedy the double black problem:
(a) and (b) configurations before the restructuring, where r is a right child and
the associated nodes in the corresponding (2,4) tree before the transfer (two other
symmetric configurations where r is a left child are possible); (c) configuration after
the restructuring and the associated nodes in the corresponding (2,4) tree after the
transfer. The grey color for node x in parts (a) and (b) and for node b in part (c)
denotes the fact that this node may be colored either red or black.
Page 506
482Chapter 10. Search Trees
Case 2: The Sibling y of r is Black and Both Children of y Are Black. (See Fig
ures 10.34 and 10.35.) Resolving this case corresponds to a fusion operation
in the corresponding (2,4) tree T′. We do a recoloring; we color r black, we
color y red, and, if x is red, we color it black (Figure 10.34); otherwise, we
color x double black (Figure 10.35). Hence, after this recoloring, the double
black problem may reappear at the parent x of r. (See Figure 10.35.) That
is, this recoloring either eliminates the double black problem or propagates
it into the parent of the current node. We then repeat a consideration of these
three cases at the parent. Thus, since Case 1 performs a trinode restructuring
operation and stops (and, as we will soon see, Case 3 is similar), the number
of recolorings caused by a removal is no more than log(n+ 1).
(a)
(b)
Figure 10.34: Recoloring of a red-black tree that fixes the double black problem: (a)
before the recoloring and corresponding nodes in the associated (2,4) tree before
the fusion (other similar configurations are possible); (b) after the recoloring and
corresponding nodes in the associated (2,4) tree after the fusion.
Page 507
10.5. Red-Black Trees483
(a)
(b)
Figure 10.35: Recoloring of a red-black tree that propagates the double black prob
lem: (a) configuration before the recoloring and corresponding nodes in the asso
ciated (2,4) tree before the fusion (other similar configurations are possible); (b)
configuration after the recoloring and corresponding nodes in the associated (2,4)
tree after the fusion.
Page 508
484Chapter 10. Search Trees
Case 3: The Sibling y of r Is Red. (See Figure 10.36.) In this case, we perform
an adjustment operation, as follows. If y is the right child of x, let z be the
right child of y; otherwise, let z be the left child of y. Execute the trinode
restructuring operation restructure(z), which makes y the parent of x. Color
y black and x red. An adjustment corresponds to choosing a different rep
resentation of a 3-node in the (2,4) tree T ′. After the adjustment operation,
the sibling of r is black, and either Case 1 or Case 2 applies, with a different
meaning of x and y. Note that if Case 2 applies, the double-black problem
cannot reappear. Thus, to complete Case 3 we make one more application
of either Case 1 or Case 2 above and we are done. Therefore, at most one
adjustment is performed in a removal operation.
(a)
(b)
Figure 10.36: Adjustment of a red-black tree in the presence of a double black
problem: (a) configuration before the adjustment and corresponding nodes in the
associated (2,4) tree (a symmetric configuration is possible); (b) configuration after
the adjustment with the same corresponding nodes in the associated (2,4) tree.
Page 509
10.5. Red-Black Trees485
From the above algorithm description, we see that the tree updating needed
after a removal involves an upward march in the tree T, while performing at most
a constant amount of work (in a restructuring, recoloring, or adjustment) per node.
Thus, since any changes we make at a node in T during this upward march takes
O(1) time (because it affects a constant number of nodes), we have the following.
Proposition 10.11: The algorithm for removing an entry from a red-black tree
with n entries takes O(logn) time and performs O(logn) recolorings and at most
one adjustment plus one additional trinode restructuring. Thus, it performs at most
two restructure operations.
In Figures 10.37 and 10.38, we show a sequence of removal operations on a
red-black tree. We illustrate Case 1 restructurings in Figure 10.37(c) and (d). We
illustrate Case 2 recolorings at several places in Figures 10.37 and 10.38. Finally,
in Figure 10.38(i) and (j), we show an example of a Case 3 adjustment.
(a)(b)
(c)(d)
Figure 10.37: Sequence of removals from a red-black tree: (a) initial tree; (b) re
moval of 3; (c) removal of 12, causing a double black (handled by restructuring);
(d) after restructuring. (Continues in Figure 10.38.)
Page 510
486Chapter 10. Search Trees
(e)(f)
(g)(h)
(i)(j)
(k)
Figure 10.38: Sequence of removals in a red-black tree : (e) removal of 17; (f) re
moval of 18, causing a double black (handled by recoloring); (g) after recoloring;
(h) removal of 15; (i) removal of 16, causing a double black (handled by an adjust
ment); (j) after the adjustment the double black needs to be handled by a recoloring;
(k) after the recoloring. (Continued from Figure 10.37.)
Page 511
10.5. Red-Black Trees487
Performance of Red-Black Trees
Table 10.4 summarizes the running times of the main operations of a map realized
by means of a red-black tree. We illustrate the justification for these bounds in
Figure 10.39.
Operation Time
size, empty O(1)
find, insert, erase O(logn)
Table 10.4: Performance of an n-entry map realized by a red-black tree. The space
usage is O(n).
Figure 10.39: The running time of searches and updates in a red-black tree. The
time performance is O(1) per level, broken into a down phase, which typically
involves searching, and an up phase, which typically involves recolorings and per
forming local trinode restructurings (rotations).
Thus, a red-black tree achieves logarithmic worst-case running times for both
searching and updating in a map. The red-black tree data structure is slightly more
complicated than its corresponding (2,4) tree. Even so, a red-black tree has a
conceptual advantage that only a constant number of trinode restructurings are ever
needed to restore the balance in a red-black tree after an update.
Page 512
488Chapter 10. Search Trees
10.5.2 C++ Implementation of a Red-Black Tree
In this section, we discuss a C++ implementation of the dictionary ADT by means
of a red-black tree. It is interesting to note that the C++ Standard Template Li
brary uses a red-black tree in its implementation of its classes map and multimap.
The difference between the two is similar to the difference between our map and
dictionary ADTs. The STL map class does not allow entries with duplicate keys,
whereas the STL multimap does. There is a significant difference, however, in the
behavior of the map’s insert(k,x) function and our map’s put(k,x) function. If the
key k is not present, both functions insert the new entry (k,x) in the map. If the key
is already present, the STL map simply ignores the request, and the current entry is
unchanged. In contrast, our put function replaces the existing value with the new
value x. The implementation presented in this section allows for multiple keys.
We present the major portions of the implementation in this section. To keep the
presentation concise, we have omitted the implementations of a number of simpler
utility functions.
We begin by presenting the enhanced entry class, called RBEntry. It is derived
from the entry class of Code Fragment 10.3. It inherits the key and value members,
and it defines a member variable col, which stores the color of the node. The color
is either RED or BLACK. It provides member functions for accessing and setting
this value. These functions have been protected, so a user cannot access them, but
RBTree can.
enum Color {RED, BLACK};// node colors
template <typename E>
class RBEntry : public E {// a red-black entry
private:
Color col;// node color
protected:// local types
typedef typename E::Key K;// key type
typedef typename E::Value V;// value type
Color color() const { return col; }// get color
bool isRed() const { return col == RED; }
bool isBlack() const { return col == BLACK; }
void setColor(Color c) { col = c; }
public:// public functions
RBEntry(const K& k = K(), const V& v = V()) // constructor
: E(k,v), col(BLACK) { }
friend class RBTree<E>;// allow RBTree access
};
Code Fragment 10.19: A key-value entry for class RBTree, containing the associ
ated node’s color.
Page 513
10.5. Red-Black Trees489
In Code Fragment 10.20, we present the class definition for RBTree. The dec
laration is almost entirely analogous to that of AVLTree, except that the utility func
tions used to maintain the structure are different. We have chosen to present only
the two most interesting utility functions, remedyDoubleRed and remedyDouble
Black. The meanings of most of the omitted utilities are easy to infer. (For ex
ample hasTwoExternalChildren(v) determines whether a node v has two external
children.)
template <typename E>// a red-black tree
class RBTree : public SearchTree< RBEntry<E> > {
public:// public types
typedef RBEntry<E> RBEntry;// an entry
typedef typename SearchTree<RBEntry>::Iterator Iterator; // an iterator
protected:// local types
typedef typename RBEntry::Key K;// a key
typedef typename RBEntry::Value V;// a value
typedef SearchTree<RBEntry> ST;// a search tree
typedef typename ST::TPos TPos;// a tree position
public:// public functions
RBTree();// constructor
Iterator insert(const K& k, const V& x);// insert (k,x)
void erase(const K& k) throw(NonexistentElement); // remove key k entry
void erase(const Iterator& p);// remove entry at p
protected:// utility functions
void remedyDoubleRed(const TPos& z);// fix double-red z
void remedyDoubleBlack(const TPos& r);// fix double-black r
// . . .(other utilities omitted)
};
Code Fragment 10.20: Class RBTree, which implements a dictionary ADT using a
red-black tree.
We first discuss the implementation of the function insert(k,x), which is given
in Code Fragment 10.21. We invoke the inserter utility function of SearchTree,
which returns the position of the inserted node. If this node is the root of the search
tree, we set its color to black. Otherwise, we set its color to red and check whether
restructuring is needed by invoking remedyDoubleRed.
This latter utility performs the necessary checks and restructuring presented in
the discussion of insertion in Section 10.5.1. Let z denote the location of the newly
inserted node. If both z and its parent are red, we need to remedy the situation.
To do so, we consider two cases. Let v denote z’s parent and let w be v’s sibling.
If w is black, we fall under Case 1 of the insertion update procedure. We apply
restructuring at z. The top vertex of the resulting subtree, denoted by v, is set to
black, and its two children are set to red.
On the other hand, if w is red, then we fall under Case 2 of the update procedure.
Page 514
490Chapter 10. Search Trees
We resolve the situation by coloring both v and its sibling w black. If their common
parent is not the root, we set its color to red. This may induce another double-red
problem at v’s parent u, so we invoke the function recursively on u.
/* RBTree〈E〉 :: */// insert (k,x)
Iterator insert(const K& k, const V& x) {
TPos v = inserter(k, x);// insert in base tree
if (v == ST::root())
setBlack(v);// root is always black
else {
setRed(v);
remedyDoubleRed(v);// rebalance if needed
}
return Iterator(v);
}
/* RBTree〈E〉 :: */// fix double-red z
void remedyDoubleRed(const TPos& z) {
TPos v = z.parent();// v is z’s parent
if (v == ST::root() | | v−>isBlack()) return;// v is black, all ok
// z, v are double-red
if (sibling(v)−>isBlack()) {// Case 1: restructuring
v = restructure(z);
setBlack(v);// top vertex now black
setRed(v.left()); setRed(v.right());// set children red
} else {// Case 2: recoloring
setBlack(v); setBlack(sibling(v));// set v and sibling black
TPos u = v.parent();// u is v’s parent
if (u == ST::root()) return;
setRed(u);// make u red
remedyDoubleRed(u);// may need to fix u now
}
}
Code Fragment 10.21: The functions related to insertion for class RBTree. The
function insert invokes the inserter utility function, which was given in Code Frag
ment 10.10.
Finally, in Code Fragment 10.22, we present the implementation of the removal
function for the red-black tree. (We have omitted the simpler iterator-based erase
function.) The removal follows the process discussed in Section 10.5.1. We first
search for the key to be removed, and generate an exception if it is not found. Oth
erwise, we invoke the eraser utility of class SearchTree, which returns the position
of the node r that replaced the deleted node. If either r or its former parent was
red, we color r black and we are done. Otherwise, we face a potential double-black
problem. We handle this by invoking the function remedyDoubleBlack.
Page 515
10.5. Red-Black Trees491
/* RBTree〈E〉 :: */// remove key k entry
void erase(const K& k) throw(NonexistentElement) {
TPos u = finder(k, ST::root());// find the node
if (Iterator(u) == ST::end())
throw NonexistentElement("Erase of nonexistent");
TPos r = eraser(u);// remove u
if (r == ST::root() | | r−>isRed() | | wasParentRed(r))
setBlack(r);// fix by color change
else// r, parent both black
remedyDoubleBlack(r);// fix double-black r
}
/* RBTree〈E〉 :: */// fix double-black r
void remedyDoubleBlack(const TPos& r) {
TPos x = r.parent();// r’s parent
TPos y = sibling(r);// r’s sibling
if (y−>isBlack()) {
if (y.left()−>isRed() | | y.right()−>isRed()) { // Case 1: restructuring
// z is y’s red child
TPos z = (y.left()−>isRed() ? y.left() : y.right());
Color topColor = x−>color();// save top vertex color
z = restructure(z);// restructure x,y,z
setColor(z, topColor);// give z saved color
setBlack(r);// set r black
setBlack(z.left()); setBlack(z.right());// set z’s children black
} else {// Case 2: recoloring
setBlack(r); setRed(y);// r=black, y=red
if (x−>isBlack() && !(x == ST::root()))
remedyDoubleBlack(x);// fix double-black x
setBlack(x);
}
}
else {// Case 3: adjustment
TPos z = (y == x.right() ? y.right() : y.left()); // grandchild on y’s side
restructure(z);// restructure x,y,z
setBlack(y); setRed(x);// y=black, x=red
remedyDoubleBlack(r);// fix r by Case 1 or 2
}
}
Code Fragment 10.22: The functions related to removal for class RBTree. The
function erase invokes the eraser utility function, which was given in Code Frag
ment 10.11.
